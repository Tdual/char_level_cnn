{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import numpy as np\n",
    "from  urllib  import request\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_file = \"data/amazon_ja/pos.txt\"\n",
    "negative_data_file = \"data/amazon_ja/neg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file, level=\"char\", lang=\"En\", dic_dir=None, t=None):\n",
    "       \n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    if level == \"char\":\n",
    "        positive_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in positive_examples]\n",
    "        negative_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in negative_examples]\n",
    "    elif level == \"word\":\n",
    "        if lang == \"Ja\":\n",
    "            #t = Tokenizer(dic_dir)\n",
    "            positive_examples = [t.tokenize(s) for s in positive_examples]\n",
    "            negative_examples = [t.tokenize(s) for s in negative_examples]\n",
    "        else:\n",
    "            positive_examples = [s.strip() for s in positive_examples]\n",
    "            negative_examples = [s.strip() for s in negative_examples]\n",
    "    else:\n",
    "        print(\"invaid value of 'level'. ('char' or 'word') \")\n",
    "        \n",
    "    n_pos = len(positive_examples)\n",
    "    n_neg = len(negative_examples)\n",
    "    ratio = n_pos/n_neg\n",
    "    print(\"# pos: \", n_pos)\n",
    "    print(\"# neg: \", n_neg)\n",
    "    print(\"pos/neg:\", ratio)\n",
    "    x_text = positive_examples + negative_examples\n",
    "\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    return x_text, y, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_dir = \"/usr/lib/mecab/dic/mecab-ipadic-neologd\"\n",
    "#dic_dir = \"/usr/local/lib/mecab/dic/mecab-ipadic-neologd/\"\n",
    "class Tokenizer:\n",
    "    def __init__(self, stopwords, parser=None, include_pos=None, exclude_posdetail=None, exclude_reg=None):\n",
    "    \n",
    "        self.stopwords = stopwords\n",
    "        self.include_pos = include_pos if include_pos else  [\"名詞\", \"動詞\", \"形容詞\"]\n",
    "        self.exclude_posdetail = exclude_posdetail if exclude_posdetail else [\"接尾\", \"数\"]\n",
    "        self.exclude_reg = exclude_reg if exclude_reg else r\"$^\"  # no matching reg\n",
    "        if parser:\n",
    "            self.parser = parser\n",
    "        else:\n",
    "            mecab = MeCab.Tagger(\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/\")\n",
    "            self.parser = mecab.parse\n",
    "            \n",
    "\n",
    "    def tokenize(self, text, show_pos=False):\n",
    "        text = re.sub(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", \"\", text)    #URL\n",
    "        text = re.sub(r\"\\\"?([-a-zA-Z0-9.`?{}]+\\.jp)\\\"?\" ,\"\", text)  # xxx.jp \n",
    "        text = text.lower()\n",
    "        l = [line.split(\"\\t\") for line in self.parser(text).split(\"\\n\")]\n",
    "        res = [\n",
    "            i[2] if not show_pos else (i[2],i[3]) for i in l \n",
    "                if len(i) >=4 # has POS.\n",
    "                    and i[3].split(\"-\")[0] in self.include_pos\n",
    "                    and i[3].split(\"-\")[1] not in self.exclude_posdetail\n",
    "                    and not re.search(r\"(-|−)\\d\", i[2])\n",
    "                    and not re.search(self.exclude_reg, i[2])\n",
    "                    and i[2] not in self.stopwords          \n",
    "            ]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_dir = \"/usr/local/lib/mecab/dic/mecab-ipadic-neologd/\" #mac\n",
    "dic_dir = \"/usr/lib/mecab/dic/mecab-ipadic-neologd\"\n",
    "mecab = MeCab.Tagger(\"-Ochasen -d {}\".format(dic_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['あそこ', 'あたり', 'あちら']\n"
     ]
    }
   ],
   "source": [
    "res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n",
    "stopwords = [line.decode(\"utf-8\").strip() for line in res]\n",
    "print(stopwords[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"you've\", 'z', 'zero']\n"
     ]
    }
   ],
   "source": [
    "res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/English.txt\")\n",
    "stopwords += [line.decode(\"utf-8\").strip() for line in res]\n",
    "print(stopwords[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(stopwords, mecab.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['認める', '自分自身', '若さ故の過ち']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokenize(\"認めたくないものだな。自分自身の若さ故の過ちというものを。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# pos:  62402\n",
      "# neg:  9060\n",
      "pos/neg: 6.887637969094922\n"
     ]
    }
   ],
   "source": [
    "level = \"word\"\n",
    "x_text, y, ratio = load_data_and_labels(positive_data_file, negative_data_file, level=level, lang=\"Ja\", t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['書き込む',\n",
       "  '読み出し',\n",
       "  '転送速度',\n",
       "  'いずれ',\n",
       "  '満足',\n",
       "  '画素',\n",
       "  'コンパクトカメラ',\n",
       "  'タイプ',\n",
       "  'デジカメ',\n",
       "  '入れる',\n",
       "  '撮影',\n",
       "  '使う',\n",
       "  '撮影後',\n",
       "  'カード',\n",
       "  'リーダ',\n",
       "  '接続',\n",
       "  'する',\n",
       "  '撮影',\n",
       "  'する',\n",
       "  '膨大',\n",
       "  '量',\n",
       "  '画像',\n",
       "  'データ',\n",
       "  'サムネイル',\n",
       "  '表示',\n",
       "  'する',\n",
       "  'ピックアップ',\n",
       "  'する',\n",
       "  '画像',\n",
       "  'コピペ',\n",
       "  'する',\n",
       "  'する',\n",
       "  'いる',\n",
       "  'ストレス',\n",
       "  '感じる',\n",
       "  'ない',\n",
       "  '快適',\n",
       "  '使える',\n",
       "  'いる',\n",
       "  '限定',\n",
       "  '個体',\n",
       "  'SDカード',\n",
       "  '本体',\n",
       "  'シンプル',\n",
       "  '小さい',\n",
       "  'ボール紙',\n",
       "  '挟む',\n",
       "  'いる',\n",
       "  '梱包',\n",
       "  'シンプル',\n",
       "  '売価',\n",
       "  '安い',\n",
       "  '性能',\n",
       "  '満足',\n",
       "  '出来る',\n",
       "  'いる',\n",
       "  '買う',\n",
       "  '良い',\n",
       "  '思う',\n",
       "  'いる',\n",
       "  '耐久性',\n",
       "  'わかる',\n",
       "  '経過',\n",
       "  '観察'],\n",
       " ['D6', '使う', '初心者', '1つ', '問題', '使える']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\\n\".join([ \" \".join(x) for x in x_text ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wkachi.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = word2vec.Text8Corpus(\"wkachi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Text8Corpus at 0x7f50e700c908>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-21 15:14:42,471 : INFO : collecting all words and their counts\n",
      "2018-07-21 15:14:42,473 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-21 15:14:43,031 : INFO : collected 56624 word types from a corpus of 1920944 raw words and 193 sentences\n",
      "2018-07-21 15:14:43,032 : INFO : Loading a fresh vocabulary\n",
      "2018-07-21 15:14:43,181 : INFO : effective_min_count=5 retains 19616 unique words (34% of original 56624, drops 37008)\n",
      "2018-07-21 15:14:43,182 : INFO : effective_min_count=5 leaves 1857771 word corpus (96% of original 1920944, drops 63173)\n",
      "2018-07-21 15:14:43,234 : INFO : deleting the raw counts dictionary of 56624 items\n",
      "2018-07-21 15:14:43,236 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2018-07-21 15:14:43,237 : INFO : downsampling leaves estimated 1526415 word corpus (82.2% of prior 1857771)\n",
      "2018-07-21 15:14:43,283 : INFO : estimated required memory for 19616 words and 100 dimensions: 25500800 bytes\n",
      "2018-07-21 15:14:43,283 : INFO : resetting layer weights\n",
      "2018-07-21 15:14:43,484 : INFO : training model with 3 workers on 19616 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-07-21 15:14:44,504 : INFO : EPOCH 1 - PROGRESS: at 80.31% examples, 1226923 words/s, in_qsize 5, out_qsize 0\n",
      "2018-07-21 15:14:44,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-21 15:14:44,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-21 15:14:44,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-21 15:14:44,727 : INFO : EPOCH - 1 : training on 1920944 raw words (1526294 effective words) took 1.2s, 1244904 effective words/s\n",
      "2018-07-21 15:14:45,735 : INFO : EPOCH 2 - PROGRESS: at 84.46% examples, 1286193 words/s, in_qsize 5, out_qsize 0\n",
      "2018-07-21 15:14:45,895 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-21 15:14:45,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-21 15:14:45,903 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-21 15:14:45,904 : INFO : EPOCH - 2 : training on 1920944 raw words (1526389 effective words) took 1.2s, 1299113 effective words/s\n",
      "2018-07-21 15:14:46,909 : INFO : EPOCH 3 - PROGRESS: at 85.49% examples, 1305523 words/s, in_qsize 5, out_qsize 0\n",
      "2018-07-21 15:14:47,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-21 15:14:47,052 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-21 15:14:47,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-21 15:14:47,061 : INFO : EPOCH - 3 : training on 1920944 raw words (1526347 effective words) took 1.2s, 1321743 effective words/s\n",
      "2018-07-21 15:14:48,064 : INFO : EPOCH 4 - PROGRESS: at 81.87% examples, 1254296 words/s, in_qsize 5, out_qsize 0\n",
      "2018-07-21 15:14:48,259 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-21 15:14:48,262 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-21 15:14:48,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-21 15:14:48,266 : INFO : EPOCH - 4 : training on 1920944 raw words (1526656 effective words) took 1.2s, 1269409 effective words/s\n",
      "2018-07-21 15:14:49,272 : INFO : EPOCH 5 - PROGRESS: at 65.80% examples, 1004007 words/s, in_qsize 6, out_qsize 0\n",
      "2018-07-21 15:14:49,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-21 15:14:49,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-21 15:14:49,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-21 15:14:49,757 : INFO : EPOCH - 5 : training on 1920944 raw words (1525777 effective words) took 1.5s, 1024879 effective words/s\n",
      "2018-07-21 15:14:49,758 : INFO : training on a 9604720 raw words (7631463 effective words) took 6.3s, 1216427 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-21 15:15:45,277 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('スピード', 0.8742904663085938)\n",
      "('読み込み', 0.8708919882774353)\n",
      "('速い', 0.8581838607788086)\n",
      "('USB3.0', 0.8490318059921265)\n",
      "('書き込み', 0.8419095277786255)\n",
      "('pro', 0.8294757008552551)\n",
      "('通信速度', 0.8217605352401733)\n",
      "('SPEC', 0.8190112113952637)\n",
      "('転送速度', 0.8144389390945435)\n",
      "('HDD', 0.8089838027954102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "results = model.wv.most_similar(positive=[\"速度\"])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'コンパクトカメラ' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-11bc8ff5e6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"コンパクトカメラ\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'コンパクトカメラ' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "len(model.wv[\"コンパクトカメラ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./amazon_ja_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"./amazon_ja_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"速度\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/amazon_ja/pos.txt\") as f:\n",
    "    pos_doc = [t.tokenize(doc) for doc in tqdm(f.readlines())]\n",
    "print(pos_doc[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.wv[d] for d in pos_doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
