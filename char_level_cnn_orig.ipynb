{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xiang Zhang, Junbo Zhao, Yann LeCun: Character-level Convolutional Networks for Text Classification   \n",
    "(https://arxiv.org/abs/1509.01626)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm is notebook version\n",
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "try:\n",
    "    if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        print(\"tqdm is notebook version\")\n",
    "        from tqdm import tqdm_notebook as tqdm\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "except (NameError, RuntimeError):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_end(char_seq):\n",
    "    char_seq_length = 600 #1014\n",
    "    if len(char_seq) > char_seq_length:\n",
    "        char_seq = char_seq[-char_seq_length:]\n",
    "    return char_seq\n",
    "\n",
    "\n",
    "def pad_sentence(char_seq, padding_char=\" \"):\n",
    "    char_seq_length = 600\n",
    "    num_padding = char_seq_length - len(char_seq)\n",
    "    new_char_seq = char_seq + [padding_char] * num_padding\n",
    "    return new_char_seq\n",
    "\n",
    "\n",
    "def string_to_int8_conversion(char_seq, alphabet):\n",
    "    x = np.array([alphabet.find(char) for char in char_seq], dtype=np.int8)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_batched_one_hot(char_seqs_indices, labels, start_index, end_index):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\"\n",
    "    x_batch = char_seqs_indices[start_index:end_index]\n",
    "    y_batch = labels[start_index:end_index]\n",
    "    x_batch_one_hot = np.zeros(shape=[len(x_batch), len(alphabet), len(x_batch[0]), 1])\n",
    "    for example_i, char_seq_indices in enumerate(x_batch):\n",
    "        for char_pos_in_seq, char_seq_char_ind in enumerate(char_seq_indices):\n",
    "            if char_seq_char_ind != -1:\n",
    "                x_batch_one_hot[example_i][char_seq_char_ind][char_pos_in_seq][0] = 1\n",
    "    return [x_batch_one_hot, y_batch]\n",
    "\n",
    "\n",
    "def batch_iter(x, y, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    # data = np.array(data)\n",
    "    data_size = len(x)\n",
    "    num_batches_per_epoch = int(data_size/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"In epoch >> \" + str(epoch + 1))\n",
    "        print(\"num batches per epoch is: \" + str(num_batches_per_epoch))\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            x_shuffled = x[shuffle_indices]\n",
    "            y_shuffled = y[shuffle_indices]\n",
    "        else:\n",
    "            x_shuffled = x\n",
    "            y_shuffled = y\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            x_batch, y_batch = get_batched_one_hot(x_shuffled, y_shuffled, start_index, end_index)\n",
    "            batch = list(zip(x_batch, y_batch))\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "evaluate_every = 5000\n",
    "checkpoint_every = 1000\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True #\"Allow device soft device placement\")\n",
    "log_device_placement = False  #, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_file = \"data/amazon/book_pos.txt\"\n",
    "negative_data_file = \"data/amazon/book_neg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sample_percentage = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(file_path, examples, raw):\n",
    "    with open(file_path) as f:\n",
    "        for line in tqdm(f):\n",
    "            text = line\n",
    "            raw.append(text)\n",
    "            text_end_extracted = extract_end(list(text.lower()))\n",
    "            padded = pad_sentence(text_end_extracted)\n",
    "            text_int8_repr = string_to_int8_conversion(padded, alphabet)           \n",
    "            examples.append(text_int8_repr)\n",
    "    return examples, raw\n",
    "\n",
    "def load_data(positive_file_path, negative_data_file, alphabet):\n",
    "    pos_examples = [] \n",
    "    pos_raw = []\n",
    "    neg_examples = []\n",
    "    neg_raw = []\n",
    "    \n",
    "    _load_data(positive_data_file, pos_examples, pos_raw)\n",
    "    _load_data(negative_data_file, neg_examples, neg_raw)\n",
    "    \n",
    "    n_posi = len(pos_examples)\n",
    "    print(\"# of positive\", n_posi)\n",
    "    n_neg = len(neg_examples)\n",
    "    print(\"# of negative\", n_neg)\n",
    "    \n",
    "    positive_labels = [[0, 1] for _ in range(n_posi)]\n",
    "    negative_labels = [[1, 0] for _ in range(n_neg)]\n",
    "    \n",
    "    labels = positive_labels + negative_labels\n",
    "    examples = pos_examples + neg_examples\n",
    "    \n",
    "    return np.array(examples, dtype=np.int8), np.array(labels, dtype=np.int8), pos_raw+neg_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893e5d542e904d31920f4c08d3047e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551f812ac35547e8916ca47f36311a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of positive 500000\n",
      "# of negative 500000\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"e6dd9068-5a0a-4316-9bab-d3ebd88c5fa6\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"e6dd9068-5a0a-4316-9bab-d3ebd88c5fa6\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x, y, raw = load_data(positive_data_file, negative_data_file, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  7,  4, -1, 18, 19, 14, 17, 24, 11,  8, 13,  4, -1,  8, 18, -1,\n",
       "        6, 17,  4,  0, 19, 39, -1, -1, 22, 14, 20, 11,  3, -1,  7,  0, 21,\n",
       "        4, -1,  1,  4,  4, 13, -1, 12, 20,  2,  7, -1, 12, 14, 17,  4, -1,\n",
       "        4, 13,  9, 14, 24,  0,  1, 11,  4, -1,  8,  5, -1,  8, 19, -1,  7,\n",
       "        0,  3, -1,  1,  4,  4, 13, -1,  1,  4, 19, 19,  4, 17, -1, 15, 17,\n",
       "       14, 14,  5,  4,  3, -1, 36, -1, 19,  7,  4, -1, 12,  0, 13, 24, -1,\n",
       "       19, 24, 15, 14, 18, -1,  0, 13,  3, -1, 18, 24, 13, 19,  0, 23, -1,\n",
       "        4, 17, 17, 14, 17, 18, -1,  0, 13,  3, -1,  8, 13,  2, 14, 17, 17,\n",
       "        4,  2, 19, -1, 20, 18,  4, -1, 14,  5, -1, 22, 14, 17,  3, 18, -1,\n",
       "       12,  0,  3,  4, -1,  5, 14, 17, -1,  0, -1, 18, 11, 14, 15, 15, 24,\n",
       "       -1,  0, 13,  3, -1,  3,  8, 18, 19, 17,  0,  2, 19,  8, 13,  6, -1,\n",
       "       17,  4,  0,  3, 39, -1, -1,  8, 13, -1, 18, 15,  8, 19,  4, -1, 14,\n",
       "        5, -1, 19,  7,  8, 18, -1,  0, 12, -1, 11, 14, 14, 10,  8, 13,  6,\n",
       "       -1,  5, 14, 17, 22,  0, 17,  3, -1, 19, 14, -1, 19,  7,  4, -1, 18,\n",
       "        4, 16, 20,  4, 11, 39, 69, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The storyline is great.  Would have been much more enjoyable if it had been better proofed - the many typos and syntax errors and incorrect use of words made for a sloppy and distracting read.  In spite of this am looking forward to the sequel.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 600)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514.629429"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_list = np.array([len(r)for r in raw])\n",
    "length_list.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length\n",
       "0     116\n",
       "1     245\n",
       "2     368\n",
       "3     131\n",
       "4     139"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>514.629429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>682.247415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>273.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>570.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29656.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               length\n",
       "count  1000000.000000\n",
       "mean       514.629429\n",
       "std        682.247415\n",
       "min          1.000000\n",
       "25%        155.000000\n",
       "50%        273.000000\n",
       "75%        570.000000\n",
       "max      29656.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x182ea51a20>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1827227be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 999500/500\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"df2f410b-8f00-4752-8108-cb4f572be627\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"df2f410b-8f00-4752-8108-cb4f572be627\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "#del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    based on the Character-level Convolutional Networks for Text Classification paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, filter_sizes=(7, 7, 3, 3, 3, 3), num_filters_per_size=256,\n",
    "                 l2_reg_lambda=0.0, sequence_max_length=1014, num_quantized_chars=70):\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, num_quantized_chars, sequence_max_length, 1], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "    \n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # ================ Layer 1 ================\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            filter_shape = [num_quantized_chars, filter_sizes[0], 1, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(self.input_x, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv1\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, 1, 3, 1], strides=[1, 1, 3, 1], padding='VALID', name=\"pool1\")\n",
    "\n",
    "        # ================ Layer 2 ================\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            filter_shape = [1, filter_sizes[1], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(pooled, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv2\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, 1, 3, 1], strides=[1, 1, 3, 1], padding='VALID', name=\"pool2\")\n",
    "\n",
    "        # ================ Layer 3 ================\n",
    "        with tf.name_scope(\"conv-3\"):\n",
    "            filter_shape = [1, filter_sizes[2], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(pooled, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv3\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 4 ================\n",
    "        with tf.name_scope(\"conv-4\"):\n",
    "            filter_shape = [1, filter_sizes[3], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv4\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 5 ================\n",
    "        with tf.name_scope(\"conv-5\"):\n",
    "            filter_shape = [1, filter_sizes[4], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv5\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 6 ================\n",
    "        with tf.name_scope(\"conv-maxpool-6\"):\n",
    "            filter_shape = [1, filter_sizes[5], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv6\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, 1, 3, 1], strides=[1, 1, 3, 1], padding='VALID', name=\"pool6\")\n",
    "\n",
    "        # ================ Layer 7 ================\n",
    "        num_features_total = num_filters_per_size * 18 #34\n",
    "        print(\"######\")\n",
    "        print(num_features_total)\n",
    "        print(pooled.shape)\n",
    "        print(\"#############\")\n",
    "        h_pool_flat = tf.reshape(pooled, [-1, num_features_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout-1\"):\n",
    "            drop1 = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        with tf.name_scope(\"fc-1\"):\n",
    "            size = 64#1024\n",
    "            W = tf.Variable(tf.truncated_normal([num_features_total, size], stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[size]), name=\"b\")\n",
    "            # l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "            fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(drop1, W, b), name=\"fc-1-out\")\n",
    "\n",
    "        # ================ Layer 8 ================\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout-2\"):\n",
    "            drop2 = tf.nn.dropout(fc_1_output, self.dropout_keep_prob)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        with tf.name_scope(\"fc-2\"):\n",
    "            W = tf.Variable(tf.truncated_normal([size, size], stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[size]), name=\"b\")\n",
    "            fc_2_output = tf.nn.relu(tf.nn.xw_plus_b(drop2, W, b), name=\"fc-2-out\")\n",
    "\n",
    "        # ================ Layer 9 ================\n",
    "        # Fully connected layer 3\n",
    "        with tf.name_scope(\"fc-3\"):\n",
    "            W = tf.Variable(tf.truncated_normal([size, num_classes], stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            scores = tf.nn.xw_plus_b(fc_2_output, W, b, name=\"output\")\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            \n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######\n",
      "4608\n",
      "(?, 1, 18, 256)\n",
      "#############\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-3/W:0/grad/hist is illegal; using conv-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-3/W:0/grad/sparsity is illegal; using conv-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-3/b:0/grad/hist is illegal; using conv-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-3/b:0/grad/sparsity is illegal; using conv-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-4/W:0/grad/hist is illegal; using conv-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-4/W:0/grad/sparsity is illegal; using conv-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-4/b:0/grad/hist is illegal; using conv-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-4/b:0/grad/sparsity is illegal; using conv-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-5/W:0/grad/hist is illegal; using conv-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-5/W:0/grad/sparsity is illegal; using conv-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-5/b:0/grad/hist is illegal; using conv-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-5/b:0/grad/sparsity is illegal; using conv-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/hist is illegal; using conv-maxpool-6/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/sparsity is illegal; using conv-maxpool-6/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/hist is illegal; using conv-maxpool-6/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/sparsity is illegal; using conv-maxpool-6/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/hist is illegal; using fc-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/sparsity is illegal; using fc-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/hist is illegal; using fc-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/sparsity is illegal; using fc-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/hist is illegal; using fc-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/sparsity is illegal; using fc-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/hist is illegal; using fc-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/sparsity is illegal; using fc-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/hist is illegal; using fc-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/sparsity is illegal; using fc-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/hist is illegal; using fc-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/sparsity is illegal; using fc-3/b_0/grad/sparsity instead.\n",
      "Writing to /Users/tdual/Workspace/char_level_cnn/runs/1525399442\n",
      "\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 7809\n",
      "2018-05-04T11:04:05.208927: step 1, loss 0.691888, acc 0.515625\n",
      "2018-05-04T11:04:06.956417: step 2, loss 0.727205, acc 0.53125\n",
      "2018-05-04T11:04:08.818196: step 3, loss 0.714509, acc 0.507812\n",
      "2018-05-04T11:04:10.620199: step 4, loss 0.69615, acc 0.53125\n",
      "2018-05-04T11:04:12.552249: step 5, loss 0.708242, acc 0.53125\n",
      "2018-05-04T11:04:14.410452: step 6, loss 0.69288, acc 0.523438\n",
      "2018-05-04T11:04:16.277157: step 7, loss 0.700344, acc 0.492188\n",
      "2018-05-04T11:04:18.097916: step 8, loss 0.716876, acc 0.554688\n",
      "2018-05-04T11:04:19.855488: step 9, loss 0.72846, acc 0.476562\n",
      "2018-05-04T11:04:21.648133: step 10, loss 0.709585, acc 0.460938\n",
      "2018-05-04T11:04:23.494233: step 11, loss 0.698914, acc 0.46875\n",
      "2018-05-04T11:04:25.324999: step 12, loss 0.681544, acc 0.59375\n",
      "2018-05-04T11:04:27.122585: step 13, loss 0.699394, acc 0.476562\n",
      "2018-05-04T11:04:28.996238: step 14, loss 0.698362, acc 0.5\n",
      "2018-05-04T11:04:31.085239: step 15, loss 0.696554, acc 0.46875\n",
      "2018-05-04T11:04:33.056243: step 16, loss 0.694686, acc 0.539062\n",
      "2018-05-04T11:04:35.128627: step 17, loss 0.701262, acc 0.5\n",
      "2018-05-04T11:04:37.051198: step 18, loss 0.690158, acc 0.570312\n",
      "2018-05-04T11:04:39.021736: step 19, loss 0.702201, acc 0.453125\n",
      "2018-05-04T11:04:40.964546: step 20, loss 0.684149, acc 0.539062\n",
      "2018-05-04T11:04:42.944262: step 21, loss 0.700704, acc 0.476562\n",
      "2018-05-04T11:04:44.893180: step 22, loss 0.701647, acc 0.40625\n",
      "2018-05-04T11:04:46.768128: step 23, loss 0.702284, acc 0.507812\n",
      "2018-05-04T11:04:48.740786: step 24, loss 0.692762, acc 0.507812\n",
      "2018-05-04T11:04:50.757753: step 25, loss 0.702257, acc 0.460938\n",
      "2018-05-04T11:04:52.773516: step 26, loss 0.686393, acc 0.570312\n",
      "2018-05-04T11:04:54.697137: step 27, loss 0.707383, acc 0.429688\n",
      "2018-05-04T11:04:56.701532: step 28, loss 0.687543, acc 0.546875\n",
      "2018-05-04T11:04:58.545503: step 29, loss 0.70133, acc 0.484375\n",
      "2018-05-04T11:05:00.532133: step 30, loss 0.683322, acc 0.585938\n",
      "2018-05-04T11:05:02.447616: step 31, loss 0.694502, acc 0.5\n",
      "2018-05-04T11:05:04.313277: step 32, loss 0.693353, acc 0.5\n",
      "2018-05-04T11:05:06.142587: step 33, loss 0.711155, acc 0.4375\n",
      "2018-05-04T11:05:07.877315: step 34, loss 0.684805, acc 0.554688\n",
      "2018-05-04T11:05:09.675415: step 35, loss 0.700216, acc 0.484375\n",
      "2018-05-04T11:05:11.653858: step 36, loss 0.693395, acc 0.476562\n",
      "2018-05-04T11:05:13.461251: step 37, loss 0.687651, acc 0.609375\n",
      "2018-05-04T11:05:15.293598: step 38, loss 0.698666, acc 0.507812\n",
      "2018-05-04T11:05:17.223442: step 39, loss 0.694742, acc 0.5\n",
      "2018-05-04T11:05:19.131648: step 40, loss 0.695516, acc 0.53125\n",
      "2018-05-04T11:05:21.025350: step 41, loss 0.697057, acc 0.507812\n",
      "2018-05-04T11:05:22.865281: step 42, loss 0.698432, acc 0.484375\n",
      "2018-05-04T11:05:24.729431: step 43, loss 0.689639, acc 0.523438\n",
      "2018-05-04T11:05:26.506366: step 44, loss 0.702972, acc 0.398438\n",
      "2018-05-04T11:05:28.267234: step 45, loss 0.698708, acc 0.5\n",
      "2018-05-04T11:05:30.036174: step 46, loss 0.698309, acc 0.515625\n",
      "2018-05-04T11:05:31.830354: step 47, loss 0.700845, acc 0.4375\n",
      "2018-05-04T11:05:33.785795: step 48, loss 0.69605, acc 0.429688\n",
      "2018-05-04T11:05:35.878974: step 49, loss 0.697563, acc 0.484375\n",
      "2018-05-04T11:05:37.807833: step 50, loss 0.692208, acc 0.507812\n",
      "2018-05-04T11:05:39.711416: step 51, loss 0.69201, acc 0.53125\n",
      "2018-05-04T11:05:41.720556: step 52, loss 0.692738, acc 0.5\n",
      "2018-05-04T11:05:43.610175: step 53, loss 0.691942, acc 0.570312\n",
      "2018-05-04T11:05:45.502346: step 54, loss 0.695476, acc 0.453125\n",
      "2018-05-04T11:05:47.423744: step 55, loss 0.695405, acc 0.445312\n",
      "2018-05-04T11:05:49.287305: step 56, loss 0.691731, acc 0.507812\n",
      "2018-05-04T11:05:51.252028: step 57, loss 0.696925, acc 0.453125\n",
      "2018-05-04T11:05:53.161381: step 58, loss 0.692293, acc 0.53125\n",
      "2018-05-04T11:05:55.068080: step 59, loss 0.694652, acc 0.5625\n",
      "2018-05-04T11:05:56.915459: step 60, loss 0.697686, acc 0.46875\n",
      "2018-05-04T11:05:58.793145: step 61, loss 0.693267, acc 0.476562\n",
      "2018-05-04T11:06:00.758229: step 62, loss 0.697231, acc 0.445312\n",
      "2018-05-04T11:06:02.707521: step 63, loss 0.691995, acc 0.476562\n",
      "2018-05-04T11:06:04.750123: step 64, loss 0.695176, acc 0.492188\n",
      "2018-05-04T11:06:06.591781: step 65, loss 0.696221, acc 0.453125\n",
      "2018-05-04T11:06:08.487308: step 66, loss 0.694478, acc 0.476562\n",
      "2018-05-04T11:06:10.467913: step 67, loss 0.697681, acc 0.414062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:06:12.518043: step 68, loss 0.694034, acc 0.5\n",
      "2018-05-04T11:06:14.393821: step 69, loss 0.696816, acc 0.445312\n",
      "2018-05-04T11:06:16.478662: step 70, loss 0.695648, acc 0.4375\n",
      "2018-05-04T11:06:18.705520: step 71, loss 0.690839, acc 0.5625\n",
      "2018-05-04T11:06:20.986909: step 72, loss 0.693281, acc 0.515625\n",
      "2018-05-04T11:06:23.242042: step 73, loss 0.691592, acc 0.539062\n",
      "2018-05-04T11:06:25.667599: step 74, loss 0.691754, acc 0.5625\n",
      "2018-05-04T11:06:28.170972: step 75, loss 0.694833, acc 0.492188\n",
      "2018-05-04T11:06:30.795499: step 76, loss 0.692072, acc 0.507812\n",
      "2018-05-04T11:06:33.613756: step 77, loss 0.686937, acc 0.5625\n",
      "2018-05-04T11:06:36.305230: step 78, loss 0.69454, acc 0.484375\n",
      "2018-05-04T11:06:38.952216: step 79, loss 0.695916, acc 0.5\n",
      "2018-05-04T11:06:41.727134: step 80, loss 0.693053, acc 0.539062\n",
      "2018-05-04T11:06:44.596163: step 81, loss 0.703147, acc 0.367188\n",
      "2018-05-04T11:06:48.325178: step 82, loss 0.697387, acc 0.460938\n",
      "2018-05-04T11:06:53.561481: step 83, loss 0.697874, acc 0.421875\n",
      "2018-05-04T11:07:01.976680: step 84, loss 0.695349, acc 0.484375\n",
      "2018-05-04T11:07:22.590885: step 85, loss 0.691994, acc 0.554688\n",
      "2018-05-04T11:07:35.931020: step 86, loss 0.69575, acc 0.484375\n",
      "2018-05-04T11:07:43.225315: step 87, loss 0.694783, acc 0.523438\n",
      "2018-05-04T11:07:50.140171: step 88, loss 0.692567, acc 0.476562\n",
      "2018-05-04T11:07:54.224963: step 89, loss 0.689685, acc 0.546875\n",
      "2018-05-04T11:07:57.366397: step 90, loss 0.694039, acc 0.523438\n",
      "2018-05-04T11:08:00.028507: step 91, loss 0.695756, acc 0.515625\n",
      "2018-05-04T11:08:02.402384: step 92, loss 0.689766, acc 0.554688\n",
      "2018-05-04T11:08:04.594686: step 93, loss 0.69727, acc 0.460938\n",
      "2018-05-04T11:08:06.735567: step 94, loss 0.694764, acc 0.453125\n",
      "2018-05-04T11:08:08.725101: step 95, loss 0.692546, acc 0.507812\n",
      "2018-05-04T11:08:10.651556: step 96, loss 0.694401, acc 0.515625\n",
      "2018-05-04T11:08:12.444104: step 97, loss 0.699587, acc 0.476562\n",
      "2018-05-04T11:08:14.344444: step 98, loss 0.68873, acc 0.546875\n",
      "2018-05-04T11:08:16.173624: step 99, loss 0.699853, acc 0.453125\n",
      "2018-05-04T11:08:17.975620: step 100, loss 0.690668, acc 0.546875\n",
      "2018-05-04T11:08:19.742651: step 101, loss 0.703464, acc 0.46875\n",
      "2018-05-04T11:08:21.478902: step 102, loss 0.699259, acc 0.421875\n",
      "2018-05-04T11:08:23.224663: step 103, loss 0.700194, acc 0.429688\n",
      "2018-05-04T11:08:24.933699: step 104, loss 0.688688, acc 0.601562\n",
      "2018-05-04T11:08:26.679635: step 105, loss 0.689061, acc 0.53125\n",
      "2018-05-04T11:08:28.382962: step 106, loss 0.694155, acc 0.46875\n",
      "2018-05-04T11:08:30.080514: step 107, loss 0.694415, acc 0.429688\n",
      "2018-05-04T11:08:31.776616: step 108, loss 0.692989, acc 0.53125\n",
      "2018-05-04T11:08:33.483757: step 109, loss 0.691852, acc 0.523438\n",
      "2018-05-04T11:08:35.225496: step 110, loss 0.693547, acc 0.476562\n",
      "2018-05-04T11:08:36.960207: step 111, loss 0.691207, acc 0.5\n",
      "2018-05-04T11:08:38.684623: step 112, loss 0.694901, acc 0.5\n",
      "2018-05-04T11:08:40.510886: step 113, loss 0.692011, acc 0.523438\n",
      "2018-05-04T11:08:42.395452: step 114, loss 0.695902, acc 0.515625\n",
      "2018-05-04T11:08:44.292667: step 115, loss 0.693649, acc 0.421875\n",
      "2018-05-04T11:08:46.336784: step 116, loss 0.69203, acc 0.53125\n",
      "2018-05-04T11:08:48.286926: step 117, loss 0.693924, acc 0.492188\n",
      "2018-05-04T11:08:50.088872: step 118, loss 0.69474, acc 0.460938\n",
      "2018-05-04T11:08:51.876007: step 119, loss 0.69796, acc 0.460938\n",
      "2018-05-04T11:08:53.788102: step 120, loss 0.695087, acc 0.523438\n",
      "2018-05-04T11:08:55.490001: step 121, loss 0.695116, acc 0.476562\n",
      "2018-05-04T11:08:57.414751: step 122, loss 0.692027, acc 0.539062\n",
      "2018-05-04T11:08:59.296345: step 123, loss 0.695443, acc 0.46875\n",
      "2018-05-04T11:09:01.273881: step 124, loss 0.695751, acc 0.453125\n",
      "2018-05-04T11:09:02.985243: step 125, loss 0.693526, acc 0.507812\n",
      "2018-05-04T11:09:04.705985: step 126, loss 0.693386, acc 0.5\n",
      "2018-05-04T11:09:06.399588: step 127, loss 0.693855, acc 0.507812\n",
      "2018-05-04T11:09:08.096183: step 128, loss 0.690562, acc 0.546875\n",
      "2018-05-04T11:09:09.804135: step 129, loss 0.695282, acc 0.414062\n",
      "2018-05-04T11:09:11.538294: step 130, loss 0.69197, acc 0.515625\n",
      "2018-05-04T11:09:13.251287: step 131, loss 0.69203, acc 0.554688\n",
      "2018-05-04T11:09:15.036642: step 132, loss 0.689304, acc 0.585938\n",
      "2018-05-04T11:09:16.742619: step 133, loss 0.691671, acc 0.5625\n",
      "2018-05-04T11:09:18.433395: step 134, loss 0.697951, acc 0.445312\n",
      "2018-05-04T11:09:20.145517: step 135, loss 0.693408, acc 0.484375\n",
      "2018-05-04T11:09:21.931141: step 136, loss 0.693995, acc 0.523438\n",
      "2018-05-04T11:09:23.630244: step 137, loss 0.692343, acc 0.539062\n",
      "2018-05-04T11:09:25.347859: step 138, loss 0.696094, acc 0.453125\n",
      "2018-05-04T11:09:27.049905: step 139, loss 0.69547, acc 0.46875\n",
      "2018-05-04T11:09:28.736226: step 140, loss 0.690385, acc 0.5625\n",
      "2018-05-04T11:09:30.425355: step 141, loss 0.689032, acc 0.523438\n",
      "2018-05-04T11:09:32.170607: step 142, loss 0.700532, acc 0.429688\n",
      "2018-05-04T11:09:33.865875: step 143, loss 0.693626, acc 0.5\n",
      "2018-05-04T11:09:35.576288: step 144, loss 0.685952, acc 0.59375\n",
      "2018-05-04T11:09:37.281997: step 145, loss 0.691681, acc 0.523438\n",
      "2018-05-04T11:09:38.970843: step 146, loss 0.696572, acc 0.421875\n",
      "2018-05-04T11:09:40.670757: step 147, loss 0.693346, acc 0.507812\n",
      "2018-05-04T11:09:42.433340: step 148, loss 0.693022, acc 0.523438\n",
      "2018-05-04T11:09:44.137827: step 149, loss 0.691713, acc 0.5\n",
      "2018-05-04T11:09:45.897180: step 150, loss 0.693742, acc 0.453125\n",
      "2018-05-04T11:09:47.607845: step 151, loss 0.695534, acc 0.484375\n",
      "2018-05-04T11:09:49.296582: step 152, loss 0.693115, acc 0.53125\n",
      "2018-05-04T11:09:50.977125: step 153, loss 0.693315, acc 0.46875\n",
      "2018-05-04T11:09:52.752060: step 154, loss 0.691339, acc 0.5625\n",
      "2018-05-04T11:09:54.454909: step 155, loss 0.691963, acc 0.5\n",
      "2018-05-04T11:09:56.150446: step 156, loss 0.693822, acc 0.460938\n",
      "2018-05-04T11:09:57.846302: step 157, loss 0.693197, acc 0.5\n",
      "2018-05-04T11:09:59.546117: step 158, loss 0.691648, acc 0.5\n",
      "2018-05-04T11:10:01.231616: step 159, loss 0.693621, acc 0.492188\n",
      "2018-05-04T11:10:03.012775: step 160, loss 0.694317, acc 0.46875\n",
      "2018-05-04T11:10:04.710637: step 161, loss 0.694294, acc 0.507812\n",
      "2018-05-04T11:10:06.444683: step 162, loss 0.69333, acc 0.515625\n",
      "2018-05-04T11:10:08.152306: step 163, loss 0.691297, acc 0.539062\n",
      "2018-05-04T11:10:09.834482: step 164, loss 0.696553, acc 0.421875\n",
      "2018-05-04T11:10:11.597478: step 165, loss 0.691899, acc 0.523438\n",
      "2018-05-04T11:10:13.305524: step 166, loss 0.691138, acc 0.554688\n",
      "2018-05-04T11:10:15.002076: step 167, loss 0.696152, acc 0.460938\n",
      "2018-05-04T11:10:16.751171: step 168, loss 0.691874, acc 0.539062\n",
      "2018-05-04T11:10:18.453708: step 169, loss 0.690202, acc 0.554688\n",
      "2018-05-04T11:10:20.152185: step 170, loss 0.692871, acc 0.5\n",
      "2018-05-04T11:10:21.871946: step 171, loss 0.695891, acc 0.46875\n",
      "2018-05-04T11:10:23.574691: step 172, loss 0.693953, acc 0.460938\n",
      "2018-05-04T11:10:25.273207: step 173, loss 0.692907, acc 0.507812\n",
      "2018-05-04T11:10:26.966414: step 174, loss 0.695359, acc 0.5\n",
      "2018-05-04T11:10:28.671208: step 175, loss 0.697356, acc 0.429688\n",
      "2018-05-04T11:10:30.360416: step 176, loss 0.697274, acc 0.445312\n",
      "2018-05-04T11:10:32.122001: step 177, loss 0.690272, acc 0.523438\n",
      "2018-05-04T11:10:33.936737: step 178, loss 0.696369, acc 0.4375\n",
      "2018-05-04T11:10:35.743494: step 179, loss 0.691795, acc 0.5\n",
      "2018-05-04T11:10:37.467735: step 180, loss 0.692621, acc 0.492188\n",
      "2018-05-04T11:10:39.172209: step 181, loss 0.699317, acc 0.4375\n",
      "2018-05-04T11:10:40.902496: step 182, loss 0.695328, acc 0.453125\n",
      "2018-05-04T11:10:42.622222: step 183, loss 0.69377, acc 0.484375\n",
      "2018-05-04T11:10:44.331551: step 184, loss 0.694467, acc 0.5\n",
      "2018-05-04T11:10:46.026549: step 185, loss 0.69399, acc 0.5\n",
      "2018-05-04T11:10:47.805633: step 186, loss 0.695293, acc 0.492188\n",
      "2018-05-04T11:10:49.501380: step 187, loss 0.693873, acc 0.507812\n",
      "2018-05-04T11:10:51.254748: step 188, loss 0.692715, acc 0.523438\n",
      "2018-05-04T11:10:52.959554: step 189, loss 0.696438, acc 0.398438\n",
      "2018-05-04T11:10:54.668195: step 190, loss 0.691, acc 0.59375\n",
      "2018-05-04T11:10:56.418353: step 191, loss 0.69739, acc 0.460938\n",
      "2018-05-04T11:10:58.170236: step 192, loss 0.693619, acc 0.476562\n",
      "2018-05-04T11:10:59.919757: step 193, loss 0.692356, acc 0.507812\n",
      "2018-05-04T11:11:01.762202: step 194, loss 0.692386, acc 0.554688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:11:03.560538: step 195, loss 0.692523, acc 0.515625\n",
      "2018-05-04T11:11:05.362877: step 196, loss 0.69363, acc 0.484375\n",
      "2018-05-04T11:11:07.150291: step 197, loss 0.694556, acc 0.484375\n",
      "2018-05-04T11:11:08.947017: step 198, loss 0.693419, acc 0.5\n",
      "2018-05-04T11:11:10.782138: step 199, loss 0.693188, acc 0.507812\n",
      "2018-05-04T11:11:12.612528: step 200, loss 0.693395, acc 0.492188\n",
      "2018-05-04T11:11:14.471049: step 201, loss 0.69422, acc 0.445312\n",
      "2018-05-04T11:11:16.325222: step 202, loss 0.693269, acc 0.46875\n",
      "2018-05-04T11:11:18.235059: step 203, loss 0.693111, acc 0.492188\n",
      "2018-05-04T11:11:20.141450: step 204, loss 0.692802, acc 0.539062\n",
      "2018-05-04T11:11:22.105323: step 205, loss 0.693492, acc 0.53125\n",
      "2018-05-04T11:11:23.989594: step 206, loss 0.694628, acc 0.398438\n",
      "2018-05-04T11:11:25.877650: step 207, loss 0.692601, acc 0.609375\n",
      "2018-05-04T11:11:27.785540: step 208, loss 0.692119, acc 0.554688\n",
      "2018-05-04T11:11:29.639226: step 209, loss 0.694495, acc 0.453125\n",
      "2018-05-04T11:11:31.550978: step 210, loss 0.693894, acc 0.515625\n",
      "2018-05-04T11:11:33.469028: step 211, loss 0.691762, acc 0.546875\n",
      "2018-05-04T11:11:35.437849: step 212, loss 0.691614, acc 0.554688\n",
      "2018-05-04T11:11:37.479679: step 213, loss 0.692284, acc 0.554688\n",
      "2018-05-04T11:11:39.521016: step 214, loss 0.694149, acc 0.4375\n",
      "2018-05-04T11:11:41.557358: step 215, loss 0.693362, acc 0.507812\n",
      "2018-05-04T11:11:43.598583: step 216, loss 0.690864, acc 0.585938\n",
      "2018-05-04T11:11:45.670904: step 217, loss 0.69348, acc 0.515625\n",
      "2018-05-04T11:11:47.706106: step 218, loss 0.694332, acc 0.445312\n",
      "2018-05-04T11:11:49.840418: step 219, loss 0.696507, acc 0.445312\n",
      "2018-05-04T11:11:51.958441: step 220, loss 0.698047, acc 0.40625\n",
      "2018-05-04T11:11:54.089617: step 221, loss 0.691725, acc 0.5\n",
      "2018-05-04T11:11:56.195970: step 222, loss 0.692541, acc 0.539062\n",
      "2018-05-04T11:11:58.293238: step 223, loss 0.694342, acc 0.476562\n",
      "2018-05-04T11:12:00.403427: step 224, loss 0.693841, acc 0.484375\n",
      "2018-05-04T11:12:02.560992: step 225, loss 0.690867, acc 0.546875\n",
      "2018-05-04T11:12:04.697907: step 226, loss 0.693987, acc 0.460938\n",
      "2018-05-04T11:12:06.799102: step 227, loss 0.695315, acc 0.429688\n",
      "2018-05-04T11:12:08.918507: step 228, loss 0.69073, acc 0.578125\n",
      "2018-05-04T11:12:11.063583: step 229, loss 0.692042, acc 0.578125\n",
      "2018-05-04T11:12:13.234540: step 230, loss 0.693905, acc 0.5\n",
      "2018-05-04T11:12:15.434623: step 231, loss 0.693039, acc 0.507812\n",
      "2018-05-04T11:12:17.626579: step 232, loss 0.694911, acc 0.515625\n",
      "2018-05-04T11:12:19.845593: step 233, loss 0.694708, acc 0.5\n",
      "2018-05-04T11:12:22.084880: step 234, loss 0.690735, acc 0.59375\n",
      "2018-05-04T11:12:24.314958: step 235, loss 0.694894, acc 0.476562\n",
      "2018-05-04T11:12:26.501318: step 236, loss 0.694481, acc 0.492188\n",
      "2018-05-04T11:12:28.695600: step 237, loss 0.693256, acc 0.484375\n",
      "2018-05-04T11:12:30.897936: step 238, loss 0.692354, acc 0.554688\n",
      "2018-05-04T11:12:33.153979: step 239, loss 0.692446, acc 0.46875\n",
      "2018-05-04T11:12:35.361584: step 240, loss 0.692524, acc 0.515625\n",
      "2018-05-04T11:12:37.602078: step 241, loss 0.69313, acc 0.492188\n",
      "2018-05-04T11:12:39.836139: step 242, loss 0.699304, acc 0.507812\n",
      "2018-05-04T11:12:42.020572: step 243, loss 0.686288, acc 0.554688\n",
      "2018-05-04T11:12:44.131325: step 244, loss 0.693602, acc 0.570312\n",
      "2018-05-04T11:12:46.241548: step 245, loss 0.70029, acc 0.484375\n",
      "2018-05-04T11:12:48.360722: step 246, loss 0.682517, acc 0.585938\n",
      "2018-05-04T11:12:50.546503: step 247, loss 0.688191, acc 0.554688\n",
      "2018-05-04T11:12:52.886681: step 248, loss 0.70358, acc 0.492188\n",
      "2018-05-04T11:12:55.097231: step 249, loss 0.698894, acc 0.507812\n",
      "2018-05-04T11:12:57.279153: step 250, loss 0.692928, acc 0.539062\n",
      "2018-05-04T11:12:59.393117: step 251, loss 0.703854, acc 0.4375\n",
      "2018-05-04T11:13:01.515868: step 252, loss 0.69039, acc 0.5625\n",
      "2018-05-04T11:13:03.694183: step 253, loss 0.693393, acc 0.484375\n",
      "2018-05-04T11:13:05.816324: step 254, loss 0.690279, acc 0.570312\n",
      "2018-05-04T11:13:07.917045: step 255, loss 0.692014, acc 0.5\n",
      "2018-05-04T11:13:10.025936: step 256, loss 0.690769, acc 0.523438\n",
      "2018-05-04T11:13:12.184083: step 257, loss 0.695781, acc 0.445312\n",
      "2018-05-04T11:13:14.317269: step 258, loss 0.686717, acc 0.570312\n",
      "2018-05-04T11:13:16.359169: step 259, loss 0.69554, acc 0.484375\n",
      "2018-05-04T11:13:18.384689: step 260, loss 0.698006, acc 0.460938\n",
      "2018-05-04T11:13:20.426489: step 261, loss 0.692289, acc 0.539062\n",
      "2018-05-04T11:13:22.642133: step 262, loss 0.695816, acc 0.492188\n",
      "2018-05-04T11:13:24.771210: step 263, loss 0.691545, acc 0.507812\n",
      "2018-05-04T11:13:26.872897: step 264, loss 0.695622, acc 0.476562\n",
      "2018-05-04T11:13:28.906122: step 265, loss 0.68821, acc 0.648438\n",
      "2018-05-04T11:13:30.973492: step 266, loss 0.696724, acc 0.429688\n",
      "2018-05-04T11:13:32.976666: step 267, loss 0.692083, acc 0.539062\n",
      "2018-05-04T11:13:35.105343: step 268, loss 0.695134, acc 0.515625\n",
      "2018-05-04T11:13:37.159505: step 269, loss 0.700761, acc 0.414062\n",
      "2018-05-04T11:13:39.135704: step 270, loss 0.696032, acc 0.453125\n",
      "2018-05-04T11:13:41.162507: step 271, loss 0.695674, acc 0.5\n",
      "2018-05-04T11:13:43.145914: step 272, loss 0.694039, acc 0.46875\n",
      "2018-05-04T11:13:45.129740: step 273, loss 0.689082, acc 0.578125\n",
      "2018-05-04T11:13:47.046100: step 274, loss 0.695676, acc 0.445312\n",
      "2018-05-04T11:13:48.971548: step 275, loss 0.692811, acc 0.523438\n",
      "2018-05-04T11:13:50.888376: step 276, loss 0.69072, acc 0.539062\n",
      "2018-05-04T11:13:52.767347: step 277, loss 0.690751, acc 0.546875\n",
      "2018-05-04T11:13:54.638975: step 278, loss 0.691685, acc 0.53125\n",
      "2018-05-04T11:13:56.494141: step 279, loss 0.691655, acc 0.539062\n",
      "2018-05-04T11:13:58.356473: step 280, loss 0.694372, acc 0.460938\n",
      "2018-05-04T11:14:00.222150: step 281, loss 0.689787, acc 0.578125\n",
      "2018-05-04T11:14:02.095246: step 282, loss 0.696858, acc 0.476562\n",
      "2018-05-04T11:14:03.874451: step 283, loss 0.696248, acc 0.460938\n",
      "2018-05-04T11:14:05.664792: step 284, loss 0.692869, acc 0.5\n",
      "2018-05-04T11:14:07.497648: step 285, loss 0.691777, acc 0.554688\n",
      "2018-05-04T11:14:09.323189: step 286, loss 0.69242, acc 0.484375\n",
      "2018-05-04T11:14:11.140446: step 287, loss 0.693485, acc 0.492188\n",
      "2018-05-04T11:14:12.968242: step 288, loss 0.693599, acc 0.5\n",
      "2018-05-04T11:14:14.765312: step 289, loss 0.692798, acc 0.554688\n",
      "2018-05-04T11:14:16.537232: step 290, loss 0.693443, acc 0.546875\n",
      "2018-05-04T11:14:18.313996: step 291, loss 0.69547, acc 0.492188\n",
      "2018-05-04T11:14:20.122072: step 292, loss 0.692313, acc 0.492188\n",
      "2018-05-04T11:14:21.894462: step 293, loss 0.694992, acc 0.429688\n",
      "2018-05-04T11:14:23.649087: step 294, loss 0.692893, acc 0.507812\n",
      "2018-05-04T11:14:25.445793: step 295, loss 0.690887, acc 0.515625\n",
      "2018-05-04T11:14:27.214327: step 296, loss 0.697631, acc 0.421875\n",
      "2018-05-04T11:14:28.982891: step 297, loss 0.693829, acc 0.507812\n",
      "2018-05-04T11:14:30.733401: step 298, loss 0.693634, acc 0.5\n",
      "2018-05-04T11:14:32.537949: step 299, loss 0.690457, acc 0.539062\n",
      "2018-05-04T11:14:34.298576: step 300, loss 0.693044, acc 0.515625\n",
      "2018-05-04T11:14:35.971703: step 301, loss 0.693946, acc 0.507812\n",
      "2018-05-04T11:14:37.643325: step 302, loss 0.691117, acc 0.554688\n",
      "2018-05-04T11:14:39.424199: step 303, loss 0.688735, acc 0.617188\n",
      "2018-05-04T11:14:41.249076: step 304, loss 0.691788, acc 0.546875\n",
      "2018-05-04T11:14:43.020249: step 305, loss 0.690427, acc 0.5625\n",
      "2018-05-04T11:14:44.821713: step 306, loss 0.697277, acc 0.460938\n",
      "2018-05-04T11:14:46.546110: step 307, loss 0.702055, acc 0.40625\n",
      "2018-05-04T11:14:48.279778: step 308, loss 0.695591, acc 0.484375\n",
      "2018-05-04T11:14:50.018148: step 309, loss 0.696698, acc 0.5\n",
      "2018-05-04T11:14:51.815672: step 310, loss 0.694865, acc 0.5\n",
      "2018-05-04T11:14:53.563180: step 311, loss 0.694585, acc 0.476562\n",
      "2018-05-04T11:14:55.403753: step 312, loss 0.691895, acc 0.492188\n",
      "2018-05-04T11:14:57.151276: step 313, loss 0.693377, acc 0.507812\n",
      "2018-05-04T11:14:58.913517: step 314, loss 0.693367, acc 0.484375\n",
      "2018-05-04T11:15:00.662655: step 315, loss 0.69398, acc 0.46875\n",
      "2018-05-04T11:15:02.407685: step 316, loss 0.692524, acc 0.492188\n",
      "2018-05-04T11:15:04.173504: step 317, loss 0.691623, acc 0.539062\n",
      "2018-05-04T11:15:05.894966: step 318, loss 0.693488, acc 0.453125\n",
      "2018-05-04T11:15:07.655344: step 319, loss 0.692176, acc 0.554688\n",
      "2018-05-04T11:15:09.408047: step 320, loss 0.694647, acc 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:15:11.180719: step 321, loss 0.691783, acc 0.546875\n",
      "2018-05-04T11:15:12.967087: step 322, loss 0.691662, acc 0.507812\n",
      "2018-05-04T11:15:14.747208: step 323, loss 0.69488, acc 0.484375\n",
      "2018-05-04T11:15:16.497888: step 324, loss 0.694139, acc 0.492188\n",
      "2018-05-04T11:15:18.266865: step 325, loss 0.696197, acc 0.46875\n",
      "2018-05-04T11:15:20.020759: step 326, loss 0.693167, acc 0.554688\n",
      "2018-05-04T11:15:21.776884: step 327, loss 0.698726, acc 0.46875\n",
      "2018-05-04T11:15:23.528624: step 328, loss 0.693389, acc 0.492188\n",
      "2018-05-04T11:15:25.376140: step 329, loss 0.69625, acc 0.460938\n",
      "2018-05-04T11:15:27.240986: step 330, loss 0.693794, acc 0.507812\n",
      "2018-05-04T11:15:29.037792: step 331, loss 0.693796, acc 0.5\n",
      "2018-05-04T11:15:30.838404: step 332, loss 0.696213, acc 0.453125\n",
      "2018-05-04T11:15:32.629724: step 333, loss 0.693574, acc 0.476562\n",
      "2018-05-04T11:15:34.441456: step 334, loss 0.692276, acc 0.570312\n",
      "2018-05-04T11:15:36.259372: step 335, loss 0.6935, acc 0.460938\n",
      "2018-05-04T11:15:38.045307: step 336, loss 0.69249, acc 0.546875\n",
      "2018-05-04T11:15:39.844942: step 337, loss 0.695549, acc 0.507812\n",
      "2018-05-04T11:15:41.658744: step 338, loss 0.692269, acc 0.46875\n",
      "2018-05-04T11:15:43.456931: step 339, loss 0.689973, acc 0.554688\n",
      "2018-05-04T11:15:45.312687: step 340, loss 0.691062, acc 0.53125\n",
      "2018-05-04T11:15:47.112730: step 341, loss 0.69503, acc 0.4375\n",
      "2018-05-04T11:15:48.906341: step 342, loss 0.690278, acc 0.523438\n",
      "2018-05-04T11:15:50.733151: step 343, loss 0.691968, acc 0.515625\n",
      "2018-05-04T11:15:52.543445: step 344, loss 0.696752, acc 0.398438\n",
      "2018-05-04T11:15:54.368869: step 345, loss 0.694538, acc 0.5\n",
      "2018-05-04T11:15:56.210883: step 346, loss 0.690405, acc 0.53125\n",
      "2018-05-04T11:15:58.130720: step 347, loss 0.694848, acc 0.460938\n",
      "2018-05-04T11:15:59.981327: step 348, loss 0.695846, acc 0.507812\n",
      "2018-05-04T11:16:01.834723: step 349, loss 0.693789, acc 0.476562\n",
      "2018-05-04T11:16:03.690878: step 350, loss 0.6927, acc 0.53125\n",
      "2018-05-04T11:16:05.535774: step 351, loss 0.691497, acc 0.585938\n",
      "2018-05-04T11:16:07.389050: step 352, loss 0.688779, acc 0.59375\n",
      "2018-05-04T11:16:09.255262: step 353, loss 0.696043, acc 0.53125\n",
      "2018-05-04T11:16:11.161153: step 354, loss 0.699718, acc 0.460938\n",
      "2018-05-04T11:16:13.006586: step 355, loss 0.693172, acc 0.523438\n",
      "2018-05-04T11:16:14.842352: step 356, loss 0.69503, acc 0.484375\n",
      "2018-05-04T11:16:16.672953: step 357, loss 0.695484, acc 0.46875\n",
      "2018-05-04T11:16:18.520549: step 358, loss 0.692707, acc 0.5\n",
      "2018-05-04T11:16:20.355512: step 359, loss 0.696647, acc 0.460938\n",
      "2018-05-04T11:16:22.195667: step 360, loss 0.697114, acc 0.375\n",
      "2018-05-04T11:16:24.034463: step 361, loss 0.692289, acc 0.523438\n",
      "2018-05-04T11:16:25.876068: step 362, loss 0.694822, acc 0.484375\n",
      "2018-05-04T11:16:27.722629: step 363, loss 0.692629, acc 0.515625\n",
      "2018-05-04T11:16:29.639548: step 364, loss 0.693432, acc 0.507812\n",
      "2018-05-04T11:16:31.608777: step 365, loss 0.690221, acc 0.570312\n",
      "2018-05-04T11:16:33.576002: step 366, loss 0.693028, acc 0.484375\n",
      "2018-05-04T11:16:35.628883: step 367, loss 0.693962, acc 0.492188\n",
      "2018-05-04T11:16:37.663284: step 368, loss 0.693159, acc 0.523438\n",
      "2018-05-04T11:16:39.624480: step 369, loss 0.696611, acc 0.460938\n",
      "2018-05-04T11:16:41.518375: step 370, loss 0.693451, acc 0.476562\n",
      "2018-05-04T11:16:43.419007: step 371, loss 0.695292, acc 0.492188\n",
      "2018-05-04T11:16:45.323447: step 372, loss 0.689906, acc 0.570312\n",
      "2018-05-04T11:16:47.295025: step 373, loss 0.696015, acc 0.4375\n",
      "2018-05-04T11:16:49.213497: step 374, loss 0.694526, acc 0.476562\n",
      "2018-05-04T11:16:51.185314: step 375, loss 0.6971, acc 0.421875\n",
      "2018-05-04T11:16:53.092115: step 376, loss 0.693793, acc 0.484375\n",
      "2018-05-04T11:16:55.022699: step 377, loss 0.694264, acc 0.46875\n",
      "2018-05-04T11:16:56.918955: step 378, loss 0.691654, acc 0.554688\n",
      "2018-05-04T11:16:58.856944: step 379, loss 0.692989, acc 0.515625\n",
      "2018-05-04T11:17:00.798497: step 380, loss 0.692548, acc 0.515625\n",
      "2018-05-04T11:17:02.756098: step 381, loss 0.693323, acc 0.5\n",
      "2018-05-04T11:17:04.733746: step 382, loss 0.69237, acc 0.53125\n",
      "2018-05-04T11:17:06.705248: step 383, loss 0.692799, acc 0.492188\n",
      "2018-05-04T11:17:08.618681: step 384, loss 0.6936, acc 0.5\n",
      "2018-05-04T11:17:10.523916: step 385, loss 0.691239, acc 0.5625\n",
      "2018-05-04T11:17:12.476958: step 386, loss 0.69584, acc 0.421875\n",
      "2018-05-04T11:17:14.382792: step 387, loss 0.695537, acc 0.476562\n",
      "2018-05-04T11:17:16.309410: step 388, loss 0.691733, acc 0.539062\n",
      "2018-05-04T11:17:18.280823: step 389, loss 0.690082, acc 0.53125\n",
      "2018-05-04T11:17:20.244498: step 390, loss 0.690587, acc 0.554688\n",
      "2018-05-04T11:17:22.160302: step 391, loss 0.690678, acc 0.554688\n",
      "2018-05-04T11:17:24.051592: step 392, loss 0.693877, acc 0.5\n",
      "2018-05-04T11:17:25.964018: step 393, loss 0.691031, acc 0.53125\n",
      "2018-05-04T11:17:27.853172: step 394, loss 0.693788, acc 0.476562\n",
      "2018-05-04T11:17:29.766003: step 395, loss 0.695568, acc 0.460938\n",
      "2018-05-04T11:17:31.789305: step 396, loss 0.688534, acc 0.578125\n",
      "2018-05-04T11:17:33.776843: step 397, loss 0.693233, acc 0.484375\n",
      "2018-05-04T11:17:35.747140: step 398, loss 0.695885, acc 0.5\n",
      "2018-05-04T11:17:37.722164: step 399, loss 0.693568, acc 0.515625\n",
      "2018-05-04T11:17:39.625761: step 400, loss 0.69681, acc 0.46875\n",
      "2018-05-04T11:17:41.528215: step 401, loss 0.693422, acc 0.476562\n",
      "2018-05-04T11:17:43.428811: step 402, loss 0.693767, acc 0.507812\n",
      "2018-05-04T11:17:45.332727: step 403, loss 0.693384, acc 0.484375\n",
      "2018-05-04T11:17:47.248773: step 404, loss 0.692877, acc 0.546875\n",
      "2018-05-04T11:17:49.049819: step 405, loss 0.690891, acc 0.515625\n",
      "2018-05-04T11:17:50.924631: step 406, loss 0.692143, acc 0.523438\n",
      "2018-05-04T11:17:52.878881: step 407, loss 0.696934, acc 0.445312\n",
      "2018-05-04T11:17:54.774311: step 408, loss 0.694388, acc 0.460938\n",
      "2018-05-04T11:17:56.673737: step 409, loss 0.694392, acc 0.492188\n",
      "2018-05-04T11:17:58.575865: step 410, loss 0.69435, acc 0.453125\n",
      "2018-05-04T11:18:00.483620: step 411, loss 0.692898, acc 0.515625\n",
      "2018-05-04T11:18:02.486024: step 412, loss 0.692015, acc 0.578125\n",
      "2018-05-04T11:18:04.421716: step 413, loss 0.693996, acc 0.460938\n",
      "2018-05-04T11:18:06.332885: step 414, loss 0.692797, acc 0.53125\n",
      "2018-05-04T11:18:08.250832: step 415, loss 0.692397, acc 0.53125\n",
      "2018-05-04T11:18:10.103765: step 416, loss 0.69301, acc 0.5\n",
      "2018-05-04T11:18:11.944070: step 417, loss 0.691052, acc 0.570312\n",
      "2018-05-04T11:18:13.784525: step 418, loss 0.693231, acc 0.515625\n",
      "2018-05-04T11:18:15.643338: step 419, loss 0.692427, acc 0.515625\n",
      "2018-05-04T11:18:17.541088: step 420, loss 0.690969, acc 0.539062\n",
      "2018-05-04T11:18:19.441605: step 421, loss 0.692901, acc 0.5\n",
      "2018-05-04T11:18:21.348267: step 422, loss 0.691046, acc 0.5\n",
      "2018-05-04T11:18:23.194667: step 423, loss 0.689014, acc 0.515625\n",
      "2018-05-04T11:18:25.067259: step 424, loss 0.690672, acc 0.539062\n",
      "2018-05-04T11:18:26.962155: step 425, loss 0.695467, acc 0.554688\n",
      "2018-05-04T11:18:28.856037: step 426, loss 0.699175, acc 0.539062\n",
      "2018-05-04T11:18:30.773039: step 427, loss 0.695008, acc 0.507812\n",
      "2018-05-04T11:18:32.756645: step 428, loss 0.696841, acc 0.476562\n",
      "2018-05-04T11:18:34.667116: step 429, loss 0.685611, acc 0.578125\n",
      "2018-05-04T11:18:36.577979: step 430, loss 0.699762, acc 0.429688\n",
      "2018-05-04T11:18:38.491521: step 431, loss 0.694712, acc 0.515625\n",
      "2018-05-04T11:18:40.392682: step 432, loss 0.693449, acc 0.546875\n",
      "2018-05-04T11:18:42.276487: step 433, loss 0.695794, acc 0.5\n",
      "2018-05-04T11:18:44.115810: step 434, loss 0.69061, acc 0.546875\n",
      "2018-05-04T11:18:45.957796: step 435, loss 0.690475, acc 0.578125\n",
      "2018-05-04T11:18:47.903303: step 436, loss 0.691536, acc 0.546875\n",
      "2018-05-04T11:18:49.865835: step 437, loss 0.69679, acc 0.4375\n",
      "2018-05-04T11:18:51.890111: step 438, loss 0.690935, acc 0.546875\n",
      "2018-05-04T11:18:53.788405: step 439, loss 0.69643, acc 0.476562\n",
      "2018-05-04T11:18:55.686810: step 440, loss 0.694193, acc 0.492188\n",
      "2018-05-04T11:18:57.572184: step 441, loss 0.694972, acc 0.476562\n",
      "2018-05-04T11:18:59.461011: step 442, loss 0.695268, acc 0.453125\n",
      "2018-05-04T11:19:01.362779: step 443, loss 0.6938, acc 0.507812\n",
      "2018-05-04T11:19:03.349261: step 444, loss 0.69546, acc 0.421875\n",
      "2018-05-04T11:19:05.326467: step 445, loss 0.693549, acc 0.484375\n",
      "2018-05-04T11:19:07.281998: step 446, loss 0.692958, acc 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:19:09.293580: step 447, loss 0.69227, acc 0.523438\n",
      "2018-05-04T11:19:11.301340: step 448, loss 0.693712, acc 0.429688\n",
      "2018-05-04T11:19:13.266591: step 449, loss 0.693658, acc 0.46875\n",
      "2018-05-04T11:19:15.167432: step 450, loss 0.693347, acc 0.5\n",
      "2018-05-04T11:19:17.071796: step 451, loss 0.693536, acc 0.46875\n",
      "2018-05-04T11:19:18.968017: step 452, loss 0.692946, acc 0.53125\n",
      "2018-05-04T11:19:20.880760: step 453, loss 0.694211, acc 0.46875\n",
      "2018-05-04T11:19:22.782182: step 454, loss 0.693705, acc 0.453125\n",
      "2018-05-04T11:19:24.694208: step 455, loss 0.694129, acc 0.421875\n",
      "2018-05-04T11:19:26.660156: step 456, loss 0.693247, acc 0.523438\n",
      "2018-05-04T11:19:28.623677: step 457, loss 0.693055, acc 0.539062\n",
      "2018-05-04T11:19:30.557645: step 458, loss 0.693233, acc 0.507812\n",
      "2018-05-04T11:19:32.474295: step 459, loss 0.693203, acc 0.515625\n",
      "2018-05-04T11:19:34.705511: step 460, loss 0.693435, acc 0.476562\n",
      "2018-05-04T11:19:36.868298: step 461, loss 0.692817, acc 0.53125\n",
      "2018-05-04T11:19:38.894379: step 462, loss 0.69275, acc 0.539062\n",
      "2018-05-04T11:19:40.931395: step 463, loss 0.693485, acc 0.484375\n",
      "2018-05-04T11:19:42.905738: step 464, loss 0.69361, acc 0.46875\n",
      "2018-05-04T11:19:44.878716: step 465, loss 0.693164, acc 0.53125\n",
      "2018-05-04T11:19:46.852965: step 466, loss 0.692945, acc 0.515625\n",
      "2018-05-04T11:19:48.832661: step 467, loss 0.693203, acc 0.484375\n",
      "2018-05-04T11:19:50.803949: step 468, loss 0.69341, acc 0.484375\n",
      "2018-05-04T11:19:52.714641: step 469, loss 0.693102, acc 0.546875\n",
      "2018-05-04T11:19:54.677215: step 470, loss 0.693703, acc 0.460938\n",
      "2018-05-04T11:19:56.644853: step 471, loss 0.692411, acc 0.5\n",
      "2018-05-04T11:19:58.602165: step 472, loss 0.693088, acc 0.5\n",
      "2018-05-04T11:20:00.563019: step 473, loss 0.691757, acc 0.570312\n",
      "2018-05-04T11:20:02.525263: step 474, loss 0.694075, acc 0.46875\n",
      "2018-05-04T11:20:04.606062: step 475, loss 0.693042, acc 0.507812\n",
      "2018-05-04T11:20:06.581622: step 476, loss 0.694438, acc 0.4375\n",
      "2018-05-04T11:20:08.549017: step 477, loss 0.693736, acc 0.445312\n",
      "2018-05-04T11:20:10.542440: step 478, loss 0.692607, acc 0.507812\n",
      "2018-05-04T11:20:12.596734: step 479, loss 0.692871, acc 0.554688\n",
      "2018-05-04T11:20:14.559891: step 480, loss 0.693497, acc 0.484375\n",
      "2018-05-04T11:20:16.523582: step 481, loss 0.693146, acc 0.53125\n",
      "2018-05-04T11:20:18.478296: step 482, loss 0.692833, acc 0.523438\n",
      "2018-05-04T11:20:20.392414: step 483, loss 0.693339, acc 0.492188\n",
      "2018-05-04T11:20:22.303024: step 484, loss 0.692682, acc 0.515625\n",
      "2018-05-04T11:20:24.214806: step 485, loss 0.692629, acc 0.546875\n",
      "2018-05-04T11:20:26.127612: step 486, loss 0.693765, acc 0.476562\n",
      "2018-05-04T11:20:27.964727: step 487, loss 0.693166, acc 0.484375\n",
      "2018-05-04T11:20:29.826153: step 488, loss 0.694459, acc 0.429688\n",
      "2018-05-04T11:20:31.699232: step 489, loss 0.693479, acc 0.554688\n",
      "2018-05-04T11:20:33.579636: step 490, loss 0.693084, acc 0.476562\n",
      "2018-05-04T11:20:35.568323: step 491, loss 0.692941, acc 0.507812\n",
      "2018-05-04T11:20:37.495141: step 492, loss 0.693062, acc 0.5\n",
      "2018-05-04T11:20:39.401812: step 493, loss 0.692922, acc 0.53125\n",
      "2018-05-04T11:20:41.271911: step 494, loss 0.692879, acc 0.546875\n",
      "2018-05-04T11:20:43.126313: step 495, loss 0.694009, acc 0.4375\n",
      "2018-05-04T11:20:44.928977: step 496, loss 0.693577, acc 0.5\n",
      "2018-05-04T11:20:46.827420: step 497, loss 0.693545, acc 0.484375\n",
      "2018-05-04T11:20:48.647022: step 498, loss 0.692994, acc 0.46875\n",
      "2018-05-04T11:20:50.497601: step 499, loss 0.692696, acc 0.546875\n",
      "2018-05-04T11:20:52.336685: step 500, loss 0.693389, acc 0.515625\n",
      "2018-05-04T11:20:54.127886: step 501, loss 0.69342, acc 0.460938\n",
      "2018-05-04T11:20:55.934364: step 502, loss 0.692907, acc 0.515625\n",
      "2018-05-04T11:20:57.776801: step 503, loss 0.693352, acc 0.53125\n",
      "2018-05-04T11:20:59.593702: step 504, loss 0.693227, acc 0.484375\n",
      "2018-05-04T11:21:01.470736: step 505, loss 0.692672, acc 0.539062\n",
      "2018-05-04T11:21:03.188943: step 506, loss 0.694335, acc 0.4375\n",
      "2018-05-04T11:21:04.934426: step 507, loss 0.692911, acc 0.554688\n",
      "2018-05-04T11:21:06.799885: step 508, loss 0.692611, acc 0.546875\n",
      "2018-05-04T11:21:08.598740: step 509, loss 0.693759, acc 0.515625\n",
      "2018-05-04T11:21:10.385711: step 510, loss 0.692089, acc 0.585938\n",
      "2018-05-04T11:21:12.183962: step 511, loss 0.694122, acc 0.453125\n",
      "2018-05-04T11:21:14.000596: step 512, loss 0.693524, acc 0.484375\n",
      "2018-05-04T11:21:15.781478: step 513, loss 0.692756, acc 0.492188\n",
      "2018-05-04T11:21:17.540866: step 514, loss 0.693533, acc 0.46875\n",
      "2018-05-04T11:21:19.330293: step 515, loss 0.69294, acc 0.507812\n",
      "2018-05-04T11:21:21.119211: step 516, loss 0.69308, acc 0.507812\n",
      "2018-05-04T11:21:22.889027: step 517, loss 0.692607, acc 0.539062\n",
      "2018-05-04T11:21:24.633505: step 518, loss 0.692936, acc 0.507812\n",
      "2018-05-04T11:21:26.437886: step 519, loss 0.693884, acc 0.414062\n",
      "2018-05-04T11:21:28.197672: step 520, loss 0.692449, acc 0.5625\n",
      "2018-05-04T11:21:29.956135: step 521, loss 0.69354, acc 0.460938\n",
      "2018-05-04T11:21:31.735750: step 522, loss 0.693608, acc 0.460938\n",
      "2018-05-04T11:21:33.467630: step 523, loss 0.69388, acc 0.476562\n",
      "2018-05-04T11:21:35.234038: step 524, loss 0.692791, acc 0.515625\n",
      "2018-05-04T11:21:37.042848: step 525, loss 0.69383, acc 0.460938\n",
      "2018-05-04T11:21:38.832558: step 526, loss 0.693238, acc 0.484375\n",
      "2018-05-04T11:21:40.644069: step 527, loss 0.69256, acc 0.507812\n",
      "2018-05-04T11:21:42.421689: step 528, loss 0.693896, acc 0.398438\n",
      "2018-05-04T11:21:44.194735: step 529, loss 0.692164, acc 0.585938\n",
      "2018-05-04T11:21:45.950230: step 530, loss 0.694232, acc 0.445312\n",
      "2018-05-04T11:21:47.706202: step 531, loss 0.692612, acc 0.578125\n",
      "2018-05-04T11:21:49.468560: step 532, loss 0.693604, acc 0.46875\n",
      "2018-05-04T11:21:51.238366: step 533, loss 0.69315, acc 0.492188\n",
      "2018-05-04T11:21:52.997062: step 534, loss 0.692569, acc 0.5\n",
      "2018-05-04T11:21:54.758446: step 535, loss 0.694355, acc 0.492188\n",
      "2018-05-04T11:21:56.517365: step 536, loss 0.693197, acc 0.476562\n",
      "2018-05-04T11:21:58.319801: step 537, loss 0.693746, acc 0.492188\n",
      "2018-05-04T11:22:00.128126: step 538, loss 0.692856, acc 0.507812\n",
      "2018-05-04T11:22:01.918028: step 539, loss 0.692388, acc 0.523438\n",
      "2018-05-04T11:22:03.688850: step 540, loss 0.693394, acc 0.523438\n",
      "2018-05-04T11:22:05.457999: step 541, loss 0.693931, acc 0.484375\n",
      "2018-05-04T11:22:07.350067: step 542, loss 0.695256, acc 0.453125\n",
      "2018-05-04T11:22:09.188166: step 543, loss 0.694517, acc 0.453125\n",
      "2018-05-04T11:22:10.993055: step 544, loss 0.693219, acc 0.523438\n",
      "2018-05-04T11:22:12.789919: step 545, loss 0.691683, acc 0.5625\n",
      "2018-05-04T11:22:14.599433: step 546, loss 0.693025, acc 0.5\n",
      "2018-05-04T11:22:16.382424: step 547, loss 0.691576, acc 0.5625\n",
      "2018-05-04T11:22:18.177298: step 548, loss 0.693269, acc 0.484375\n",
      "2018-05-04T11:22:19.966184: step 549, loss 0.692712, acc 0.507812\n",
      "2018-05-04T11:22:21.819504: step 550, loss 0.692603, acc 0.507812\n",
      "2018-05-04T11:22:23.669220: step 551, loss 0.693008, acc 0.492188\n",
      "2018-05-04T11:22:25.513901: step 552, loss 0.695439, acc 0.4375\n",
      "2018-05-04T11:22:27.383117: step 553, loss 0.694132, acc 0.484375\n",
      "2018-05-04T11:22:29.201437: step 554, loss 0.693511, acc 0.492188\n",
      "2018-05-04T11:22:31.003495: step 555, loss 0.693135, acc 0.484375\n",
      "2018-05-04T11:22:32.827469: step 556, loss 0.693635, acc 0.5\n",
      "2018-05-04T11:22:34.757877: step 557, loss 0.692349, acc 0.554688\n",
      "2018-05-04T11:22:36.773533: step 558, loss 0.694961, acc 0.429688\n",
      "2018-05-04T11:22:38.715192: step 559, loss 0.692626, acc 0.53125\n",
      "2018-05-04T11:22:40.571820: step 560, loss 0.694373, acc 0.460938\n",
      "2018-05-04T11:22:42.487937: step 561, loss 0.691695, acc 0.578125\n",
      "2018-05-04T11:22:44.428236: step 562, loss 0.692922, acc 0.515625\n",
      "2018-05-04T11:22:46.345158: step 563, loss 0.693646, acc 0.46875\n",
      "2018-05-04T11:22:48.203593: step 564, loss 0.694599, acc 0.40625\n",
      "2018-05-04T11:22:50.040379: step 565, loss 0.692645, acc 0.539062\n",
      "2018-05-04T11:22:51.932110: step 566, loss 0.693313, acc 0.476562\n",
      "2018-05-04T11:22:53.768615: step 567, loss 0.69214, acc 0.5625\n",
      "2018-05-04T11:22:55.612018: step 568, loss 0.691811, acc 0.554688\n",
      "2018-05-04T11:22:57.440894: step 569, loss 0.693165, acc 0.492188\n",
      "2018-05-04T11:22:59.305840: step 570, loss 0.692902, acc 0.429688\n",
      "2018-05-04T11:23:01.201933: step 571, loss 0.692924, acc 0.476562\n",
      "2018-05-04T11:23:03.084982: step 572, loss 0.693215, acc 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:23:05.012635: step 573, loss 0.694416, acc 0.382812\n",
      "2018-05-04T11:23:06.946740: step 574, loss 0.694401, acc 0.445312\n",
      "2018-05-04T11:23:08.897295: step 575, loss 0.693987, acc 0.46875\n",
      "2018-05-04T11:23:10.828905: step 576, loss 0.692743, acc 0.5625\n",
      "2018-05-04T11:23:12.721179: step 577, loss 0.693512, acc 0.5\n",
      "2018-05-04T11:23:14.637152: step 578, loss 0.693436, acc 0.476562\n",
      "2018-05-04T11:23:16.539591: step 579, loss 0.692804, acc 0.445312\n",
      "2018-05-04T11:23:18.434895: step 580, loss 0.693123, acc 0.5\n",
      "2018-05-04T11:23:20.328725: step 581, loss 0.691838, acc 0.632812\n",
      "2018-05-04T11:23:22.240197: step 582, loss 0.692245, acc 0.546875\n",
      "2018-05-04T11:23:24.147408: step 583, loss 0.692802, acc 0.5\n",
      "2018-05-04T11:23:26.041770: step 584, loss 0.693655, acc 0.476562\n",
      "2018-05-04T11:23:27.953376: step 585, loss 0.693893, acc 0.492188\n",
      "2018-05-04T11:23:29.953074: step 586, loss 0.694045, acc 0.492188\n",
      "2018-05-04T11:23:31.905712: step 587, loss 0.695786, acc 0.414062\n",
      "2018-05-04T11:23:33.887213: step 588, loss 0.693142, acc 0.507812\n",
      "2018-05-04T11:23:35.857891: step 589, loss 0.693727, acc 0.492188\n",
      "2018-05-04T11:23:37.939068: step 590, loss 0.693033, acc 0.515625\n",
      "2018-05-04T11:23:39.895502: step 591, loss 0.692956, acc 0.476562\n",
      "2018-05-04T11:23:41.790836: step 592, loss 0.692329, acc 0.601562\n",
      "2018-05-04T11:23:43.743637: step 593, loss 0.692566, acc 0.546875\n",
      "2018-05-04T11:23:45.686850: step 594, loss 0.692452, acc 0.523438\n",
      "2018-05-04T11:23:47.658150: step 595, loss 0.692047, acc 0.578125\n",
      "2018-05-04T11:23:49.663274: step 596, loss 0.693734, acc 0.507812\n",
      "2018-05-04T11:23:51.676290: step 597, loss 0.692978, acc 0.523438\n",
      "2018-05-04T11:23:53.635047: step 598, loss 0.695506, acc 0.429688\n",
      "2018-05-04T11:23:55.586040: step 599, loss 0.691282, acc 0.5625\n",
      "2018-05-04T11:23:57.522685: step 600, loss 0.693448, acc 0.476562\n",
      "2018-05-04T11:23:59.462434: step 601, loss 0.693132, acc 0.492188\n",
      "2018-05-04T11:24:01.411543: step 602, loss 0.694736, acc 0.445312\n",
      "2018-05-04T11:24:03.370288: step 603, loss 0.693245, acc 0.507812\n",
      "2018-05-04T11:24:05.351242: step 604, loss 0.694819, acc 0.460938\n",
      "2018-05-04T11:24:07.398495: step 605, loss 0.69253, acc 0.539062\n",
      "2018-05-04T11:24:09.391320: step 606, loss 0.692465, acc 0.507812\n",
      "2018-05-04T11:24:11.456942: step 607, loss 0.691612, acc 0.570312\n",
      "2018-05-04T11:24:13.489895: step 608, loss 0.69483, acc 0.445312\n",
      "2018-05-04T11:24:15.461332: step 609, loss 0.692762, acc 0.53125\n",
      "2018-05-04T11:24:17.355479: step 610, loss 0.69291, acc 0.5\n",
      "2018-05-04T11:24:19.247217: step 611, loss 0.692034, acc 0.539062\n",
      "2018-05-04T11:24:21.083367: step 612, loss 0.695169, acc 0.429688\n",
      "2018-05-04T11:24:23.005541: step 613, loss 0.694474, acc 0.484375\n",
      "2018-05-04T11:24:24.980702: step 614, loss 0.694126, acc 0.476562\n",
      "2018-05-04T11:24:26.975924: step 615, loss 0.692358, acc 0.507812\n",
      "2018-05-04T11:24:28.882207: step 616, loss 0.693659, acc 0.484375\n",
      "2018-05-04T11:24:30.788794: step 617, loss 0.692941, acc 0.515625\n",
      "2018-05-04T11:24:32.763507: step 618, loss 0.692081, acc 0.539062\n",
      "2018-05-04T11:24:34.689504: step 619, loss 0.693532, acc 0.46875\n",
      "2018-05-04T11:24:36.584376: step 620, loss 0.693296, acc 0.515625\n",
      "2018-05-04T11:24:38.605927: step 621, loss 0.693677, acc 0.484375\n",
      "2018-05-04T11:24:40.493788: step 622, loss 0.694575, acc 0.484375\n",
      "2018-05-04T11:24:42.422397: step 623, loss 0.692145, acc 0.523438\n",
      "2018-05-04T11:24:44.339780: step 624, loss 0.691894, acc 0.546875\n",
      "2018-05-04T11:24:46.259863: step 625, loss 0.692267, acc 0.5625\n",
      "2018-05-04T11:24:48.147205: step 626, loss 0.693157, acc 0.523438\n",
      "2018-05-04T11:24:50.035894: step 627, loss 0.692903, acc 0.484375\n",
      "2018-05-04T11:24:51.926779: step 628, loss 0.692598, acc 0.515625\n",
      "2018-05-04T11:24:53.820360: step 629, loss 0.693276, acc 0.484375\n",
      "2018-05-04T11:24:55.727505: step 630, loss 0.693195, acc 0.515625\n",
      "2018-05-04T11:24:57.626177: step 631, loss 0.693792, acc 0.484375\n",
      "2018-05-04T11:24:59.532753: step 632, loss 0.694517, acc 0.484375\n",
      "2018-05-04T11:25:01.437857: step 633, loss 0.693152, acc 0.492188\n",
      "2018-05-04T11:25:03.346359: step 634, loss 0.692908, acc 0.523438\n",
      "2018-05-04T11:25:05.263768: step 635, loss 0.693379, acc 0.507812\n",
      "2018-05-04T11:25:07.240309: step 636, loss 0.694357, acc 0.445312\n",
      "2018-05-04T11:25:09.204525: step 637, loss 0.69237, acc 0.539062\n",
      "2018-05-04T11:25:11.173274: step 638, loss 0.694007, acc 0.484375\n",
      "2018-05-04T11:25:13.138030: step 639, loss 0.693902, acc 0.453125\n",
      "2018-05-04T11:25:15.088534: step 640, loss 0.692079, acc 0.578125\n",
      "2018-05-04T11:25:16.986040: step 641, loss 0.693915, acc 0.453125\n",
      "2018-05-04T11:25:18.892935: step 642, loss 0.694496, acc 0.484375\n",
      "2018-05-04T11:25:20.799366: step 643, loss 0.692053, acc 0.554688\n",
      "2018-05-04T11:25:22.690656: step 644, loss 0.692329, acc 0.5\n",
      "2018-05-04T11:25:24.593410: step 645, loss 0.692469, acc 0.53125\n",
      "2018-05-04T11:25:26.381196: step 646, loss 0.69254, acc 0.546875\n",
      "2018-05-04T11:25:28.237187: step 647, loss 0.692996, acc 0.546875\n",
      "2018-05-04T11:25:30.208773: step 648, loss 0.693507, acc 0.460938\n",
      "2018-05-04T11:25:32.106118: step 649, loss 0.692711, acc 0.5\n",
      "2018-05-04T11:25:34.123287: step 650, loss 0.693944, acc 0.46875\n",
      "2018-05-04T11:25:36.177446: step 651, loss 0.692509, acc 0.5625\n",
      "2018-05-04T11:25:38.279870: step 652, loss 0.692768, acc 0.507812\n",
      "2018-05-04T11:25:40.315159: step 653, loss 0.692005, acc 0.554688\n",
      "2018-05-04T11:25:42.267170: step 654, loss 0.694611, acc 0.414062\n",
      "2018-05-04T11:25:44.174334: step 655, loss 0.692375, acc 0.539062\n",
      "2018-05-04T11:25:46.064868: step 656, loss 0.69371, acc 0.445312\n",
      "2018-05-04T11:25:47.968899: step 657, loss 0.69527, acc 0.421875\n",
      "2018-05-04T11:25:49.897739: step 658, loss 0.694111, acc 0.484375\n",
      "2018-05-04T11:25:51.861435: step 659, loss 0.691665, acc 0.546875\n",
      "2018-05-04T11:25:53.752763: step 660, loss 0.690426, acc 0.546875\n",
      "2018-05-04T11:25:55.632517: step 661, loss 0.697503, acc 0.375\n",
      "2018-05-04T11:25:57.514273: step 662, loss 0.689896, acc 0.585938\n",
      "2018-05-04T11:25:59.464511: step 663, loss 0.693184, acc 0.492188\n",
      "2018-05-04T11:26:01.426781: step 664, loss 0.697145, acc 0.429688\n",
      "2018-05-04T11:26:03.370979: step 665, loss 0.689167, acc 0.617188\n",
      "2018-05-04T11:26:05.338794: step 666, loss 0.695126, acc 0.414062\n",
      "2018-05-04T11:26:07.382198: step 667, loss 0.693591, acc 0.5\n",
      "2018-05-04T11:26:09.399739: step 668, loss 0.69575, acc 0.460938\n",
      "2018-05-04T11:26:11.404908: step 669, loss 0.69183, acc 0.53125\n",
      "2018-05-04T11:26:13.417418: step 670, loss 0.692167, acc 0.46875\n",
      "2018-05-04T11:26:15.369961: step 671, loss 0.693831, acc 0.484375\n",
      "2018-05-04T11:26:17.324744: step 672, loss 0.692976, acc 0.539062\n",
      "2018-05-04T11:26:19.275278: step 673, loss 0.692696, acc 0.539062\n",
      "2018-05-04T11:26:21.300714: step 674, loss 0.695975, acc 0.453125\n",
      "2018-05-04T11:26:23.262724: step 675, loss 0.692589, acc 0.570312\n",
      "2018-05-04T11:26:25.298517: step 676, loss 0.69708, acc 0.46875\n",
      "2018-05-04T11:26:27.323459: step 677, loss 0.693707, acc 0.492188\n",
      "2018-05-04T11:26:29.292288: step 678, loss 0.690892, acc 0.53125\n",
      "2018-05-04T11:26:31.281704: step 679, loss 0.694217, acc 0.5\n",
      "2018-05-04T11:26:33.242956: step 680, loss 0.693595, acc 0.484375\n",
      "2018-05-04T11:26:35.204394: step 681, loss 0.692772, acc 0.53125\n",
      "2018-05-04T11:26:37.256726: step 682, loss 0.694404, acc 0.5\n",
      "2018-05-04T11:26:39.297370: step 683, loss 0.691434, acc 0.546875\n",
      "2018-05-04T11:26:41.388799: step 684, loss 0.693812, acc 0.507812\n",
      "2018-05-04T11:26:43.598863: step 685, loss 0.691712, acc 0.515625\n",
      "2018-05-04T11:26:45.827272: step 686, loss 0.692602, acc 0.53125\n",
      "2018-05-04T11:26:48.100536: step 687, loss 0.694671, acc 0.398438\n",
      "2018-05-04T11:26:50.327135: step 688, loss 0.692113, acc 0.539062\n",
      "2018-05-04T11:26:52.641867: step 689, loss 0.692187, acc 0.523438\n",
      "2018-05-04T11:26:54.668704: step 690, loss 0.693024, acc 0.515625\n",
      "2018-05-04T11:26:56.625125: step 691, loss 0.691613, acc 0.554688\n",
      "2018-05-04T11:26:58.583078: step 692, loss 0.69573, acc 0.484375\n",
      "2018-05-04T11:27:00.470749: step 693, loss 0.695404, acc 0.4375\n",
      "2018-05-04T11:27:02.404967: step 694, loss 0.689238, acc 0.507812\n",
      "2018-05-04T11:27:04.315479: step 695, loss 0.693823, acc 0.515625\n",
      "2018-05-04T11:27:06.217352: step 696, loss 0.692583, acc 0.484375\n",
      "2018-05-04T11:27:08.192215: step 697, loss 0.688218, acc 0.546875\n",
      "2018-05-04T11:27:10.094402: step 698, loss 0.692973, acc 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:27:12.041918: step 699, loss 0.691744, acc 0.5\n",
      "2018-05-04T11:27:13.881000: step 700, loss 0.701339, acc 0.476562\n",
      "2018-05-04T11:27:15.701241: step 701, loss 0.690195, acc 0.554688\n",
      "2018-05-04T11:27:17.534881: step 702, loss 0.691988, acc 0.507812\n",
      "2018-05-04T11:27:19.314893: step 703, loss 0.703191, acc 0.4375\n",
      "2018-05-04T11:27:21.096996: step 704, loss 0.70164, acc 0.453125\n",
      "2018-05-04T11:27:22.884875: step 705, loss 0.700795, acc 0.429688\n",
      "2018-05-04T11:27:24.662325: step 706, loss 0.693408, acc 0.507812\n",
      "2018-05-04T11:27:26.491341: step 707, loss 0.694662, acc 0.546875\n",
      "2018-05-04T11:27:28.316499: step 708, loss 0.693051, acc 0.515625\n",
      "2018-05-04T11:27:30.105853: step 709, loss 0.694225, acc 0.46875\n",
      "2018-05-04T11:27:31.964270: step 710, loss 0.693922, acc 0.46875\n",
      "2018-05-04T11:27:33.756812: step 711, loss 0.693578, acc 0.460938\n",
      "2018-05-04T11:27:35.557186: step 712, loss 0.693757, acc 0.453125\n",
      "2018-05-04T11:27:37.343483: step 713, loss 0.694149, acc 0.421875\n",
      "2018-05-04T11:27:39.225765: step 714, loss 0.693753, acc 0.421875\n",
      "2018-05-04T11:27:40.954708: step 715, loss 0.693115, acc 0.484375\n",
      "2018-05-04T11:27:42.667828: step 716, loss 0.692943, acc 0.523438\n",
      "2018-05-04T11:27:44.465779: step 717, loss 0.692679, acc 0.554688\n",
      "2018-05-04T11:27:46.240113: step 718, loss 0.692749, acc 0.515625\n",
      "2018-05-04T11:27:48.015246: step 719, loss 0.693989, acc 0.523438\n",
      "2018-05-04T11:27:49.804357: step 720, loss 0.692865, acc 0.539062\n",
      "2018-05-04T11:27:51.668283: step 721, loss 0.693026, acc 0.492188\n",
      "2018-05-04T11:27:53.483643: step 722, loss 0.690923, acc 0.585938\n",
      "2018-05-04T11:27:55.137371: step 723, loss 0.693435, acc 0.476562\n",
      "2018-05-04T11:27:56.881369: step 724, loss 0.695346, acc 0.445312\n",
      "2018-05-04T11:27:58.676748: step 725, loss 0.691312, acc 0.523438\n",
      "2018-05-04T11:28:00.481684: step 726, loss 0.688549, acc 0.617188\n",
      "2018-05-04T11:28:02.273227: step 727, loss 0.692243, acc 0.523438\n",
      "2018-05-04T11:28:04.035579: step 728, loss 0.694007, acc 0.476562\n",
      "2018-05-04T11:28:05.778512: step 729, loss 0.689864, acc 0.539062\n",
      "2018-05-04T11:28:07.522077: step 730, loss 0.69494, acc 0.460938\n",
      "2018-05-04T11:28:09.376532: step 731, loss 0.694049, acc 0.453125\n",
      "2018-05-04T11:28:11.165822: step 732, loss 0.693083, acc 0.53125\n",
      "2018-05-04T11:28:12.965592: step 733, loss 0.689665, acc 0.53125\n",
      "2018-05-04T11:28:14.798284: step 734, loss 0.692445, acc 0.484375\n",
      "2018-05-04T11:28:16.541504: step 735, loss 0.694699, acc 0.484375\n",
      "2018-05-04T11:28:18.301535: step 736, loss 0.703661, acc 0.421875\n",
      "2018-05-04T11:28:20.049552: step 737, loss 0.695193, acc 0.515625\n",
      "2018-05-04T11:28:21.797191: step 738, loss 0.698622, acc 0.492188\n",
      "2018-05-04T11:28:23.538292: step 739, loss 0.693203, acc 0.492188\n",
      "2018-05-04T11:28:25.291831: step 740, loss 0.696917, acc 0.453125\n",
      "2018-05-04T11:28:27.031539: step 741, loss 0.692537, acc 0.53125\n",
      "2018-05-04T11:28:28.782429: step 742, loss 0.691085, acc 0.539062\n",
      "2018-05-04T11:28:30.530950: step 743, loss 0.691899, acc 0.546875\n",
      "2018-05-04T11:28:32.291623: step 744, loss 0.693551, acc 0.445312\n",
      "2018-05-04T11:28:34.255706: step 745, loss 0.692091, acc 0.539062\n",
      "2018-05-04T11:28:36.162517: step 746, loss 0.693213, acc 0.46875\n",
      "2018-05-04T11:28:38.025636: step 747, loss 0.69394, acc 0.40625\n",
      "2018-05-04T11:28:39.822460: step 748, loss 0.693026, acc 0.507812\n",
      "2018-05-04T11:28:41.618742: step 749, loss 0.693328, acc 0.515625\n",
      "2018-05-04T11:28:43.412195: step 750, loss 0.692897, acc 0.554688\n",
      "2018-05-04T11:28:45.259083: step 751, loss 0.693187, acc 0.507812\n",
      "2018-05-04T11:28:47.052083: step 752, loss 0.693056, acc 0.515625\n",
      "2018-05-04T11:28:48.838897: step 753, loss 0.693091, acc 0.492188\n",
      "2018-05-04T11:28:50.630634: step 754, loss 0.694014, acc 0.476562\n",
      "2018-05-04T11:28:52.418604: step 755, loss 0.692312, acc 0.53125\n",
      "2018-05-04T11:28:54.267798: step 756, loss 0.691362, acc 0.5625\n",
      "2018-05-04T11:28:56.051969: step 757, loss 0.690954, acc 0.578125\n",
      "2018-05-04T11:28:57.800660: step 758, loss 0.692398, acc 0.523438\n",
      "2018-05-04T11:28:59.547902: step 759, loss 0.691602, acc 0.546875\n",
      "2018-05-04T11:29:01.346636: step 760, loss 0.689899, acc 0.578125\n",
      "2018-05-04T11:29:03.133964: step 761, loss 0.693921, acc 0.492188\n",
      "2018-05-04T11:29:04.948943: step 762, loss 0.688578, acc 0.585938\n",
      "2018-05-04T11:29:06.732506: step 763, loss 0.691355, acc 0.539062\n",
      "2018-05-04T11:29:08.639748: step 764, loss 0.697342, acc 0.445312\n",
      "2018-05-04T11:29:10.566834: step 765, loss 0.695184, acc 0.476562\n",
      "2018-05-04T11:29:12.767270: step 766, loss 0.693742, acc 0.507812\n",
      "2018-05-04T11:29:14.921283: step 767, loss 0.700891, acc 0.40625\n",
      "2018-05-04T11:29:17.138248: step 768, loss 0.694494, acc 0.492188\n",
      "2018-05-04T11:29:19.256727: step 769, loss 0.699762, acc 0.414062\n",
      "2018-05-04T11:29:21.403726: step 770, loss 0.69222, acc 0.53125\n",
      "2018-05-04T11:29:23.510018: step 771, loss 0.691933, acc 0.546875\n",
      "2018-05-04T11:29:25.540847: step 772, loss 0.693959, acc 0.492188\n",
      "2018-05-04T11:29:27.452387: step 773, loss 0.695215, acc 0.46875\n",
      "2018-05-04T11:29:29.465542: step 774, loss 0.696157, acc 0.4375\n",
      "2018-05-04T11:29:31.690169: step 775, loss 0.692593, acc 0.53125\n",
      "2018-05-04T11:29:33.970840: step 776, loss 0.69301, acc 0.484375\n",
      "2018-05-04T11:29:36.024826: step 777, loss 0.694546, acc 0.46875\n",
      "2018-05-04T11:29:37.817126: step 778, loss 0.693713, acc 0.507812\n",
      "2018-05-04T11:29:39.605349: step 779, loss 0.693763, acc 0.460938\n",
      "2018-05-04T11:29:41.479340: step 780, loss 0.693307, acc 0.53125\n",
      "2018-05-04T11:29:43.212196: step 781, loss 0.692186, acc 0.546875\n",
      "2018-05-04T11:29:44.926179: step 782, loss 0.692495, acc 0.546875\n",
      "2018-05-04T11:29:46.759991: step 783, loss 0.692287, acc 0.554688\n",
      "2018-05-04T11:29:48.713042: step 784, loss 0.692901, acc 0.507812\n",
      "2018-05-04T11:29:50.830563: step 785, loss 0.692734, acc 0.523438\n",
      "2018-05-04T11:29:52.845067: step 786, loss 0.692466, acc 0.507812\n",
      "2018-05-04T11:29:54.821660: step 787, loss 0.686603, acc 0.65625\n",
      "2018-05-04T11:29:56.703542: step 788, loss 0.695885, acc 0.492188\n",
      "2018-05-04T11:29:58.627288: step 789, loss 0.693532, acc 0.5\n",
      "2018-05-04T11:30:00.558383: step 790, loss 0.688179, acc 0.5625\n",
      "2018-05-04T11:30:02.462794: step 791, loss 0.688335, acc 0.546875\n",
      "2018-05-04T11:30:04.332042: step 792, loss 0.692859, acc 0.5\n",
      "2018-05-04T11:30:06.302688: step 793, loss 0.701114, acc 0.445312\n",
      "2018-05-04T11:30:08.034642: step 794, loss 0.691338, acc 0.554688\n",
      "2018-05-04T11:30:10.116739: step 795, loss 0.692082, acc 0.523438\n",
      "2018-05-04T11:30:12.162796: step 796, loss 0.702249, acc 0.453125\n",
      "2018-05-04T11:30:14.205819: step 797, loss 0.70469, acc 0.4375\n",
      "2018-05-04T11:30:16.193548: step 798, loss 0.697883, acc 0.46875\n",
      "2018-05-04T11:30:18.099780: step 799, loss 0.690527, acc 0.5625\n",
      "2018-05-04T11:30:20.000476: step 800, loss 0.695616, acc 0.492188\n",
      "2018-05-04T11:30:22.009721: step 801, loss 0.691124, acc 0.539062\n",
      "2018-05-04T11:30:23.918217: step 802, loss 0.693579, acc 0.484375\n",
      "2018-05-04T11:30:25.826425: step 803, loss 0.695088, acc 0.414062\n",
      "2018-05-04T11:30:27.726120: step 804, loss 0.69269, acc 0.53125\n",
      "2018-05-04T11:30:29.678577: step 805, loss 0.692754, acc 0.515625\n",
      "2018-05-04T11:30:31.673156: step 806, loss 0.6932, acc 0.5\n",
      "2018-05-04T11:30:33.558638: step 807, loss 0.692235, acc 0.5\n",
      "2018-05-04T11:30:35.457794: step 808, loss 0.69373, acc 0.5\n",
      "2018-05-04T11:30:37.373810: step 809, loss 0.690293, acc 0.585938\n",
      "2018-05-04T11:30:39.281436: step 810, loss 0.693217, acc 0.5\n",
      "2018-05-04T11:30:41.475224: step 811, loss 0.693182, acc 0.5\n",
      "2018-05-04T11:30:43.336635: step 812, loss 0.692838, acc 0.53125\n",
      "2018-05-04T11:30:45.276205: step 813, loss 0.694309, acc 0.492188\n",
      "2018-05-04T11:30:47.120329: step 814, loss 0.692784, acc 0.515625\n",
      "2018-05-04T11:30:48.864360: step 815, loss 0.69564, acc 0.445312\n",
      "2018-05-04T11:30:50.554677: step 816, loss 0.693927, acc 0.492188\n",
      "2018-05-04T11:30:52.304485: step 817, loss 0.692145, acc 0.515625\n",
      "2018-05-04T11:30:53.997644: step 818, loss 0.693328, acc 0.5\n",
      "2018-05-04T11:30:55.686123: step 819, loss 0.689875, acc 0.59375\n",
      "2018-05-04T11:30:57.364031: step 820, loss 0.693996, acc 0.484375\n",
      "2018-05-04T11:30:59.038241: step 821, loss 0.694235, acc 0.492188\n",
      "2018-05-04T11:31:00.730163: step 822, loss 0.692499, acc 0.523438\n",
      "2018-05-04T11:31:02.502971: step 823, loss 0.694701, acc 0.46875\n",
      "2018-05-04T11:31:04.217563: step 824, loss 0.69195, acc 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:31:05.951525: step 825, loss 0.692756, acc 0.507812\n",
      "2018-05-04T11:31:07.674378: step 826, loss 0.693114, acc 0.507812\n",
      "2018-05-04T11:31:09.380193: step 827, loss 0.692957, acc 0.507812\n",
      "2018-05-04T11:31:11.031665: step 828, loss 0.695403, acc 0.46875\n",
      "2018-05-04T11:31:12.617701: step 829, loss 0.692309, acc 0.53125\n",
      "2018-05-04T11:31:14.162428: step 830, loss 0.694036, acc 0.484375\n",
      "2018-05-04T11:31:15.715604: step 831, loss 0.692629, acc 0.515625\n",
      "2018-05-04T11:31:17.406736: step 832, loss 0.693867, acc 0.476562\n",
      "2018-05-04T11:31:19.120293: step 833, loss 0.697671, acc 0.375\n",
      "2018-05-04T11:31:20.823080: step 834, loss 0.694873, acc 0.445312\n",
      "2018-05-04T11:31:22.559171: step 835, loss 0.694434, acc 0.46875\n",
      "2018-05-04T11:31:24.389262: step 836, loss 0.693815, acc 0.4375\n",
      "2018-05-04T11:31:26.163457: step 837, loss 0.692403, acc 0.546875\n",
      "2018-05-04T11:31:27.962820: step 838, loss 0.69355, acc 0.523438\n",
      "2018-05-04T11:31:29.989206: step 839, loss 0.692614, acc 0.5625\n",
      "2018-05-04T11:31:32.025344: step 840, loss 0.691887, acc 0.570312\n",
      "2018-05-04T11:31:34.097011: step 841, loss 0.69242, acc 0.523438\n",
      "2018-05-04T11:31:36.184881: step 842, loss 0.692016, acc 0.554688\n",
      "2018-05-04T11:31:38.198649: step 843, loss 0.695662, acc 0.445312\n",
      "2018-05-04T11:31:40.192046: step 844, loss 0.691841, acc 0.5625\n",
      "2018-05-04T11:31:42.398817: step 845, loss 0.688399, acc 0.59375\n",
      "2018-05-04T11:31:44.406771: step 846, loss 0.691762, acc 0.539062\n",
      "2018-05-04T11:31:46.305789: step 847, loss 0.692374, acc 0.5\n",
      "2018-05-04T11:31:48.221439: step 848, loss 0.697261, acc 0.460938\n",
      "2018-05-04T11:31:50.114404: step 849, loss 0.691794, acc 0.515625\n",
      "2018-05-04T11:31:52.207673: step 850, loss 0.692406, acc 0.539062\n",
      "2018-05-04T11:31:54.272360: step 851, loss 0.697749, acc 0.460938\n",
      "2018-05-04T11:31:56.275998: step 852, loss 0.697317, acc 0.421875\n",
      "2018-05-04T11:31:58.289918: step 853, loss 0.690664, acc 0.539062\n",
      "2018-05-04T11:32:00.213410: step 854, loss 0.693285, acc 0.484375\n",
      "2018-05-04T11:32:02.002108: step 855, loss 0.693904, acc 0.460938\n",
      "2018-05-04T11:32:03.804030: step 856, loss 0.695655, acc 0.476562\n",
      "2018-05-04T11:32:05.684172: step 857, loss 0.695452, acc 0.453125\n",
      "2018-05-04T11:32:07.631571: step 858, loss 0.69341, acc 0.523438\n",
      "2018-05-04T11:32:09.530841: step 859, loss 0.69165, acc 0.546875\n",
      "2018-05-04T11:32:11.684657: step 860, loss 0.692306, acc 0.554688\n",
      "2018-05-04T11:32:13.425047: step 861, loss 0.693683, acc 0.515625\n",
      "2018-05-04T11:32:15.146214: step 862, loss 0.691506, acc 0.585938\n",
      "2018-05-04T11:32:16.829614: step 863, loss 0.695221, acc 0.460938\n",
      "2018-05-04T11:32:18.532560: step 864, loss 0.694901, acc 0.40625\n",
      "2018-05-04T11:32:20.530742: step 865, loss 0.693331, acc 0.492188\n",
      "2018-05-04T11:32:22.482004: step 866, loss 0.694409, acc 0.476562\n",
      "2018-05-04T11:32:24.306563: step 867, loss 0.693883, acc 0.46875\n",
      "2018-05-04T11:32:26.236086: step 868, loss 0.693582, acc 0.507812\n",
      "2018-05-04T11:32:28.120156: step 869, loss 0.69467, acc 0.5\n",
      "2018-05-04T11:32:29.868968: step 870, loss 0.694736, acc 0.5\n",
      "2018-05-04T11:32:31.650492: step 871, loss 0.694698, acc 0.523438\n",
      "2018-05-04T11:32:33.414645: step 872, loss 0.69321, acc 0.460938\n",
      "2018-05-04T11:32:35.138618: step 873, loss 0.693737, acc 0.5\n",
      "2018-05-04T11:32:36.887344: step 874, loss 0.693371, acc 0.5625\n",
      "2018-05-04T11:32:38.581174: step 875, loss 0.693266, acc 0.4375\n",
      "2018-05-04T11:32:40.283458: step 876, loss 0.692483, acc 0.539062\n",
      "2018-05-04T11:32:42.122354: step 877, loss 0.693848, acc 0.492188\n",
      "2018-05-04T11:32:43.808644: step 878, loss 0.694176, acc 0.4375\n",
      "2018-05-04T11:32:45.518773: step 879, loss 0.692515, acc 0.546875\n",
      "2018-05-04T11:32:47.240896: step 880, loss 0.6915, acc 0.578125\n",
      "2018-05-04T11:32:49.124680: step 881, loss 0.692559, acc 0.507812\n",
      "2018-05-04T11:32:51.129779: step 882, loss 0.69413, acc 0.476562\n",
      "2018-05-04T11:32:53.157438: step 883, loss 0.693593, acc 0.5\n",
      "2018-05-04T11:32:55.118805: step 884, loss 0.693362, acc 0.476562\n",
      "2018-05-04T11:32:57.099354: step 885, loss 0.69087, acc 0.5625\n",
      "2018-05-04T11:32:58.975642: step 886, loss 0.692256, acc 0.53125\n",
      "2018-05-04T11:33:00.771853: step 887, loss 0.692987, acc 0.492188\n",
      "2018-05-04T11:33:02.521820: step 888, loss 0.690943, acc 0.53125\n",
      "2018-05-04T11:33:04.246140: step 889, loss 0.69594, acc 0.476562\n",
      "2018-05-04T11:33:05.921565: step 890, loss 0.694281, acc 0.484375\n",
      "2018-05-04T11:33:07.603731: step 891, loss 0.698041, acc 0.429688\n",
      "2018-05-04T11:33:09.464551: step 892, loss 0.694436, acc 0.539062\n",
      "2018-05-04T11:33:11.400430: step 893, loss 0.694429, acc 0.460938\n",
      "2018-05-04T11:33:13.324576: step 894, loss 0.690079, acc 0.5625\n",
      "2018-05-04T11:33:15.063690: step 895, loss 0.691534, acc 0.546875\n",
      "2018-05-04T11:33:16.767362: step 896, loss 0.691129, acc 0.515625\n",
      "2018-05-04T11:33:18.469490: step 897, loss 0.695925, acc 0.46875\n",
      "2018-05-04T11:33:20.172180: step 898, loss 0.69251, acc 0.523438\n",
      "2018-05-04T11:33:21.926569: step 899, loss 0.695692, acc 0.46875\n",
      "2018-05-04T11:33:23.617457: step 900, loss 0.695534, acc 0.421875\n",
      "2018-05-04T11:33:25.334569: step 901, loss 0.692421, acc 0.515625\n",
      "2018-05-04T11:33:27.071885: step 902, loss 0.693369, acc 0.53125\n",
      "2018-05-04T11:33:28.773380: step 903, loss 0.69446, acc 0.492188\n",
      "2018-05-04T11:33:30.475928: step 904, loss 0.691782, acc 0.546875\n",
      "2018-05-04T11:33:32.219761: step 905, loss 0.693448, acc 0.484375\n",
      "2018-05-04T11:33:33.935447: step 906, loss 0.695062, acc 0.429688\n",
      "2018-05-04T11:33:35.631542: step 907, loss 0.693172, acc 0.484375\n",
      "2018-05-04T11:33:37.341504: step 908, loss 0.69281, acc 0.546875\n",
      "2018-05-04T11:33:39.041370: step 909, loss 0.692986, acc 0.554688\n",
      "2018-05-04T11:33:40.732990: step 910, loss 0.692964, acc 0.5\n",
      "2018-05-04T11:33:42.473232: step 911, loss 0.692672, acc 0.5625\n",
      "2018-05-04T11:33:44.266104: step 912, loss 0.694054, acc 0.429688\n",
      "2018-05-04T11:33:46.000398: step 913, loss 0.694168, acc 0.421875\n",
      "2018-05-04T11:33:47.726290: step 914, loss 0.693835, acc 0.460938\n",
      "2018-05-04T11:33:49.467036: step 915, loss 0.69382, acc 0.460938\n",
      "2018-05-04T11:33:51.202598: step 916, loss 0.693161, acc 0.46875\n",
      "2018-05-04T11:33:53.005375: step 917, loss 0.692883, acc 0.585938\n",
      "2018-05-04T11:33:54.768215: step 918, loss 0.693232, acc 0.46875\n",
      "2018-05-04T11:33:56.532445: step 919, loss 0.693044, acc 0.492188\n",
      "2018-05-04T11:33:58.302730: step 920, loss 0.693877, acc 0.460938\n",
      "2018-05-04T11:34:00.070905: step 921, loss 0.692848, acc 0.507812\n",
      "2018-05-04T11:34:02.027789: step 922, loss 0.693058, acc 0.476562\n",
      "2018-05-04T11:34:03.947915: step 923, loss 0.694303, acc 0.414062\n",
      "2018-05-04T11:34:05.793608: step 924, loss 0.693865, acc 0.507812\n",
      "2018-05-04T11:34:07.651179: step 925, loss 0.693802, acc 0.46875\n",
      "2018-05-04T11:34:09.477799: step 926, loss 0.693058, acc 0.492188\n",
      "2018-05-04T11:34:11.336942: step 927, loss 0.692923, acc 0.554688\n",
      "2018-05-04T11:34:13.132102: step 928, loss 0.693511, acc 0.476562\n",
      "2018-05-04T11:34:15.083460: step 929, loss 0.693853, acc 0.453125\n",
      "2018-05-04T11:34:16.821664: step 930, loss 0.693354, acc 0.445312\n",
      "2018-05-04T11:34:18.674458: step 931, loss 0.693114, acc 0.492188\n",
      "2018-05-04T11:34:20.503260: step 932, loss 0.69293, acc 0.523438\n",
      "2018-05-04T11:34:22.335430: step 933, loss 0.693187, acc 0.484375\n",
      "2018-05-04T11:34:24.140194: step 934, loss 0.693363, acc 0.445312\n",
      "2018-05-04T11:34:25.937814: step 935, loss 0.693276, acc 0.484375\n",
      "2018-05-04T11:34:27.748875: step 936, loss 0.693472, acc 0.476562\n",
      "2018-05-04T11:34:29.560460: step 937, loss 0.69284, acc 0.507812\n",
      "2018-05-04T11:34:31.444027: step 938, loss 0.692846, acc 0.523438\n",
      "2018-05-04T11:34:33.277813: step 939, loss 0.693865, acc 0.484375\n",
      "2018-05-04T11:34:35.205451: step 940, loss 0.692458, acc 0.546875\n",
      "2018-05-04T11:34:37.105261: step 941, loss 0.693644, acc 0.46875\n",
      "2018-05-04T11:34:39.014343: step 942, loss 0.693675, acc 0.445312\n",
      "2018-05-04T11:34:40.932193: step 943, loss 0.694102, acc 0.453125\n",
      "2018-05-04T11:34:42.752263: step 944, loss 0.693588, acc 0.5\n",
      "2018-05-04T11:34:44.568804: step 945, loss 0.693363, acc 0.453125\n",
      "2018-05-04T11:34:46.503674: step 946, loss 0.691964, acc 0.515625\n",
      "2018-05-04T11:34:48.361961: step 947, loss 0.693114, acc 0.515625\n",
      "2018-05-04T11:34:50.224805: step 948, loss 0.692558, acc 0.5\n",
      "2018-05-04T11:34:52.086458: step 949, loss 0.692483, acc 0.507812\n",
      "2018-05-04T11:34:53.883626: step 950, loss 0.694835, acc 0.453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:34:55.701379: step 951, loss 0.695177, acc 0.46875\n",
      "2018-05-04T11:34:57.505559: step 952, loss 0.691377, acc 0.539062\n",
      "2018-05-04T11:34:59.305291: step 953, loss 0.694357, acc 0.492188\n",
      "2018-05-04T11:35:01.116743: step 954, loss 0.696225, acc 0.46875\n",
      "2018-05-04T11:35:02.887085: step 955, loss 0.6938, acc 0.460938\n",
      "2018-05-04T11:35:04.665403: step 956, loss 0.694097, acc 0.46875\n",
      "2018-05-04T11:35:06.462871: step 957, loss 0.692917, acc 0.523438\n",
      "2018-05-04T11:35:08.159543: step 958, loss 0.692683, acc 0.492188\n",
      "2018-05-04T11:35:09.852801: step 959, loss 0.69145, acc 0.539062\n",
      "2018-05-04T11:35:11.636340: step 960, loss 0.690859, acc 0.546875\n",
      "2018-05-04T11:35:13.373877: step 961, loss 0.690853, acc 0.554688\n",
      "2018-05-04T11:35:15.172888: step 962, loss 0.700846, acc 0.453125\n",
      "2018-05-04T11:35:17.070965: step 963, loss 0.693197, acc 0.5\n",
      "2018-05-04T11:35:18.842960: step 964, loss 0.690252, acc 0.578125\n",
      "2018-05-04T11:35:20.623870: step 965, loss 0.683215, acc 0.617188\n",
      "2018-05-04T11:35:22.417731: step 966, loss 0.693201, acc 0.5\n",
      "2018-05-04T11:35:24.308693: step 967, loss 0.696388, acc 0.523438\n",
      "2018-05-04T11:35:26.236833: step 968, loss 0.69306, acc 0.5\n",
      "2018-05-04T11:35:28.103949: step 969, loss 0.692043, acc 0.539062\n",
      "2018-05-04T11:35:30.056833: step 970, loss 0.696627, acc 0.476562\n",
      "2018-05-04T11:35:32.129817: step 971, loss 0.692194, acc 0.523438\n",
      "2018-05-04T11:35:34.265282: step 972, loss 0.695708, acc 0.507812\n",
      "2018-05-04T11:35:36.107097: step 973, loss 0.689589, acc 0.5625\n",
      "2018-05-04T11:35:38.095772: step 974, loss 0.691441, acc 0.53125\n",
      "2018-05-04T11:35:40.253104: step 975, loss 0.692881, acc 0.507812\n",
      "2018-05-04T11:35:42.320286: step 976, loss 0.691873, acc 0.523438\n",
      "2018-05-04T11:35:44.190329: step 977, loss 0.69277, acc 0.515625\n",
      "2018-05-04T11:35:46.018418: step 978, loss 0.694038, acc 0.5\n",
      "2018-05-04T11:35:47.885765: step 979, loss 0.689563, acc 0.5625\n",
      "2018-05-04T11:35:49.676276: step 980, loss 0.690569, acc 0.546875\n",
      "2018-05-04T11:35:51.502654: step 981, loss 0.692775, acc 0.515625\n",
      "2018-05-04T11:35:53.240828: step 982, loss 0.692417, acc 0.515625\n",
      "2018-05-04T11:35:55.000420: step 983, loss 0.692878, acc 0.507812\n",
      "2018-05-04T11:35:56.724581: step 984, loss 0.698293, acc 0.445312\n",
      "2018-05-04T11:35:58.438008: step 985, loss 0.689393, acc 0.554688\n",
      "2018-05-04T11:36:00.088279: step 986, loss 0.693391, acc 0.5\n",
      "2018-05-04T11:36:01.863121: step 987, loss 0.692465, acc 0.515625\n",
      "2018-05-04T11:36:03.611333: step 988, loss 0.695513, acc 0.460938\n",
      "2018-05-04T11:36:05.341811: step 989, loss 0.691405, acc 0.507812\n",
      "2018-05-04T11:36:07.073073: step 990, loss 0.693222, acc 0.53125\n",
      "2018-05-04T11:36:08.861914: step 991, loss 0.692607, acc 0.539062\n",
      "2018-05-04T11:36:10.703151: step 992, loss 0.700279, acc 0.40625\n",
      "2018-05-04T11:36:12.717580: step 993, loss 0.695454, acc 0.507812\n",
      "2018-05-04T11:36:14.699472: step 994, loss 0.695584, acc 0.484375\n",
      "2018-05-04T11:36:16.841198: step 995, loss 0.692329, acc 0.53125\n",
      "2018-05-04T11:36:19.016828: step 996, loss 0.692755, acc 0.484375\n",
      "2018-05-04T11:36:21.048201: step 997, loss 0.694703, acc 0.453125\n",
      "2018-05-04T11:36:23.104914: step 998, loss 0.690993, acc 0.53125\n",
      "2018-05-04T11:36:24.766084: step 999, loss 0.694851, acc 0.46875\n",
      "2018-05-04T11:36:26.490784: step 1000, loss 0.693561, acc 0.476562\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/1525399442/checkpoints/model-1000\n",
      "\n",
      "2018-05-04T11:36:28.352940: step 1001, loss 0.693279, acc 0.484375\n",
      "2018-05-04T11:36:30.081507: step 1002, loss 0.695293, acc 0.46875\n",
      "2018-05-04T11:36:31.854286: step 1003, loss 0.68978, acc 0.59375\n",
      "2018-05-04T11:36:33.578157: step 1004, loss 0.691942, acc 0.53125\n",
      "2018-05-04T11:36:35.331547: step 1005, loss 0.694021, acc 0.484375\n",
      "2018-05-04T11:36:37.046290: step 1006, loss 0.692647, acc 0.515625\n",
      "2018-05-04T11:36:38.739012: step 1007, loss 0.695297, acc 0.46875\n",
      "2018-05-04T11:36:40.412128: step 1008, loss 0.69472, acc 0.476562\n",
      "2018-05-04T11:36:41.977565: step 1009, loss 0.693301, acc 0.53125\n",
      "2018-05-04T11:36:43.634514: step 1010, loss 0.694429, acc 0.4375\n",
      "2018-05-04T11:36:45.390882: step 1011, loss 0.693221, acc 0.523438\n",
      "2018-05-04T11:36:47.117276: step 1012, loss 0.693444, acc 0.453125\n",
      "2018-05-04T11:36:49.022226: step 1013, loss 0.693163, acc 0.484375\n",
      "2018-05-04T11:36:50.811937: step 1014, loss 0.694045, acc 0.46875\n",
      "2018-05-04T11:36:52.553285: step 1015, loss 0.692537, acc 0.515625\n",
      "2018-05-04T11:36:54.281422: step 1016, loss 0.692351, acc 0.53125\n",
      "2018-05-04T11:36:55.962820: step 1017, loss 0.694429, acc 0.523438\n",
      "2018-05-04T11:36:57.626718: step 1018, loss 0.694949, acc 0.476562\n",
      "2018-05-04T11:36:59.172244: step 1019, loss 0.690724, acc 0.523438\n",
      "2018-05-04T11:37:00.729341: step 1020, loss 0.699053, acc 0.484375\n",
      "2018-05-04T11:37:02.474874: step 1021, loss 0.696335, acc 0.4375\n",
      "2018-05-04T11:37:04.224167: step 1022, loss 0.696037, acc 0.476562\n",
      "2018-05-04T11:37:05.937065: step 1023, loss 0.690793, acc 0.546875\n",
      "2018-05-04T11:37:07.633095: step 1024, loss 0.691578, acc 0.539062\n",
      "2018-05-04T11:37:09.343947: step 1025, loss 0.693005, acc 0.5\n",
      "2018-05-04T11:37:11.068966: step 1026, loss 0.694624, acc 0.484375\n",
      "2018-05-04T11:37:12.783982: step 1027, loss 0.697645, acc 0.40625\n",
      "2018-05-04T11:37:14.502178: step 1028, loss 0.693507, acc 0.515625\n",
      "2018-05-04T11:37:16.212530: step 1029, loss 0.695368, acc 0.476562\n",
      "2018-05-04T11:37:17.992653: step 1030, loss 0.695522, acc 0.460938\n",
      "2018-05-04T11:37:19.878522: step 1031, loss 0.69358, acc 0.53125\n",
      "2018-05-04T11:37:21.491596: step 1032, loss 0.693742, acc 0.515625\n",
      "2018-05-04T11:37:23.050768: step 1033, loss 0.692788, acc 0.53125\n",
      "2018-05-04T11:37:24.656034: step 1034, loss 0.691724, acc 0.484375\n",
      "2018-05-04T11:37:26.267934: step 1035, loss 0.693593, acc 0.484375\n",
      "2018-05-04T11:37:27.907513: step 1036, loss 0.692095, acc 0.5625\n",
      "2018-05-04T11:37:29.660927: step 1037, loss 0.690899, acc 0.539062\n",
      "2018-05-04T11:37:31.446929: step 1038, loss 0.691246, acc 0.53125\n",
      "2018-05-04T11:37:33.201898: step 1039, loss 0.69421, acc 0.492188\n",
      "2018-05-04T11:37:35.056764: step 1040, loss 0.694325, acc 0.46875\n",
      "2018-05-04T11:37:36.935050: step 1041, loss 0.695622, acc 0.484375\n",
      "2018-05-04T11:37:38.635323: step 1042, loss 0.689613, acc 0.578125\n",
      "2018-05-04T11:37:40.368721: step 1043, loss 0.69554, acc 0.5\n",
      "2018-05-04T11:37:42.118264: step 1044, loss 0.695695, acc 0.476562\n",
      "2018-05-04T11:37:43.838236: step 1045, loss 0.691588, acc 0.484375\n",
      "2018-05-04T11:37:45.537853: step 1046, loss 0.692747, acc 0.507812\n",
      "2018-05-04T11:37:47.278730: step 1047, loss 0.6902, acc 0.609375\n",
      "2018-05-04T11:37:49.233235: step 1048, loss 0.690791, acc 0.554688\n",
      "2018-05-04T11:37:50.987469: step 1049, loss 0.69435, acc 0.507812\n",
      "2018-05-04T11:37:52.684720: step 1050, loss 0.693953, acc 0.515625\n",
      "2018-05-04T11:37:54.395674: step 1051, loss 0.694552, acc 0.484375\n",
      "2018-05-04T11:37:56.089494: step 1052, loss 0.692689, acc 0.523438\n",
      "2018-05-04T11:37:57.784289: step 1053, loss 0.693399, acc 0.484375\n",
      "2018-05-04T11:37:59.484637: step 1054, loss 0.693034, acc 0.484375\n",
      "2018-05-04T11:38:01.240425: step 1055, loss 0.694475, acc 0.460938\n",
      "2018-05-04T11:38:02.958814: step 1056, loss 0.696712, acc 0.445312\n",
      "2018-05-04T11:38:04.675927: step 1057, loss 0.691831, acc 0.523438\n",
      "2018-05-04T11:38:06.413935: step 1058, loss 0.696083, acc 0.46875\n",
      "2018-05-04T11:38:08.109819: step 1059, loss 0.693223, acc 0.515625\n",
      "2018-05-04T11:38:09.738221: step 1060, loss 0.691871, acc 0.554688\n",
      "2018-05-04T11:38:11.501390: step 1061, loss 0.69488, acc 0.453125\n",
      "2018-05-04T11:38:13.195093: step 1062, loss 0.693183, acc 0.5\n",
      "2018-05-04T11:38:14.881804: step 1063, loss 0.693655, acc 0.476562\n",
      "2018-05-04T11:38:16.566061: step 1064, loss 0.690303, acc 0.601562\n",
      "2018-05-04T11:38:18.359906: step 1065, loss 0.69414, acc 0.5\n",
      "2018-05-04T11:38:20.195898: step 1066, loss 0.692687, acc 0.53125\n",
      "2018-05-04T11:38:21.973212: step 1067, loss 0.691342, acc 0.539062\n",
      "2018-05-04T11:38:23.686702: step 1068, loss 0.692774, acc 0.53125\n",
      "2018-05-04T11:38:25.382118: step 1069, loss 0.694067, acc 0.46875\n",
      "2018-05-04T11:38:27.068476: step 1070, loss 0.693512, acc 0.476562\n",
      "2018-05-04T11:38:28.746023: step 1071, loss 0.69336, acc 0.476562\n",
      "2018-05-04T11:38:30.444124: step 1072, loss 0.693806, acc 0.546875\n",
      "2018-05-04T11:38:32.102421: step 1073, loss 0.693382, acc 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:38:33.802952: step 1074, loss 0.695104, acc 0.476562\n",
      "2018-05-04T11:38:35.501744: step 1075, loss 0.693439, acc 0.484375\n",
      "2018-05-04T11:38:37.185041: step 1076, loss 0.692759, acc 0.53125\n",
      "2018-05-04T11:38:38.883504: step 1077, loss 0.692773, acc 0.515625\n",
      "2018-05-04T11:38:40.594355: step 1078, loss 0.692058, acc 0.523438\n",
      "2018-05-04T11:38:42.325318: step 1079, loss 0.693934, acc 0.484375\n",
      "2018-05-04T11:38:44.030943: step 1080, loss 0.693527, acc 0.5\n",
      "2018-05-04T11:38:45.715564: step 1081, loss 0.687972, acc 0.601562\n",
      "2018-05-04T11:38:47.462769: step 1082, loss 0.693277, acc 0.5\n",
      "2018-05-04T11:38:49.408198: step 1083, loss 0.691479, acc 0.53125\n",
      "2018-05-04T11:38:51.112749: step 1084, loss 0.694359, acc 0.507812\n",
      "2018-05-04T11:38:52.851903: step 1085, loss 0.695811, acc 0.46875\n",
      "2018-05-04T11:38:54.566365: step 1086, loss 0.693448, acc 0.515625\n",
      "2018-05-04T11:38:56.272515: step 1087, loss 0.697025, acc 0.429688\n",
      "2018-05-04T11:38:57.966775: step 1088, loss 0.691534, acc 0.523438\n",
      "2018-05-04T11:38:59.663477: step 1089, loss 0.693188, acc 0.507812\n",
      "2018-05-04T11:39:01.369016: step 1090, loss 0.691833, acc 0.515625\n",
      "2018-05-04T11:39:03.099233: step 1091, loss 0.690922, acc 0.570312\n",
      "2018-05-04T11:39:04.806740: step 1092, loss 0.697022, acc 0.421875\n",
      "2018-05-04T11:39:06.515536: step 1093, loss 0.694045, acc 0.492188\n",
      "2018-05-04T11:39:08.193895: step 1094, loss 0.692667, acc 0.484375\n",
      "2018-05-04T11:39:09.903885: step 1095, loss 0.694144, acc 0.53125\n",
      "2018-05-04T11:39:11.605800: step 1096, loss 0.694914, acc 0.476562\n",
      "2018-05-04T11:39:13.354497: step 1097, loss 0.691297, acc 0.539062\n",
      "2018-05-04T11:39:15.060145: step 1098, loss 0.692727, acc 0.515625\n",
      "2018-05-04T11:39:16.772628: step 1099, loss 0.696742, acc 0.40625\n",
      "2018-05-04T11:39:18.581119: step 1100, loss 0.692687, acc 0.515625\n",
      "2018-05-04T11:39:20.427041: step 1101, loss 0.691636, acc 0.546875\n",
      "2018-05-04T11:39:22.140467: step 1102, loss 0.694018, acc 0.460938\n",
      "2018-05-04T11:39:23.928112: step 1103, loss 0.694845, acc 0.492188\n",
      "2018-05-04T11:39:25.641588: step 1104, loss 0.691373, acc 0.539062\n",
      "2018-05-04T11:39:27.330675: step 1105, loss 0.691523, acc 0.539062\n",
      "2018-05-04T11:39:29.039787: step 1106, loss 0.691999, acc 0.523438\n",
      "2018-05-04T11:39:30.763072: step 1107, loss 0.693826, acc 0.515625\n",
      "2018-05-04T11:39:32.482593: step 1108, loss 0.69334, acc 0.484375\n",
      "2018-05-04T11:39:34.212133: step 1109, loss 0.694947, acc 0.476562\n",
      "2018-05-04T11:39:35.935383: step 1110, loss 0.691881, acc 0.554688\n",
      "2018-05-04T11:39:37.616185: step 1111, loss 0.694908, acc 0.445312\n",
      "2018-05-04T11:39:39.325080: step 1112, loss 0.693576, acc 0.492188\n",
      "2018-05-04T11:39:41.034421: step 1113, loss 0.691739, acc 0.523438\n",
      "2018-05-04T11:39:42.755706: step 1114, loss 0.689996, acc 0.578125\n",
      "2018-05-04T11:39:44.447769: step 1115, loss 0.694685, acc 0.53125\n",
      "2018-05-04T11:39:46.136425: step 1116, loss 0.696136, acc 0.476562\n",
      "2018-05-04T11:39:47.905766: step 1117, loss 0.693245, acc 0.53125\n",
      "2018-05-04T11:39:49.772251: step 1118, loss 0.69056, acc 0.601562\n",
      "2018-05-04T11:39:51.512024: step 1119, loss 0.694837, acc 0.46875\n",
      "2018-05-04T11:39:53.209344: step 1120, loss 0.69358, acc 0.5\n",
      "2018-05-04T11:39:54.897847: step 1121, loss 0.692407, acc 0.53125\n",
      "2018-05-04T11:39:56.592795: step 1122, loss 0.693461, acc 0.515625\n",
      "2018-05-04T11:39:58.283474: step 1123, loss 0.694192, acc 0.46875\n",
      "2018-05-04T11:39:59.973272: step 1124, loss 0.693978, acc 0.46875\n",
      "2018-05-04T11:40:01.701420: step 1125, loss 0.691822, acc 0.539062\n",
      "2018-05-04T11:40:03.414408: step 1126, loss 0.693382, acc 0.492188\n",
      "2018-05-04T11:40:05.142309: step 1127, loss 0.69323, acc 0.5\n",
      "2018-05-04T11:40:06.831338: step 1128, loss 0.69328, acc 0.507812\n",
      "2018-05-04T11:40:08.514354: step 1129, loss 0.693848, acc 0.507812\n",
      "2018-05-04T11:40:10.201791: step 1130, loss 0.69189, acc 0.570312\n",
      "2018-05-04T11:40:11.958153: step 1131, loss 0.692517, acc 0.523438\n",
      "2018-05-04T11:40:13.665372: step 1132, loss 0.693203, acc 0.492188\n",
      "2018-05-04T11:40:15.373660: step 1133, loss 0.693705, acc 0.484375\n",
      "2018-05-04T11:40:17.062549: step 1134, loss 0.691123, acc 0.546875\n",
      "2018-05-04T11:40:18.962733: step 1135, loss 0.691016, acc 0.546875\n",
      "2018-05-04T11:40:20.685367: step 1136, loss 0.693284, acc 0.484375\n",
      "2018-05-04T11:40:22.453717: step 1137, loss 0.693515, acc 0.453125\n",
      "2018-05-04T11:40:24.149727: step 1138, loss 0.697382, acc 0.421875\n",
      "2018-05-04T11:40:25.871998: step 1139, loss 0.693327, acc 0.539062\n",
      "2018-05-04T11:40:27.592878: step 1140, loss 0.694403, acc 0.5\n",
      "2018-05-04T11:40:29.308879: step 1141, loss 0.693306, acc 0.539062\n",
      "2018-05-04T11:40:31.035945: step 1142, loss 0.693646, acc 0.492188\n",
      "2018-05-04T11:40:32.828977: step 1143, loss 0.694667, acc 0.484375\n",
      "2018-05-04T11:40:34.541389: step 1144, loss 0.691697, acc 0.53125\n",
      "2018-05-04T11:40:36.197238: step 1145, loss 0.694142, acc 0.46875\n",
      "2018-05-04T11:40:37.874007: step 1146, loss 0.694776, acc 0.476562\n",
      "2018-05-04T11:40:39.438722: step 1147, loss 0.692759, acc 0.507812\n",
      "2018-05-04T11:40:41.211941: step 1148, loss 0.692289, acc 0.453125\n",
      "2018-05-04T11:40:42.903346: step 1149, loss 0.692876, acc 0.5625\n",
      "2018-05-04T11:40:44.625460: step 1150, loss 0.692377, acc 0.507812\n",
      "2018-05-04T11:40:46.337769: step 1151, loss 0.692296, acc 0.5\n",
      "2018-05-04T11:40:48.151797: step 1152, loss 0.695855, acc 0.4375\n",
      "2018-05-04T11:40:50.030548: step 1153, loss 0.689483, acc 0.59375\n",
      "2018-05-04T11:40:51.791869: step 1154, loss 0.695759, acc 0.460938\n",
      "2018-05-04T11:40:53.502480: step 1155, loss 0.694778, acc 0.484375\n",
      "2018-05-04T11:40:55.210195: step 1156, loss 0.691069, acc 0.539062\n",
      "2018-05-04T11:40:56.907080: step 1157, loss 0.693565, acc 0.492188\n",
      "2018-05-04T11:40:58.607610: step 1158, loss 0.695423, acc 0.445312\n",
      "2018-05-04T11:41:00.317440: step 1159, loss 0.69126, acc 0.5625\n",
      "2018-05-04T11:41:02.064107: step 1160, loss 0.695725, acc 0.421875\n",
      "2018-05-04T11:41:03.786076: step 1161, loss 0.691778, acc 0.523438\n",
      "2018-05-04T11:41:05.495408: step 1162, loss 0.693879, acc 0.453125\n",
      "2018-05-04T11:41:07.193572: step 1163, loss 0.691779, acc 0.5\n",
      "2018-05-04T11:41:08.873862: step 1164, loss 0.69189, acc 0.578125\n",
      "2018-05-04T11:41:10.592111: step 1165, loss 0.694871, acc 0.429688\n",
      "2018-05-04T11:41:12.336920: step 1166, loss 0.692367, acc 0.578125\n",
      "2018-05-04T11:41:14.030260: step 1167, loss 0.693244, acc 0.53125\n",
      "2018-05-04T11:41:15.741261: step 1168, loss 0.694081, acc 0.515625\n",
      "2018-05-04T11:41:17.465436: step 1169, loss 0.692548, acc 0.523438\n",
      "2018-05-04T11:41:19.390407: step 1170, loss 0.693594, acc 0.460938\n",
      "2018-05-04T11:41:21.149986: step 1171, loss 0.69192, acc 0.515625\n",
      "2018-05-04T11:41:22.849418: step 1172, loss 0.693595, acc 0.476562\n",
      "2018-05-04T11:41:24.522310: step 1173, loss 0.693074, acc 0.539062\n",
      "2018-05-04T11:41:26.214214: step 1174, loss 0.694512, acc 0.484375\n",
      "2018-05-04T11:41:27.937013: step 1175, loss 0.692731, acc 0.507812\n",
      "2018-05-04T11:41:29.649069: step 1176, loss 0.694104, acc 0.476562\n",
      "2018-05-04T11:41:31.398976: step 1177, loss 0.691583, acc 0.515625\n",
      "2018-05-04T11:41:33.085196: step 1178, loss 0.693726, acc 0.492188\n",
      "2018-05-04T11:41:34.808062: step 1179, loss 0.696309, acc 0.421875\n",
      "2018-05-04T11:41:36.493910: step 1180, loss 0.694824, acc 0.445312\n",
      "2018-05-04T11:41:38.205462: step 1181, loss 0.694546, acc 0.4375\n",
      "2018-05-04T11:41:39.890815: step 1182, loss 0.694359, acc 0.492188\n",
      "2018-05-04T11:41:41.506175: step 1183, loss 0.694385, acc 0.453125\n",
      "2018-05-04T11:41:43.036819: step 1184, loss 0.691917, acc 0.492188\n",
      "2018-05-04T11:41:44.584548: step 1185, loss 0.694806, acc 0.421875\n",
      "2018-05-04T11:41:46.298925: step 1186, loss 0.695208, acc 0.414062\n",
      "2018-05-04T11:41:48.432599: step 1187, loss 0.69309, acc 0.5\n",
      "2018-05-04T11:41:50.580182: step 1188, loss 0.694014, acc 0.445312\n",
      "2018-05-04T11:41:52.473733: step 1189, loss 0.693923, acc 0.453125\n",
      "2018-05-04T11:41:54.294951: step 1190, loss 0.694237, acc 0.507812\n",
      "2018-05-04T11:41:56.393867: step 1191, loss 0.693626, acc 0.5\n",
      "2018-05-04T11:41:58.372408: step 1192, loss 0.69253, acc 0.5\n",
      "2018-05-04T11:42:00.228130: step 1193, loss 0.692162, acc 0.492188\n",
      "2018-05-04T11:42:02.168416: step 1194, loss 0.693403, acc 0.492188\n",
      "2018-05-04T11:42:03.903610: step 1195, loss 0.694227, acc 0.476562\n",
      "2018-05-04T11:42:05.635711: step 1196, loss 0.693254, acc 0.484375\n",
      "2018-05-04T11:42:07.358720: step 1197, loss 0.692884, acc 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:42:09.086926: step 1198, loss 0.695852, acc 0.414062\n",
      "2018-05-04T11:42:10.834666: step 1199, loss 0.693961, acc 0.515625\n",
      "2018-05-04T11:42:12.575050: step 1200, loss 0.693147, acc 0.523438\n",
      "2018-05-04T11:42:14.135330: step 1201, loss 0.692488, acc 0.523438\n",
      "2018-05-04T11:42:15.707121: step 1202, loss 0.694212, acc 0.476562\n",
      "2018-05-04T11:42:17.380961: step 1203, loss 0.693344, acc 0.515625\n",
      "2018-05-04T11:42:19.144998: step 1204, loss 0.692988, acc 0.507812\n",
      "2018-05-04T11:42:20.998647: step 1205, loss 0.693157, acc 0.460938\n",
      "2018-05-04T11:42:22.793360: step 1206, loss 0.691889, acc 0.585938\n",
      "2018-05-04T11:42:24.551161: step 1207, loss 0.693722, acc 0.5\n",
      "2018-05-04T11:42:26.316319: step 1208, loss 0.693809, acc 0.453125\n",
      "2018-05-04T11:42:28.102778: step 1209, loss 0.69329, acc 0.484375\n",
      "2018-05-04T11:42:29.887711: step 1210, loss 0.694226, acc 0.445312\n",
      "2018-05-04T11:42:31.636352: step 1211, loss 0.693323, acc 0.492188\n",
      "2018-05-04T11:42:33.488966: step 1212, loss 0.693006, acc 0.539062\n",
      "2018-05-04T11:42:35.329932: step 1213, loss 0.693071, acc 0.570312\n",
      "2018-05-04T11:42:37.194834: step 1214, loss 0.693948, acc 0.4375\n",
      "2018-05-04T11:42:39.049293: step 1215, loss 0.694837, acc 0.375\n",
      "2018-05-04T11:42:40.894092: step 1216, loss 0.693424, acc 0.46875\n",
      "2018-05-04T11:42:42.656178: step 1217, loss 0.693167, acc 0.492188\n",
      "2018-05-04T11:42:44.493514: step 1218, loss 0.692991, acc 0.523438\n",
      "2018-05-04T11:42:46.399063: step 1219, loss 0.693297, acc 0.515625\n",
      "2018-05-04T11:42:48.336435: step 1220, loss 0.694328, acc 0.46875\n",
      "2018-05-04T11:42:50.276032: step 1221, loss 0.692504, acc 0.539062\n",
      "2018-05-04T11:42:52.426699: step 1222, loss 0.693147, acc 0.5\n",
      "2018-05-04T11:42:54.383723: step 1223, loss 0.693721, acc 0.5\n",
      "2018-05-04T11:42:56.300675: step 1224, loss 0.692004, acc 0.546875\n",
      "2018-05-04T11:42:58.215201: step 1225, loss 0.693568, acc 0.46875\n",
      "2018-05-04T11:43:00.123763: step 1226, loss 0.692077, acc 0.507812\n",
      "2018-05-04T11:43:02.093523: step 1227, loss 0.692966, acc 0.492188\n",
      "2018-05-04T11:43:04.022031: step 1228, loss 0.693644, acc 0.484375\n",
      "2018-05-04T11:43:05.945552: step 1229, loss 0.690812, acc 0.570312\n",
      "2018-05-04T11:43:07.869074: step 1230, loss 0.691283, acc 0.5625\n",
      "2018-05-04T11:43:09.850868: step 1231, loss 0.691783, acc 0.5625\n",
      "2018-05-04T11:43:11.913910: step 1232, loss 0.693294, acc 0.492188\n",
      "2018-05-04T11:43:13.896021: step 1233, loss 0.68941, acc 0.59375\n",
      "2018-05-04T11:43:15.843678: step 1234, loss 0.695004, acc 0.460938\n",
      "2018-05-04T11:43:17.773064: step 1235, loss 0.693198, acc 0.5\n",
      "2018-05-04T11:43:19.687516: step 1236, loss 0.694804, acc 0.484375\n",
      "2018-05-04T11:43:21.772097: step 1237, loss 0.691285, acc 0.523438\n",
      "2018-05-04T11:43:23.911578: step 1238, loss 0.694439, acc 0.515625\n",
      "2018-05-04T11:43:25.894150: step 1239, loss 0.694135, acc 0.507812\n",
      "2018-05-04T11:43:27.870993: step 1240, loss 0.695031, acc 0.484375\n",
      "2018-05-04T11:43:29.865791: step 1241, loss 0.696691, acc 0.453125\n",
      "2018-05-04T11:43:31.919365: step 1242, loss 0.699107, acc 0.421875\n",
      "2018-05-04T11:43:33.928831: step 1243, loss 0.695098, acc 0.492188\n",
      "2018-05-04T11:43:35.974232: step 1244, loss 0.698763, acc 0.414062\n",
      "2018-05-04T11:43:38.050628: step 1245, loss 0.693944, acc 0.5\n",
      "2018-05-04T11:43:39.963133: step 1246, loss 0.696048, acc 0.453125\n",
      "2018-05-04T11:43:41.977189: step 1247, loss 0.694159, acc 0.5\n",
      "2018-05-04T11:43:43.890612: step 1248, loss 0.692543, acc 0.523438\n",
      "2018-05-04T11:43:45.734689: step 1249, loss 0.693058, acc 0.492188\n",
      "2018-05-04T11:43:47.600188: step 1250, loss 0.693118, acc 0.515625\n",
      "2018-05-04T11:43:49.410904: step 1251, loss 0.693463, acc 0.476562\n",
      "2018-05-04T11:43:51.216383: step 1252, loss 0.693022, acc 0.515625\n",
      "2018-05-04T11:43:53.203617: step 1253, loss 0.692645, acc 0.570312\n",
      "2018-05-04T11:43:55.125994: step 1254, loss 0.693103, acc 0.492188\n",
      "2018-05-04T11:43:56.985833: step 1255, loss 0.693119, acc 0.507812\n",
      "2018-05-04T11:43:58.720948: step 1256, loss 0.693154, acc 0.507812\n",
      "2018-05-04T11:44:00.455056: step 1257, loss 0.693223, acc 0.46875\n",
      "2018-05-04T11:44:02.298340: step 1258, loss 0.693084, acc 0.546875\n",
      "2018-05-04T11:44:04.130495: step 1259, loss 0.693351, acc 0.40625\n",
      "2018-05-04T11:44:05.954917: step 1260, loss 0.693037, acc 0.546875\n",
      "2018-05-04T11:44:07.720382: step 1261, loss 0.693208, acc 0.476562\n",
      "2018-05-04T11:44:09.507445: step 1262, loss 0.693168, acc 0.492188\n",
      "2018-05-04T11:44:11.312097: step 1263, loss 0.69303, acc 0.554688\n",
      "2018-05-04T11:44:13.061157: step 1264, loss 0.693247, acc 0.445312\n",
      "2018-05-04T11:44:14.802502: step 1265, loss 0.69324, acc 0.460938\n",
      "2018-05-04T11:44:16.614537: step 1266, loss 0.693086, acc 0.507812\n",
      "2018-05-04T11:44:18.442085: step 1267, loss 0.693107, acc 0.507812\n",
      "2018-05-04T11:44:20.323688: step 1268, loss 0.693007, acc 0.546875\n",
      "2018-05-04T11:44:22.444746: step 1269, loss 0.693018, acc 0.523438\n",
      "2018-05-04T11:44:24.690448: step 1270, loss 0.693322, acc 0.476562\n",
      "2018-05-04T11:44:26.815914: step 1271, loss 0.6933, acc 0.484375\n",
      "2018-05-04T11:44:28.987504: step 1272, loss 0.693543, acc 0.460938\n",
      "2018-05-04T11:44:31.281188: step 1273, loss 0.693235, acc 0.492188\n",
      "2018-05-04T11:44:33.531818: step 1274, loss 0.69362, acc 0.445312\n",
      "2018-05-04T11:44:35.825702: step 1275, loss 0.693771, acc 0.421875\n",
      "2018-05-04T11:44:38.063462: step 1276, loss 0.693456, acc 0.453125\n",
      "2018-05-04T11:44:40.315103: step 1277, loss 0.693201, acc 0.484375\n",
      "2018-05-04T11:44:42.695081: step 1278, loss 0.693101, acc 0.546875\n",
      "2018-05-04T11:44:45.011458: step 1279, loss 0.693099, acc 0.515625\n",
      "2018-05-04T11:44:47.299481: step 1280, loss 0.693244, acc 0.484375\n",
      "2018-05-04T11:44:49.607033: step 1281, loss 0.693443, acc 0.46875\n",
      "2018-05-04T11:44:52.042876: step 1282, loss 0.692488, acc 0.5625\n",
      "2018-05-04T11:44:54.820452: step 1283, loss 0.69211, acc 0.585938\n",
      "2018-05-04T11:44:57.576988: step 1284, loss 0.692539, acc 0.539062\n",
      "2018-05-04T11:45:00.236739: step 1285, loss 0.692689, acc 0.523438\n",
      "2018-05-04T11:45:03.015550: step 1286, loss 0.69477, acc 0.4375\n",
      "2018-05-04T11:45:05.695076: step 1287, loss 0.694966, acc 0.4375\n",
      "2018-05-04T11:45:08.047086: step 1288, loss 0.691018, acc 0.578125\n",
      "2018-05-04T11:45:10.420401: step 1289, loss 0.691424, acc 0.5625\n",
      "2018-05-04T11:45:12.658379: step 1290, loss 0.694028, acc 0.476562\n",
      "2018-05-04T11:45:14.717289: step 1291, loss 0.692523, acc 0.523438\n",
      "2018-05-04T11:45:16.748822: step 1292, loss 0.691118, acc 0.5625\n",
      "2018-05-04T11:45:18.788097: step 1293, loss 0.697989, acc 0.382812\n",
      "2018-05-04T11:45:20.791958: step 1294, loss 0.693974, acc 0.484375\n",
      "2018-05-04T11:45:22.717275: step 1295, loss 0.694561, acc 0.46875\n",
      "2018-05-04T11:45:24.760071: step 1296, loss 0.69365, acc 0.492188\n",
      "2018-05-04T11:45:26.657902: step 1297, loss 0.692172, acc 0.53125\n",
      "2018-05-04T11:45:28.480897: step 1298, loss 0.694433, acc 0.46875\n",
      "2018-05-04T11:45:30.298235: step 1299, loss 0.693862, acc 0.484375\n",
      "2018-05-04T11:45:32.161740: step 1300, loss 0.695612, acc 0.429688\n",
      "2018-05-04T11:45:33.982939: step 1301, loss 0.692074, acc 0.539062\n",
      "2018-05-04T11:45:35.780165: step 1302, loss 0.694144, acc 0.46875\n",
      "2018-05-04T11:45:37.567706: step 1303, loss 0.691782, acc 0.554688\n",
      "2018-05-04T11:45:39.320598: step 1304, loss 0.695364, acc 0.414062\n",
      "2018-05-04T11:45:41.061027: step 1305, loss 0.691132, acc 0.59375\n",
      "2018-05-04T11:45:42.818393: step 1306, loss 0.693031, acc 0.507812\n",
      "2018-05-04T11:45:44.534224: step 1307, loss 0.692564, acc 0.53125\n",
      "2018-05-04T11:45:46.248076: step 1308, loss 0.69289, acc 0.515625\n",
      "2018-05-04T11:45:47.969083: step 1309, loss 0.693806, acc 0.46875\n",
      "2018-05-04T11:45:49.683937: step 1310, loss 0.691556, acc 0.585938\n",
      "2018-05-04T11:45:51.412248: step 1311, loss 0.693157, acc 0.5\n",
      "2018-05-04T11:45:53.141078: step 1312, loss 0.693828, acc 0.46875\n",
      "2018-05-04T11:45:54.937075: step 1313, loss 0.693365, acc 0.492188\n",
      "2018-05-04T11:45:56.634121: step 1314, loss 0.694435, acc 0.4375\n",
      "2018-05-04T11:45:58.337499: step 1315, loss 0.693188, acc 0.5\n",
      "2018-05-04T11:46:00.040440: step 1316, loss 0.692381, acc 0.546875\n",
      "2018-05-04T11:46:01.784956: step 1317, loss 0.693443, acc 0.484375\n",
      "2018-05-04T11:46:03.535964: step 1318, loss 0.693052, acc 0.507812\n",
      "2018-05-04T11:46:05.265568: step 1319, loss 0.693633, acc 0.46875\n",
      "2018-05-04T11:46:06.893260: step 1320, loss 0.69358, acc 0.46875\n",
      "2018-05-04T11:46:08.610814: step 1321, loss 0.693275, acc 0.492188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:46:10.302273: step 1322, loss 0.693325, acc 0.484375\n",
      "2018-05-04T11:46:11.938653: step 1323, loss 0.693156, acc 0.5\n",
      "2018-05-04T11:46:13.495944: step 1324, loss 0.692976, acc 0.523438\n",
      "2018-05-04T11:46:15.179927: step 1325, loss 0.692901, acc 0.539062\n",
      "2018-05-04T11:46:16.866358: step 1326, loss 0.692672, acc 0.578125\n",
      "2018-05-04T11:46:18.569621: step 1327, loss 0.693492, acc 0.445312\n",
      "2018-05-04T11:46:20.291805: step 1328, loss 0.693393, acc 0.46875\n",
      "2018-05-04T11:46:22.056249: step 1329, loss 0.692753, acc 0.570312\n",
      "2018-05-04T11:46:23.773804: step 1330, loss 0.69269, acc 0.570312\n",
      "2018-05-04T11:46:25.547833: step 1331, loss 0.692944, acc 0.523438\n",
      "2018-05-04T11:46:27.315431: step 1332, loss 0.693586, acc 0.460938\n",
      "2018-05-04T11:46:29.028899: step 1333, loss 0.69273, acc 0.539062\n",
      "2018-05-04T11:46:30.797120: step 1334, loss 0.693365, acc 0.484375\n",
      "2018-05-04T11:46:32.523767: step 1335, loss 0.693702, acc 0.460938\n",
      "2018-05-04T11:46:34.410690: step 1336, loss 0.693834, acc 0.453125\n",
      "2018-05-04T11:46:36.286242: step 1337, loss 0.694198, acc 0.421875\n",
      "2018-05-04T11:46:37.863886: step 1338, loss 0.693944, acc 0.429688\n",
      "2018-05-04T11:46:39.448180: step 1339, loss 0.693043, acc 0.515625\n",
      "2018-05-04T11:46:41.233185: step 1340, loss 0.69366, acc 0.414062\n",
      "2018-05-04T11:46:42.938845: step 1341, loss 0.693173, acc 0.492188\n",
      "2018-05-04T11:46:44.645749: step 1342, loss 0.693194, acc 0.46875\n",
      "2018-05-04T11:46:46.349778: step 1343, loss 0.693191, acc 0.484375\n",
      "2018-05-04T11:46:48.041952: step 1344, loss 0.692886, acc 0.546875\n",
      "2018-05-04T11:46:49.742230: step 1345, loss 0.692952, acc 0.515625\n",
      "2018-05-04T11:46:51.504982: step 1346, loss 0.693035, acc 0.507812\n",
      "2018-05-04T11:46:53.221857: step 1347, loss 0.692692, acc 0.539062\n",
      "2018-05-04T11:46:54.946428: step 1348, loss 0.692658, acc 0.53125\n",
      "2018-05-04T11:46:56.753903: step 1349, loss 0.69359, acc 0.476562\n",
      "2018-05-04T11:46:58.460487: step 1350, loss 0.694099, acc 0.460938\n",
      "2018-05-04T11:47:00.043254: step 1351, loss 0.691812, acc 0.5625\n",
      "2018-05-04T11:47:01.653476: step 1352, loss 0.693654, acc 0.484375\n",
      "2018-05-04T11:47:03.347217: step 1353, loss 0.694093, acc 0.46875\n",
      "2018-05-04T11:47:05.069940: step 1354, loss 0.693887, acc 0.476562\n",
      "2018-05-04T11:47:06.762161: step 1355, loss 0.695071, acc 0.429688\n",
      "2018-05-04T11:47:08.457464: step 1356, loss 0.692499, acc 0.53125\n",
      "2018-05-04T11:47:10.170754: step 1357, loss 0.69279, acc 0.515625\n",
      "2018-05-04T11:47:11.985623: step 1358, loss 0.693168, acc 0.5\n",
      "2018-05-04T11:47:13.738784: step 1359, loss 0.691901, acc 0.554688\n",
      "2018-05-04T11:47:15.449412: step 1360, loss 0.693715, acc 0.476562\n",
      "2018-05-04T11:47:17.183896: step 1361, loss 0.693386, acc 0.5\n",
      "2018-05-04T11:47:18.893950: step 1362, loss 0.692795, acc 0.515625\n",
      "2018-05-04T11:47:20.599254: step 1363, loss 0.692359, acc 0.539062\n",
      "2018-05-04T11:47:22.306740: step 1364, loss 0.692935, acc 0.507812\n",
      "2018-05-04T11:47:24.013088: step 1365, loss 0.694791, acc 0.4375\n",
      "2018-05-04T11:47:25.712149: step 1366, loss 0.692887, acc 0.507812\n",
      "2018-05-04T11:47:27.476723: step 1367, loss 0.69422, acc 0.46875\n",
      "2018-05-04T11:47:29.271581: step 1368, loss 0.693649, acc 0.476562\n",
      "2018-05-04T11:47:30.963554: step 1369, loss 0.694717, acc 0.429688\n",
      "2018-05-04T11:47:32.671911: step 1370, loss 0.693924, acc 0.476562\n",
      "2018-05-04T11:47:34.388458: step 1371, loss 0.694313, acc 0.453125\n",
      "2018-05-04T11:47:36.105228: step 1372, loss 0.693422, acc 0.46875\n",
      "2018-05-04T11:47:37.786358: step 1373, loss 0.693765, acc 0.429688\n",
      "2018-05-04T11:47:39.533773: step 1374, loss 0.693513, acc 0.460938\n",
      "2018-05-04T11:47:41.235333: step 1375, loss 0.693165, acc 0.523438\n",
      "2018-05-04T11:47:42.949880: step 1376, loss 0.692539, acc 0.585938\n",
      "2018-05-04T11:47:44.661426: step 1377, loss 0.692497, acc 0.546875\n",
      "2018-05-04T11:47:46.359895: step 1378, loss 0.69191, acc 0.5625\n",
      "2018-05-04T11:47:48.062800: step 1379, loss 0.692031, acc 0.546875\n",
      "2018-05-04T11:47:49.825715: step 1380, loss 0.691384, acc 0.5625\n",
      "2018-05-04T11:47:51.554860: step 1381, loss 0.694211, acc 0.476562\n",
      "2018-05-04T11:47:53.263194: step 1382, loss 0.694043, acc 0.484375\n",
      "2018-05-04T11:47:54.961538: step 1383, loss 0.693071, acc 0.507812\n",
      "2018-05-04T11:47:56.660071: step 1384, loss 0.692655, acc 0.515625\n",
      "2018-05-04T11:47:58.530266: step 1385, loss 0.699182, acc 0.398438\n",
      "2018-05-04T11:48:00.322786: step 1386, loss 0.692223, acc 0.523438\n",
      "2018-05-04T11:48:02.047618: step 1387, loss 0.694016, acc 0.492188\n",
      "2018-05-04T11:48:03.782860: step 1388, loss 0.691316, acc 0.539062\n",
      "2018-05-04T11:48:05.479570: step 1389, loss 0.695042, acc 0.476562\n",
      "2018-05-04T11:48:07.172483: step 1390, loss 0.693112, acc 0.507812\n",
      "2018-05-04T11:48:08.909235: step 1391, loss 0.68892, acc 0.578125\n",
      "2018-05-04T11:48:10.618597: step 1392, loss 0.699777, acc 0.398438\n",
      "2018-05-04T11:48:12.347850: step 1393, loss 0.689856, acc 0.5625\n",
      "2018-05-04T11:48:14.051661: step 1394, loss 0.692192, acc 0.523438\n",
      "2018-05-04T11:48:15.751512: step 1395, loss 0.689809, acc 0.5625\n",
      "2018-05-04T11:48:17.484017: step 1396, loss 0.693099, acc 0.507812\n",
      "2018-05-04T11:48:19.270302: step 1397, loss 0.689743, acc 0.5625\n",
      "2018-05-04T11:48:21.044542: step 1398, loss 0.696606, acc 0.453125\n",
      "2018-05-04T11:48:22.781034: step 1399, loss 0.689684, acc 0.5625\n",
      "2018-05-04T11:48:24.482627: step 1400, loss 0.690618, acc 0.546875\n",
      "2018-05-04T11:48:26.182444: step 1401, loss 0.693709, acc 0.5\n",
      "2018-05-04T11:48:27.884268: step 1402, loss 0.693671, acc 0.5\n",
      "2018-05-04T11:48:29.677140: step 1403, loss 0.690016, acc 0.554688\n",
      "2018-05-04T11:48:31.512522: step 1404, loss 0.693195, acc 0.507812\n",
      "2018-05-04T11:48:33.246816: step 1405, loss 0.695933, acc 0.46875\n",
      "2018-05-04T11:48:34.980481: step 1406, loss 0.696004, acc 0.46875\n",
      "2018-05-04T11:48:36.683516: step 1407, loss 0.698012, acc 0.4375\n",
      "2018-05-04T11:48:38.386923: step 1408, loss 0.695235, acc 0.476562\n",
      "2018-05-04T11:48:40.086398: step 1409, loss 0.693169, acc 0.507812\n",
      "2018-05-04T11:48:41.855847: step 1410, loss 0.693148, acc 0.507812\n",
      "2018-05-04T11:48:43.587478: step 1411, loss 0.692217, acc 0.523438\n",
      "2018-05-04T11:48:45.302140: step 1412, loss 0.694058, acc 0.492188\n",
      "2018-05-04T11:48:46.993194: step 1413, loss 0.691378, acc 0.539062\n",
      "2018-05-04T11:48:48.701039: step 1414, loss 0.695181, acc 0.46875\n",
      "2018-05-04T11:48:50.418947: step 1415, loss 0.691472, acc 0.539062\n",
      "2018-05-04T11:48:52.200596: step 1416, loss 0.69389, acc 0.492188\n",
      "2018-05-04T11:48:53.914362: step 1417, loss 0.696456, acc 0.4375\n",
      "2018-05-04T11:48:55.605737: step 1418, loss 0.694464, acc 0.476562\n",
      "2018-05-04T11:48:57.305963: step 1419, loss 0.695702, acc 0.445312\n",
      "2018-05-04T11:48:59.003570: step 1420, loss 0.696046, acc 0.429688\n",
      "2018-05-04T11:49:00.819249: step 1421, loss 0.694622, acc 0.460938\n",
      "2018-05-04T11:49:02.585012: step 1422, loss 0.693707, acc 0.484375\n",
      "2018-05-04T11:49:04.314200: step 1423, loss 0.691152, acc 0.585938\n",
      "2018-05-04T11:49:06.024381: step 1424, loss 0.691995, acc 0.554688\n",
      "2018-05-04T11:49:07.728217: step 1425, loss 0.692927, acc 0.515625\n",
      "2018-05-04T11:49:09.439813: step 1426, loss 0.693075, acc 0.507812\n",
      "2018-05-04T11:49:11.152356: step 1427, loss 0.693314, acc 0.492188\n",
      "2018-05-04T11:49:12.934887: step 1428, loss 0.692733, acc 0.53125\n",
      "2018-05-04T11:49:14.662243: step 1429, loss 0.692822, acc 0.523438\n",
      "2018-05-04T11:49:16.359064: step 1430, loss 0.693916, acc 0.445312\n",
      "2018-05-04T11:49:18.051677: step 1431, loss 0.692273, acc 0.570312\n",
      "2018-05-04T11:49:19.750623: step 1432, loss 0.693532, acc 0.46875\n",
      "2018-05-04T11:49:21.447357: step 1433, loss 0.693331, acc 0.484375\n",
      "2018-05-04T11:49:23.254244: step 1434, loss 0.692984, acc 0.515625\n",
      "2018-05-04T11:49:24.959231: step 1435, loss 0.693554, acc 0.460938\n",
      "2018-05-04T11:49:26.659096: step 1436, loss 0.69304, acc 0.515625\n",
      "2018-05-04T11:49:28.378185: step 1437, loss 0.69273, acc 0.554688\n",
      "2018-05-04T11:49:30.077563: step 1438, loss 0.69276, acc 0.546875\n",
      "2018-05-04T11:49:31.940614: step 1439, loss 0.693392, acc 0.476562\n",
      "2018-05-04T11:49:33.731015: step 1440, loss 0.694882, acc 0.3125\n",
      "2018-05-04T11:49:35.559472: step 1441, loss 0.693628, acc 0.429688\n",
      "2018-05-04T11:49:37.212347: step 1442, loss 0.693311, acc 0.453125\n",
      "2018-05-04T11:49:38.769359: step 1443, loss 0.693132, acc 0.515625\n",
      "2018-05-04T11:49:40.388627: step 1444, loss 0.693505, acc 0.429688\n",
      "2018-05-04T11:49:42.145989: step 1445, loss 0.693133, acc 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:49:43.861990: step 1446, loss 0.69327, acc 0.484375\n",
      "2018-05-04T11:49:45.569370: step 1447, loss 0.69341, acc 0.476562\n",
      "2018-05-04T11:49:47.268250: step 1448, loss 0.692972, acc 0.515625\n",
      "2018-05-04T11:49:48.987905: step 1449, loss 0.693049, acc 0.507812\n",
      "2018-05-04T11:49:50.734962: step 1450, loss 0.692517, acc 0.546875\n",
      "2018-05-04T11:49:52.463955: step 1451, loss 0.693309, acc 0.492188\n",
      "2018-05-04T11:49:54.181996: step 1452, loss 0.692783, acc 0.523438\n",
      "2018-05-04T11:49:55.887291: step 1453, loss 0.69422, acc 0.445312\n",
      "2018-05-04T11:49:57.620321: step 1454, loss 0.693367, acc 0.492188\n",
      "2018-05-04T11:49:59.353535: step 1455, loss 0.69382, acc 0.46875\n",
      "2018-05-04T11:50:01.080541: step 1456, loss 0.693345, acc 0.492188\n",
      "2018-05-04T11:50:02.909008: step 1457, loss 0.694042, acc 0.453125\n",
      "2018-05-04T11:50:04.632935: step 1458, loss 0.693961, acc 0.453125\n",
      "2018-05-04T11:50:06.368622: step 1459, loss 0.693413, acc 0.484375\n",
      "2018-05-04T11:50:08.082373: step 1460, loss 0.692895, acc 0.523438\n",
      "2018-05-04T11:50:09.804933: step 1461, loss 0.693136, acc 0.5\n",
      "2018-05-04T11:50:11.559998: step 1462, loss 0.692805, acc 0.539062\n",
      "2018-05-04T11:50:13.293457: step 1463, loss 0.693027, acc 0.515625\n",
      "2018-05-04T11:50:15.016323: step 1464, loss 0.692324, acc 0.601562\n",
      "2018-05-04T11:50:16.711420: step 1465, loss 0.693217, acc 0.492188\n",
      "2018-05-04T11:50:18.442110: step 1466, loss 0.693408, acc 0.476562\n",
      "2018-05-04T11:50:20.162487: step 1467, loss 0.692744, acc 0.539062\n",
      "2018-05-04T11:50:21.875256: step 1468, loss 0.693598, acc 0.460938\n",
      "2018-05-04T11:50:23.572645: step 1469, loss 0.693606, acc 0.460938\n",
      "2018-05-04T11:50:25.287613: step 1470, loss 0.693342, acc 0.484375\n",
      "2018-05-04T11:50:26.988449: step 1471, loss 0.693485, acc 0.46875\n",
      "2018-05-04T11:50:28.694334: step 1472, loss 0.693656, acc 0.445312\n",
      "2018-05-04T11:50:30.448040: step 1473, loss 0.693048, acc 0.515625\n",
      "2018-05-04T11:50:32.245398: step 1474, loss 0.693671, acc 0.40625\n",
      "2018-05-04T11:50:34.118547: step 1475, loss 0.692985, acc 0.5625\n",
      "2018-05-04T11:50:35.836706: step 1476, loss 0.693145, acc 0.515625\n",
      "2018-05-04T11:50:37.549086: step 1477, loss 0.693114, acc 0.5625\n",
      "2018-05-04T11:50:39.273899: step 1478, loss 0.693094, acc 0.523438\n",
      "2018-05-04T11:50:41.056931: step 1479, loss 0.692903, acc 0.5625\n",
      "2018-05-04T11:50:42.794831: step 1480, loss 0.69288, acc 0.539062\n",
      "2018-05-04T11:50:44.544383: step 1481, loss 0.693622, acc 0.453125\n",
      "2018-05-04T11:50:46.269939: step 1482, loss 0.692953, acc 0.515625\n",
      "2018-05-04T11:50:48.012037: step 1483, loss 0.691913, acc 0.59375\n",
      "2018-05-04T11:50:49.726169: step 1484, loss 0.693833, acc 0.460938\n",
      "2018-05-04T11:50:51.452365: step 1485, loss 0.694232, acc 0.445312\n",
      "2018-05-04T11:50:53.181312: step 1486, loss 0.691951, acc 0.5625\n",
      "2018-05-04T11:50:54.888181: step 1487, loss 0.693879, acc 0.46875\n",
      "2018-05-04T11:50:56.596930: step 1488, loss 0.694275, acc 0.453125\n",
      "2018-05-04T11:50:58.315245: step 1489, loss 0.691271, acc 0.585938\n",
      "2018-05-04T11:51:00.124313: step 1490, loss 0.693211, acc 0.5\n",
      "2018-05-04T11:51:01.848622: step 1491, loss 0.692262, acc 0.539062\n",
      "2018-05-04T11:51:03.569620: step 1492, loss 0.696616, acc 0.375\n",
      "2018-05-04T11:51:05.429018: step 1493, loss 0.693213, acc 0.5\n",
      "2018-05-04T11:51:07.186677: step 1494, loss 0.693632, acc 0.484375\n",
      "2018-05-04T11:51:08.936951: step 1495, loss 0.695461, acc 0.40625\n",
      "2018-05-04T11:51:10.706258: step 1496, loss 0.693026, acc 0.507812\n",
      "2018-05-04T11:51:12.470131: step 1497, loss 0.693396, acc 0.492188\n",
      "2018-05-04T11:51:14.246885: step 1498, loss 0.692452, acc 0.539062\n",
      "2018-05-04T11:51:16.035770: step 1499, loss 0.692662, acc 0.53125\n",
      "2018-05-04T11:51:17.847601: step 1500, loss 0.693161, acc 0.5\n",
      "2018-05-04T11:51:19.649068: step 1501, loss 0.693736, acc 0.460938\n",
      "2018-05-04T11:51:21.410662: step 1502, loss 0.693925, acc 0.445312\n",
      "2018-05-04T11:51:23.191388: step 1503, loss 0.693478, acc 0.476562\n",
      "2018-05-04T11:51:24.980114: step 1504, loss 0.693196, acc 0.5\n",
      "2018-05-04T11:51:26.762426: step 1505, loss 0.692606, acc 0.570312\n",
      "2018-05-04T11:51:28.568899: step 1506, loss 0.693008, acc 0.515625\n",
      "2018-05-04T11:51:30.391644: step 1507, loss 0.693287, acc 0.484375\n",
      "2018-05-04T11:51:32.182164: step 1508, loss 0.693125, acc 0.507812\n",
      "2018-05-04T11:51:33.991730: step 1509, loss 0.69283, acc 0.5625\n",
      "2018-05-04T11:51:35.889003: step 1510, loss 0.693437, acc 0.4375\n",
      "2018-05-04T11:51:37.679709: step 1511, loss 0.69287, acc 0.5625\n",
      "2018-05-04T11:51:39.467501: step 1512, loss 0.692967, acc 0.539062\n",
      "2018-05-04T11:51:41.284504: step 1513, loss 0.693211, acc 0.492188\n",
      "2018-05-04T11:51:43.090207: step 1514, loss 0.692954, acc 0.523438\n",
      "2018-05-04T11:51:44.887312: step 1515, loss 0.693169, acc 0.507812\n",
      "2018-05-04T11:51:46.688424: step 1516, loss 0.692879, acc 0.546875\n",
      "2018-05-04T11:51:48.458311: step 1517, loss 0.693578, acc 0.460938\n",
      "2018-05-04T11:51:50.281290: step 1518, loss 0.693497, acc 0.46875\n",
      "2018-05-04T11:51:52.077426: step 1519, loss 0.692278, acc 0.570312\n",
      "2018-05-04T11:51:53.843299: step 1520, loss 0.693557, acc 0.476562\n",
      "2018-05-04T11:51:55.636077: step 1521, loss 0.692643, acc 0.539062\n",
      "2018-05-04T11:51:57.446412: step 1522, loss 0.692093, acc 0.578125\n",
      "2018-05-04T11:51:59.199243: step 1523, loss 0.692018, acc 0.570312\n",
      "2018-05-04T11:52:00.949888: step 1524, loss 0.694722, acc 0.421875\n",
      "2018-05-04T11:52:02.776430: step 1525, loss 0.692972, acc 0.515625\n",
      "2018-05-04T11:52:04.613935: step 1526, loss 0.693522, acc 0.484375\n",
      "2018-05-04T11:52:06.534680: step 1527, loss 0.69392, acc 0.46875\n",
      "2018-05-04T11:52:08.400795: step 1528, loss 0.691549, acc 0.570312\n",
      "2018-05-04T11:52:10.267313: step 1529, loss 0.693393, acc 0.492188\n",
      "2018-05-04T11:52:12.135648: step 1530, loss 0.693007, acc 0.507812\n",
      "2018-05-04T11:52:13.996754: step 1531, loss 0.693508, acc 0.492188\n",
      "2018-05-04T11:52:15.837423: step 1532, loss 0.692887, acc 0.515625\n",
      "2018-05-04T11:52:17.657264: step 1533, loss 0.693218, acc 0.5\n",
      "2018-05-04T11:52:19.491167: step 1534, loss 0.692294, acc 0.53125\n",
      "2018-05-04T11:52:21.306366: step 1535, loss 0.693434, acc 0.492188\n",
      "2018-05-04T11:52:23.147186: step 1536, loss 0.691364, acc 0.5625\n",
      "2018-05-04T11:52:24.906414: step 1537, loss 0.692747, acc 0.515625\n",
      "2018-05-04T11:52:26.645128: step 1538, loss 0.694009, acc 0.476562\n",
      "2018-05-04T11:52:28.401484: step 1539, loss 0.692789, acc 0.515625\n",
      "2018-05-04T11:52:30.225825: step 1540, loss 0.692268, acc 0.53125\n",
      "2018-05-04T11:52:32.126356: step 1541, loss 0.693358, acc 0.5\n",
      "2018-05-04T11:52:34.063493: step 1542, loss 0.694143, acc 0.476562\n",
      "2018-05-04T11:52:35.969919: step 1543, loss 0.693043, acc 0.507812\n",
      "2018-05-04T11:52:37.979054: step 1544, loss 0.691901, acc 0.539062\n",
      "2018-05-04T11:52:39.895319: step 1545, loss 0.693335, acc 0.5\n",
      "2018-05-04T11:52:41.835492: step 1546, loss 0.69271, acc 0.515625\n",
      "2018-05-04T11:52:43.663795: step 1547, loss 0.688933, acc 0.617188\n",
      "2018-05-04T11:52:45.517838: step 1548, loss 0.693027, acc 0.507812\n",
      "2018-05-04T11:52:47.324567: step 1549, loss 0.694637, acc 0.46875\n",
      "2018-05-04T11:52:49.143108: step 1550, loss 0.694353, acc 0.476562\n",
      "2018-05-04T11:52:50.941531: step 1551, loss 0.690693, acc 0.5625\n",
      "2018-05-04T11:52:52.736115: step 1552, loss 0.691305, acc 0.546875\n",
      "2018-05-04T11:52:54.570248: step 1553, loss 0.697333, acc 0.414062\n",
      "2018-05-04T11:52:56.418743: step 1554, loss 0.692704, acc 0.515625\n",
      "2018-05-04T11:52:58.250904: step 1555, loss 0.695215, acc 0.460938\n",
      "2018-05-04T11:53:00.085981: step 1556, loss 0.694113, acc 0.484375\n",
      "2018-05-04T11:53:01.938797: step 1557, loss 0.69236, acc 0.523438\n",
      "2018-05-04T11:53:03.724974: step 1558, loss 0.692699, acc 0.515625\n",
      "2018-05-04T11:53:05.588351: step 1559, loss 0.689799, acc 0.585938\n",
      "2018-05-04T11:53:07.346498: step 1560, loss 0.697048, acc 0.414062\n",
      "2018-05-04T11:53:09.296730: step 1561, loss 0.694667, acc 0.46875\n",
      "2018-05-04T11:53:11.110971: step 1562, loss 0.694914, acc 0.460938\n",
      "2018-05-04T11:53:12.907030: step 1563, loss 0.693029, acc 0.507812\n",
      "2018-05-04T11:53:14.695009: step 1564, loss 0.693595, acc 0.492188\n",
      "2018-05-04T11:53:16.478306: step 1565, loss 0.694322, acc 0.46875\n",
      "2018-05-04T11:53:18.253940: step 1566, loss 0.693993, acc 0.476562\n",
      "2018-05-04T11:53:20.049226: step 1567, loss 0.691899, acc 0.546875\n",
      "2018-05-04T11:53:21.889752: step 1568, loss 0.692608, acc 0.523438\n",
      "2018-05-04T11:53:23.674394: step 1569, loss 0.693619, acc 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:53:25.469835: step 1570, loss 0.69324, acc 0.5\n",
      "2018-05-04T11:53:27.263248: step 1571, loss 0.6934, acc 0.492188\n",
      "2018-05-04T11:53:29.051646: step 1572, loss 0.693852, acc 0.46875\n",
      "2018-05-04T11:53:30.828250: step 1573, loss 0.693777, acc 0.46875\n",
      "2018-05-04T11:53:32.600866: step 1574, loss 0.692663, acc 0.53125\n",
      "2018-05-04T11:53:34.398615: step 1575, loss 0.692594, acc 0.539062\n",
      "2018-05-04T11:53:36.177672: step 1576, loss 0.693602, acc 0.46875\n",
      "2018-05-04T11:53:37.954251: step 1577, loss 0.692984, acc 0.515625\n",
      "2018-05-04T11:53:39.845839: step 1578, loss 0.692895, acc 0.523438\n",
      "2018-05-04T11:53:41.736585: step 1579, loss 0.69274, acc 0.539062\n",
      "2018-05-04T11:53:43.507380: step 1580, loss 0.69313, acc 0.5\n",
      "2018-05-04T11:53:45.296633: step 1581, loss 0.69308, acc 0.507812\n",
      "2018-05-04T11:53:47.091855: step 1582, loss 0.693332, acc 0.484375\n",
      "2018-05-04T11:53:48.879603: step 1583, loss 0.693403, acc 0.476562\n",
      "2018-05-04T11:53:50.648408: step 1584, loss 0.69323, acc 0.492188\n",
      "2018-05-04T11:53:52.434434: step 1585, loss 0.693292, acc 0.484375\n",
      "2018-05-04T11:53:54.235404: step 1586, loss 0.694062, acc 0.390625\n",
      "2018-05-04T11:53:56.021578: step 1587, loss 0.692717, acc 0.578125\n",
      "2018-05-04T11:53:57.793661: step 1588, loss 0.693128, acc 0.5\n",
      "2018-05-04T11:53:59.590485: step 1589, loss 0.692922, acc 0.570312\n",
      "2018-05-04T11:54:01.406558: step 1590, loss 0.693037, acc 0.53125\n",
      "2018-05-04T11:54:03.206843: step 1591, loss 0.69303, acc 0.53125\n",
      "2018-05-04T11:54:04.999116: step 1592, loss 0.693375, acc 0.453125\n",
      "2018-05-04T11:54:06.799029: step 1593, loss 0.693383, acc 0.453125\n",
      "2018-05-04T11:54:08.609214: step 1594, loss 0.693182, acc 0.492188\n",
      "2018-05-04T11:54:10.469206: step 1595, loss 0.693343, acc 0.453125\n",
      "2018-05-04T11:54:12.323076: step 1596, loss 0.693241, acc 0.46875\n",
      "2018-05-04T11:54:14.131965: step 1597, loss 0.693153, acc 0.492188\n",
      "2018-05-04T11:54:15.926757: step 1598, loss 0.693205, acc 0.421875\n",
      "2018-05-04T11:54:17.708366: step 1599, loss 0.693184, acc 0.484375\n",
      "2018-05-04T11:54:19.514125: step 1600, loss 0.693185, acc 0.476562\n",
      "2018-05-04T11:54:21.301250: step 1601, loss 0.693138, acc 0.515625\n",
      "2018-05-04T11:54:23.104939: step 1602, loss 0.693118, acc 0.539062\n",
      "2018-05-04T11:54:25.009221: step 1603, loss 0.693181, acc 0.476562\n",
      "2018-05-04T11:54:26.770518: step 1604, loss 0.693249, acc 0.421875\n",
      "2018-05-04T11:54:28.542541: step 1605, loss 0.693143, acc 0.515625\n",
      "2018-05-04T11:54:30.319738: step 1606, loss 0.693149, acc 0.46875\n",
      "2018-05-04T11:54:32.037176: step 1607, loss 0.693164, acc 0.429688\n",
      "2018-05-04T11:54:33.835550: step 1608, loss 0.693155, acc 0.492188\n",
      "2018-05-04T11:54:35.617888: step 1609, loss 0.693188, acc 0.476562\n",
      "2018-05-04T11:54:37.469826: step 1610, loss 0.693057, acc 0.546875\n",
      "2018-05-04T11:54:39.260425: step 1611, loss 0.693092, acc 0.515625\n",
      "2018-05-04T11:54:41.107032: step 1612, loss 0.693308, acc 0.460938\n",
      "2018-05-04T11:54:42.972436: step 1613, loss 0.693188, acc 0.492188\n",
      "2018-05-04T11:54:44.814620: step 1614, loss 0.693363, acc 0.453125\n",
      "2018-05-04T11:54:46.593593: step 1615, loss 0.693083, acc 0.515625\n",
      "2018-05-04T11:54:48.390326: step 1616, loss 0.693182, acc 0.492188\n",
      "2018-05-04T11:54:50.181152: step 1617, loss 0.693217, acc 0.476562\n",
      "2018-05-04T11:54:51.961292: step 1618, loss 0.693184, acc 0.484375\n",
      "2018-05-04T11:54:53.745528: step 1619, loss 0.693106, acc 0.523438\n",
      "2018-05-04T11:54:55.525813: step 1620, loss 0.693245, acc 0.414062\n",
      "2018-05-04T11:54:57.394531: step 1621, loss 0.693137, acc 0.515625\n",
      "2018-05-04T11:54:59.192541: step 1622, loss 0.692964, acc 0.5625\n",
      "2018-05-04T11:55:00.980240: step 1623, loss 0.693151, acc 0.5\n",
      "2018-05-04T11:55:02.756485: step 1624, loss 0.693353, acc 0.476562\n",
      "2018-05-04T11:55:04.575092: step 1625, loss 0.693159, acc 0.5\n",
      "2018-05-04T11:55:06.361830: step 1626, loss 0.693256, acc 0.492188\n",
      "2018-05-04T11:55:08.170910: step 1627, loss 0.693167, acc 0.5\n",
      "2018-05-04T11:55:09.954922: step 1628, loss 0.693833, acc 0.453125\n",
      "2018-05-04T11:55:11.761084: step 1629, loss 0.692168, acc 0.570312\n",
      "2018-05-04T11:55:13.654171: step 1630, loss 0.6928, acc 0.523438\n",
      "2018-05-04T11:55:15.474464: step 1631, loss 0.693829, acc 0.460938\n",
      "2018-05-04T11:55:17.363119: step 1632, loss 0.693869, acc 0.460938\n",
      "2018-05-04T11:55:19.197801: step 1633, loss 0.691704, acc 0.585938\n",
      "2018-05-04T11:55:21.051919: step 1634, loss 0.693626, acc 0.476562\n",
      "2018-05-04T11:55:22.859362: step 1635, loss 0.693938, acc 0.460938\n",
      "2018-05-04T11:55:24.651463: step 1636, loss 0.693192, acc 0.5\n",
      "2018-05-04T11:55:26.477038: step 1637, loss 0.693914, acc 0.460938\n",
      "2018-05-04T11:55:28.281679: step 1638, loss 0.69305, acc 0.507812\n",
      "2018-05-04T11:55:30.060229: step 1639, loss 0.694783, acc 0.40625\n",
      "2018-05-04T11:55:31.836356: step 1640, loss 0.692227, acc 0.5625\n",
      "2018-05-04T11:55:33.701049: step 1641, loss 0.693387, acc 0.484375\n",
      "2018-05-04T11:55:35.580313: step 1642, loss 0.693173, acc 0.5\n",
      "2018-05-04T11:55:37.434146: step 1643, loss 0.693163, acc 0.5\n",
      "2018-05-04T11:55:39.351178: step 1644, loss 0.694116, acc 0.40625\n",
      "2018-05-04T11:55:41.203147: step 1645, loss 0.693355, acc 0.476562\n",
      "2018-05-04T11:55:43.052772: step 1646, loss 0.693186, acc 0.492188\n",
      "2018-05-04T11:55:44.939386: step 1647, loss 0.693169, acc 0.492188\n",
      "2018-05-04T11:55:46.751548: step 1648, loss 0.693117, acc 0.507812\n",
      "2018-05-04T11:55:48.559742: step 1649, loss 0.693075, acc 0.53125\n",
      "2018-05-04T11:55:50.374448: step 1650, loss 0.693089, acc 0.515625\n",
      "2018-05-04T11:55:52.152887: step 1651, loss 0.693155, acc 0.5\n",
      "2018-05-04T11:55:53.928815: step 1652, loss 0.69281, acc 0.539062\n",
      "2018-05-04T11:55:55.725450: step 1653, loss 0.692796, acc 0.53125\n",
      "2018-05-04T11:55:57.494823: step 1654, loss 0.692952, acc 0.515625\n",
      "2018-05-04T11:55:59.307808: step 1655, loss 0.693475, acc 0.484375\n",
      "2018-05-04T11:56:01.111208: step 1656, loss 0.693906, acc 0.460938\n",
      "2018-05-04T11:56:02.938005: step 1657, loss 0.693681, acc 0.476562\n",
      "2018-05-04T11:56:04.770519: step 1658, loss 0.69339, acc 0.492188\n",
      "2018-05-04T11:56:06.634537: step 1659, loss 0.692248, acc 0.546875\n",
      "2018-05-04T11:56:08.434319: step 1660, loss 0.694, acc 0.460938\n",
      "2018-05-04T11:56:10.244429: step 1661, loss 0.692145, acc 0.546875\n",
      "2018-05-04T11:56:12.095320: step 1662, loss 0.692858, acc 0.515625\n",
      "2018-05-04T11:56:13.905100: step 1663, loss 0.69318, acc 0.5\n",
      "2018-05-04T11:56:15.781236: step 1664, loss 0.692278, acc 0.539062\n",
      "2018-05-04T11:56:17.589560: step 1665, loss 0.692238, acc 0.539062\n",
      "2018-05-04T11:56:19.396606: step 1666, loss 0.693051, acc 0.507812\n",
      "2018-05-04T11:56:21.178005: step 1667, loss 0.695405, acc 0.421875\n",
      "2018-05-04T11:56:22.959661: step 1668, loss 0.693481, acc 0.492188\n",
      "2018-05-04T11:56:24.734930: step 1669, loss 0.692405, acc 0.53125\n",
      "2018-05-04T11:56:26.504727: step 1670, loss 0.694974, acc 0.4375\n",
      "2018-05-04T11:56:28.280737: step 1671, loss 0.693463, acc 0.492188\n",
      "2018-05-04T11:56:30.052111: step 1672, loss 0.693435, acc 0.492188\n",
      "2018-05-04T11:56:31.970987: step 1673, loss 0.693622, acc 0.484375\n",
      "2018-05-04T11:56:33.814587: step 1674, loss 0.693339, acc 0.492188\n",
      "2018-05-04T11:56:35.626585: step 1675, loss 0.693184, acc 0.5\n",
      "2018-05-04T11:56:37.411719: step 1676, loss 0.694101, acc 0.453125\n",
      "2018-05-04T11:56:39.185509: step 1677, loss 0.692741, acc 0.523438\n",
      "2018-05-04T11:56:41.024759: step 1678, loss 0.693531, acc 0.476562\n",
      "2018-05-04T11:56:42.825551: step 1679, loss 0.69264, acc 0.539062\n",
      "2018-05-04T11:56:44.635588: step 1680, loss 0.69367, acc 0.460938\n",
      "2018-05-04T11:56:46.521583: step 1681, loss 0.693236, acc 0.492188\n",
      "2018-05-04T11:56:48.340608: step 1682, loss 0.693301, acc 0.484375\n",
      "2018-05-04T11:56:50.153450: step 1683, loss 0.693553, acc 0.445312\n",
      "2018-05-04T11:56:51.965164: step 1684, loss 0.693063, acc 0.523438\n",
      "2018-05-04T11:56:53.739541: step 1685, loss 0.693206, acc 0.484375\n",
      "2018-05-04T11:56:55.528812: step 1686, loss 0.693127, acc 0.5\n",
      "2018-05-04T11:56:57.312176: step 1687, loss 0.693178, acc 0.4375\n",
      "2018-05-04T11:56:59.036998: step 1688, loss 0.693252, acc 0.4375\n",
      "2018-05-04T11:57:00.904311: step 1689, loss 0.693102, acc 0.53125\n",
      "2018-05-04T11:57:02.651983: step 1690, loss 0.693087, acc 0.53125\n",
      "2018-05-04T11:57:04.470721: step 1691, loss 0.693319, acc 0.429688\n",
      "2018-05-04T11:57:06.294358: step 1692, loss 0.693083, acc 0.53125\n",
      "2018-05-04T11:57:08.097730: step 1693, loss 0.69309, acc 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T11:57:09.924661: step 1694, loss 0.693135, acc 0.507812\n",
      "2018-05-04T11:57:11.695666: step 1695, loss 0.693228, acc 0.484375\n",
      "2018-05-04T11:57:13.459038: step 1696, loss 0.69313, acc 0.5\n",
      "2018-05-04T11:57:15.247793: step 1697, loss 0.693199, acc 0.507812\n",
      "2018-05-04T11:57:17.156805: step 1698, loss 0.693071, acc 0.515625\n",
      "2018-05-04T11:57:18.962360: step 1699, loss 0.693239, acc 0.476562\n",
      "2018-05-04T11:57:20.810598: step 1700, loss 0.693259, acc 0.476562\n",
      "2018-05-04T11:57:22.618607: step 1701, loss 0.693437, acc 0.429688\n",
      "2018-05-04T11:57:24.416978: step 1702, loss 0.693105, acc 0.507812\n",
      "2018-05-04T11:57:26.185199: step 1703, loss 0.693062, acc 0.546875\n",
      "2018-05-04T11:57:27.963923: step 1704, loss 0.693093, acc 0.515625\n",
      "2018-05-04T11:57:29.741498: step 1705, loss 0.693222, acc 0.492188\n",
      "2018-05-04T11:57:31.542996: step 1706, loss 0.693096, acc 0.523438\n",
      "2018-05-04T11:57:33.326050: step 1707, loss 0.693171, acc 0.46875\n",
      "2018-05-04T11:57:35.115322: step 1708, loss 0.69311, acc 0.515625\n",
      "2018-05-04T11:57:36.881449: step 1709, loss 0.693085, acc 0.554688\n",
      "2018-05-04T11:57:38.676088: step 1710, loss 0.692953, acc 0.601562\n",
      "2018-05-04T11:57:40.540026: step 1711, loss 0.692929, acc 0.546875\n",
      "2018-05-04T11:57:42.417052: step 1712, loss 0.693163, acc 0.492188\n",
      "2018-05-04T11:57:44.245340: step 1713, loss 0.693852, acc 0.429688\n",
      "2018-05-04T11:57:46.065484: step 1714, loss 0.693523, acc 0.46875\n",
      "2018-05-04T11:57:47.950175: step 1715, loss 0.693584, acc 0.460938\n",
      "2018-05-04T11:57:49.757798: step 1716, loss 0.693345, acc 0.484375\n",
      "2018-05-04T11:57:51.574034: step 1717, loss 0.693389, acc 0.476562\n",
      "2018-05-04T11:57:53.362562: step 1718, loss 0.69306, acc 0.507812\n",
      "2018-05-04T11:57:55.157615: step 1719, loss 0.693532, acc 0.453125\n",
      "2018-05-04T11:57:56.948714: step 1720, loss 0.693592, acc 0.4375\n",
      "2018-05-04T11:57:58.746456: step 1721, loss 0.693121, acc 0.507812\n",
      "2018-05-04T11:58:00.537806: step 1722, loss 0.693091, acc 0.523438\n",
      "2018-05-04T11:58:02.369452: step 1723, loss 0.692987, acc 0.601562\n",
      "2018-05-04T11:58:04.199459: step 1724, loss 0.693196, acc 0.5\n",
      "2018-05-04T11:58:06.021489: step 1725, loss 0.693215, acc 0.476562\n",
      "2018-05-04T11:58:07.837547: step 1726, loss 0.692799, acc 0.640625\n",
      "2018-05-04T11:58:09.645290: step 1727, loss 0.693025, acc 0.523438\n",
      "2018-05-04T11:58:11.430608: step 1728, loss 0.693187, acc 0.476562\n",
      "2018-05-04T11:58:13.216131: step 1729, loss 0.693376, acc 0.460938\n",
      "2018-05-04T11:58:15.013131: step 1730, loss 0.693379, acc 0.46875\n",
      "2018-05-04T11:58:16.793839: step 1731, loss 0.692415, acc 0.585938\n",
      "2018-05-04T11:58:18.690478: step 1732, loss 0.693666, acc 0.445312\n",
      "2018-05-04T11:58:20.515994: step 1733, loss 0.693274, acc 0.5\n",
      "2018-05-04T11:58:22.375167: step 1734, loss 0.693424, acc 0.476562\n",
      "2018-05-04T11:58:24.186975: step 1735, loss 0.693344, acc 0.476562\n",
      "2018-05-04T11:58:25.997612: step 1736, loss 0.693719, acc 0.453125\n",
      "2018-05-04T11:58:27.805639: step 1737, loss 0.692929, acc 0.515625\n",
      "2018-05-04T11:58:29.630448: step 1738, loss 0.693269, acc 0.492188\n",
      "2018-05-04T11:58:31.481238: step 1739, loss 0.692719, acc 0.523438\n",
      "2018-05-04T11:58:33.352938: step 1740, loss 0.692799, acc 0.539062\n",
      "2018-05-04T11:58:35.345968: step 1741, loss 0.693311, acc 0.476562\n",
      "2018-05-04T11:58:37.319188: step 1742, loss 0.693134, acc 0.476562\n",
      "2018-05-04T11:58:39.134762: step 1743, loss 0.693273, acc 0.476562\n",
      "2018-05-04T11:58:40.954571: step 1744, loss 0.693272, acc 0.492188\n",
      "2018-05-04T11:58:42.775387: step 1745, loss 0.692958, acc 0.523438\n",
      "2018-05-04T11:58:44.582235: step 1746, loss 0.693295, acc 0.453125\n",
      "2018-05-04T11:58:46.395060: step 1747, loss 0.693289, acc 0.523438\n",
      "2018-05-04T11:58:48.208767: step 1748, loss 0.693018, acc 0.492188\n",
      "2018-05-04T11:58:50.118187: step 1749, loss 0.693224, acc 0.46875\n",
      "2018-05-04T11:58:51.936647: step 1750, loss 0.693311, acc 0.4375\n",
      "2018-05-04T11:58:53.759607: step 1751, loss 0.693046, acc 0.601562\n",
      "2018-05-04T11:58:55.563632: step 1752, loss 0.693162, acc 0.492188\n",
      "2018-05-04T11:58:57.336646: step 1753, loss 0.69307, acc 0.507812\n",
      "2018-05-04T11:58:59.159520: step 1754, loss 0.693234, acc 0.5\n",
      "2018-05-04T11:59:01.010999: step 1755, loss 0.693472, acc 0.46875\n",
      "2018-05-04T11:59:02.821206: step 1756, loss 0.693218, acc 0.476562\n",
      "2018-05-04T11:59:04.649305: step 1757, loss 0.693776, acc 0.414062\n",
      "2018-05-04T11:59:06.469931: step 1758, loss 0.693125, acc 0.492188\n",
      "2018-05-04T11:59:08.276340: step 1759, loss 0.692928, acc 0.578125\n",
      "2018-05-04T11:59:10.274914: step 1760, loss 0.693249, acc 0.492188\n",
      "2018-05-04T11:59:12.345519: step 1761, loss 0.693102, acc 0.507812\n",
      "2018-05-04T11:59:14.456573: step 1762, loss 0.693081, acc 0.515625\n",
      "2018-05-04T11:59:16.490996: step 1763, loss 0.693157, acc 0.492188\n",
      "2018-05-04T11:59:18.444552: step 1764, loss 0.692701, acc 0.585938\n",
      "2018-05-04T11:59:20.788078: step 1765, loss 0.693051, acc 0.515625\n",
      "2018-05-04T11:59:22.991953: step 1766, loss 0.693465, acc 0.453125\n",
      "2018-05-04T11:59:25.108935: step 1767, loss 0.692885, acc 0.523438\n",
      "2018-05-04T11:59:27.222709: step 1768, loss 0.692859, acc 0.523438\n",
      "2018-05-04T11:59:29.559244: step 1769, loss 0.692917, acc 0.53125\n",
      "2018-05-04T11:59:32.019217: step 1770, loss 0.693405, acc 0.484375\n",
      "2018-05-04T11:59:34.503552: step 1771, loss 0.694337, acc 0.40625\n",
      "2018-05-04T11:59:37.137036: step 1772, loss 0.692727, acc 0.523438\n",
      "2018-05-04T11:59:39.432695: step 1773, loss 0.693462, acc 0.476562\n",
      "2018-05-04T11:59:41.539314: step 1774, loss 0.692664, acc 0.546875\n",
      "2018-05-04T11:59:43.493078: step 1775, loss 0.692582, acc 0.570312\n",
      "2018-05-04T11:59:45.378402: step 1776, loss 0.692968, acc 0.507812\n",
      "2018-05-04T11:59:47.197747: step 1777, loss 0.693643, acc 0.460938\n",
      "2018-05-04T11:59:48.957927: step 1778, loss 0.693179, acc 0.5\n",
      "2018-05-04T11:59:50.759029: step 1779, loss 0.694125, acc 0.4375\n",
      "2018-05-04T11:59:52.634333: step 1780, loss 0.692613, acc 0.554688\n",
      "2018-05-04T11:59:54.448299: step 1781, loss 0.692529, acc 0.546875\n",
      "2018-05-04T11:59:56.234855: step 1782, loss 0.692615, acc 0.523438\n",
      "2018-05-04T11:59:57.964608: step 1783, loss 0.693182, acc 0.492188\n",
      "2018-05-04T11:59:59.709713: step 1784, loss 0.694462, acc 0.429688\n",
      "2018-05-04T12:00:01.510875: step 1785, loss 0.693476, acc 0.492188\n",
      "2018-05-04T12:00:03.312639: step 1786, loss 0.692938, acc 0.507812\n",
      "2018-05-04T12:00:05.283100: step 1787, loss 0.693336, acc 0.476562\n",
      "2018-05-04T12:00:07.375365: step 1788, loss 0.693068, acc 0.507812\n",
      "2018-05-04T12:00:09.616023: step 1789, loss 0.694315, acc 0.421875\n",
      "2018-05-04T12:00:11.934284: step 1790, loss 0.692872, acc 0.53125\n",
      "2018-05-04T12:00:14.155360: step 1791, loss 0.693257, acc 0.46875\n",
      "2018-05-04T12:00:16.566022: step 1792, loss 0.693217, acc 0.492188\n",
      "2018-05-04T12:00:18.867452: step 1793, loss 0.69352, acc 0.453125\n",
      "2018-05-04T12:00:21.066633: step 1794, loss 0.692836, acc 0.5625\n",
      "2018-05-04T12:00:23.403171: step 1795, loss 0.693058, acc 0.523438\n",
      "2018-05-04T12:00:25.424387: step 1796, loss 0.693183, acc 0.546875\n",
      "2018-05-04T12:00:27.587523: step 1797, loss 0.693043, acc 0.5625\n",
      "2018-05-04T12:00:29.635902: step 1798, loss 0.692905, acc 0.53125\n",
      "2018-05-04T12:00:31.845196: step 1799, loss 0.69347, acc 0.4375\n",
      "2018-05-04T12:00:33.927686: step 1800, loss 0.692446, acc 0.632812\n",
      "2018-05-04T12:00:35.952755: step 1801, loss 0.692738, acc 0.554688\n",
      "2018-05-04T12:00:37.942919: step 1802, loss 0.693251, acc 0.507812\n",
      "2018-05-04T12:00:39.822203: step 1803, loss 0.692952, acc 0.507812\n",
      "2018-05-04T12:00:41.796991: step 1804, loss 0.69322, acc 0.5\n",
      "2018-05-04T12:00:43.726655: step 1805, loss 0.693778, acc 0.476562\n",
      "2018-05-04T12:00:45.544155: step 1806, loss 0.692094, acc 0.554688\n",
      "2018-05-04T12:00:47.345246: step 1807, loss 0.694122, acc 0.460938\n",
      "2018-05-04T12:00:49.135631: step 1808, loss 0.695488, acc 0.40625\n",
      "2018-05-04T12:00:51.046991: step 1809, loss 0.691202, acc 0.59375\n",
      "2018-05-04T12:00:52.876335: step 1810, loss 0.694364, acc 0.453125\n",
      "2018-05-04T12:00:54.821024: step 1811, loss 0.692838, acc 0.515625\n",
      "2018-05-04T12:00:56.710298: step 1812, loss 0.6934, acc 0.5\n",
      "2018-05-04T12:00:58.616785: step 1813, loss 0.69475, acc 0.4375\n",
      "2018-05-04T12:01:00.499111: step 1814, loss 0.693396, acc 0.492188\n",
      "2018-05-04T12:01:02.467481: step 1815, loss 0.692808, acc 0.523438\n",
      "2018-05-04T12:01:04.501058: step 1816, loss 0.692967, acc 0.507812\n",
      "2018-05-04T12:01:06.562292: step 1817, loss 0.694048, acc 0.460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04T12:01:08.584347: step 1818, loss 0.6933, acc 0.492188\n",
      "2018-05-04T12:01:10.506839: step 1819, loss 0.692925, acc 0.507812\n",
      "2018-05-04T12:01:12.567075: step 1820, loss 0.692037, acc 0.554688\n",
      "2018-05-04T12:01:14.583579: step 1821, loss 0.69353, acc 0.484375\n",
      "2018-05-04T12:01:16.603917: step 1822, loss 0.693614, acc 0.484375\n",
      "2018-05-04T12:01:18.640085: step 1823, loss 0.692556, acc 0.546875\n",
      "2018-05-04T12:01:20.637709: step 1824, loss 0.692335, acc 0.554688\n",
      "2018-05-04T12:01:22.616468: step 1825, loss 0.693965, acc 0.445312\n",
      "2018-05-04T12:01:24.738144: step 1826, loss 0.69426, acc 0.429688\n",
      "2018-05-04T12:01:26.821244: step 1827, loss 0.693139, acc 0.5\n",
      "2018-05-04T12:01:28.827775: step 1828, loss 0.69395, acc 0.4375\n",
      "2018-05-04T12:01:30.808134: step 1829, loss 0.693174, acc 0.492188\n",
      "2018-05-04T12:01:33.011991: step 1830, loss 0.692785, acc 0.523438\n",
      "2018-05-04T12:01:35.674976: step 1831, loss 0.693118, acc 0.507812\n",
      "2018-05-04T12:01:38.143039: step 1832, loss 0.692853, acc 0.554688\n",
      "2018-05-04T12:01:40.560470: step 1833, loss 0.693356, acc 0.484375\n",
      "2018-05-04T12:01:43.200805: step 1834, loss 0.692864, acc 0.539062\n",
      "2018-05-04T12:01:45.574090: step 1835, loss 0.693324, acc 0.4375\n",
      "2018-05-04T12:01:47.650335: step 1836, loss 0.692855, acc 0.585938\n",
      "2018-05-04T12:01:49.844837: step 1837, loss 0.693499, acc 0.429688\n",
      "2018-05-04T12:01:51.869900: step 1838, loss 0.693298, acc 0.4375\n",
      "2018-05-04T12:01:53.823094: step 1839, loss 0.69332, acc 0.390625\n",
      "2018-05-04T12:01:55.911169: step 1840, loss 0.692925, acc 0.578125\n",
      "2018-05-04T12:01:57.873601: step 1841, loss 0.693444, acc 0.460938\n",
      "2018-05-04T12:01:59.965146: step 1842, loss 0.693412, acc 0.492188\n",
      "2018-05-04T12:02:01.967033: step 1843, loss 0.692909, acc 0.53125\n",
      "2018-05-04T12:02:03.857671: step 1844, loss 0.692588, acc 0.523438\n",
      "2018-05-04T12:02:05.768693: step 1845, loss 0.694956, acc 0.429688\n",
      "2018-05-04T12:02:07.611899: step 1846, loss 0.692673, acc 0.515625\n",
      "2018-05-04T12:02:09.482407: step 1847, loss 0.692499, acc 0.523438\n",
      "2018-05-04T12:02:11.599297: step 1848, loss 0.694755, acc 0.429688\n",
      "2018-05-04T12:02:13.837658: step 1849, loss 0.693886, acc 0.476562\n",
      "2018-05-04T12:02:16.264028: step 1850, loss 0.692729, acc 0.507812\n",
      "2018-05-04T12:02:18.581865: step 1851, loss 0.694612, acc 0.453125\n",
      "2018-05-04T12:02:20.808494: step 1852, loss 0.69337, acc 0.492188\n",
      "2018-05-04T12:02:23.071191: step 1853, loss 0.691664, acc 0.570312\n",
      "2018-05-04T12:02:25.188961: step 1854, loss 0.692271, acc 0.539062\n",
      "2018-05-04T12:02:27.369946: step 1855, loss 0.692831, acc 0.507812\n",
      "2018-05-04T12:02:29.266103: step 1856, loss 0.693115, acc 0.507812\n",
      "2018-05-04T12:02:31.125732: step 1857, loss 0.694776, acc 0.453125\n",
      "2018-05-04T12:02:32.939783: step 1858, loss 0.693728, acc 0.515625\n",
      "2018-05-04T12:02:34.845456: step 1859, loss 0.694368, acc 0.453125\n",
      "2018-05-04T12:02:36.845584: step 1860, loss 0.692685, acc 0.507812\n",
      "2018-05-04T12:02:38.815832: step 1861, loss 0.693255, acc 0.507812\n",
      "2018-05-04T12:02:40.840177: step 1862, loss 0.692048, acc 0.554688\n",
      "2018-05-04T12:02:43.015435: step 1863, loss 0.694082, acc 0.476562\n",
      "2018-05-04T12:02:45.025942: step 1864, loss 0.692849, acc 0.515625\n",
      "2018-05-04T12:02:47.148990: step 1865, loss 0.692936, acc 0.523438\n",
      "2018-05-04T12:02:49.192885: step 1866, loss 0.692878, acc 0.523438\n",
      "2018-05-04T12:02:51.153011: step 1867, loss 0.694474, acc 0.4375\n",
      "2018-05-04T12:02:53.080087: step 1868, loss 0.693731, acc 0.476562\n",
      "2018-05-04T12:02:54.929850: step 1869, loss 0.693385, acc 0.46875\n",
      "2018-05-04T12:02:56.728038: step 1870, loss 0.692973, acc 0.53125\n",
      "2018-05-04T12:02:58.742227: step 1871, loss 0.693721, acc 0.460938\n",
      "2018-05-04T12:03:00.810770: step 1872, loss 0.692213, acc 0.546875\n",
      "2018-05-04T12:03:02.944655: step 1873, loss 0.69283, acc 0.53125\n",
      "2018-05-04T12:03:05.065869: step 1874, loss 0.693786, acc 0.460938\n",
      "2018-05-04T12:03:07.457815: step 1875, loss 0.694151, acc 0.398438\n",
      "2018-05-04T12:03:10.023035: step 1876, loss 0.694084, acc 0.445312\n",
      "2018-05-04T12:03:12.289510: step 1877, loss 0.693441, acc 0.453125\n",
      "2018-05-04T12:03:14.398343: step 1878, loss 0.693424, acc 0.460938\n",
      "2018-05-04T12:03:16.625187: step 1879, loss 0.692988, acc 0.578125\n",
      "2018-05-04T12:03:18.957163: step 1880, loss 0.692981, acc 0.484375\n",
      "2018-05-04T12:03:21.466659: step 1881, loss 0.692561, acc 0.570312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d438384dfaf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-25c4910401f0>\u001b[0m in \u001b[0;36mbatch_iter\u001b[0;34m(x, y, batch_size, num_epochs, shuffle)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mend_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batched_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-25c4910401f0>\u001b[0m in \u001b[0;36mget_batched_one_hot\u001b[0;34m(char_seqs_indices, labels, start_index, end_index)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexample_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_seq_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar_pos_in_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_seq_char_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_seq_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mchar_seq_char_ind\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mx_batch_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_seq_char_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_pos_in_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_batch_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=allow_soft_placement, log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = CharCNN(sequence_max_length=600)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "    \n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run([train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            dev_size = len(x_batch)\n",
    "            max_batch_size = 500\n",
    "            num_batches = dev_size/max_batch_size\n",
    "            acc = []\n",
    "            losses = []\n",
    "            print(\"Number of batches in dev set is \" + str(num_batches))\n",
    "            for i in range(num_batches):\n",
    "                x_batch_dev, y_batch_dev = get_batched_one_hot(x_batch, y_batch, i * max_batch_size, (i + 1) * max_batch_size)\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch_dev,\n",
    "                  cnn.input_y: y_batch_dev,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run([global_step, dev_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "                acc.append(accuracy)\n",
    "                losses.append(loss)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"batch \" + str(i + 1) + \" in dev >>\" +\n",
    "                      \" {}: loss {:g}, acc {:g}\".format(time_str, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "            print(\"\\nMean accuracy=\" + str(sum(acc)/len(acc)))\n",
    "            print(\"Mean loss=\" + str(sum(losses)/len(losses)))\n",
    "\n",
    "\n",
    "        for batch in batch_iter(x_train, y_train, batch_size, num_epochs):\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3.6)",
   "language": "python",
   "name": "conda_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
