{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset  \n",
    "- amazon review\n",
    "http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz\n",
    "\n",
    "- chABSA\n",
    "https://github.com/chakki-works/chABSA-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ShopRunner/jupyter-notify\n",
    "```\n",
    "pip install jupyternotify\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib.learn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    " \n",
    "        outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded,W,strides=[1, 1, 1, 1],padding=\"VALID\", name=\"conv\")\n",
    "                bn_conv = self.batch_normalization(conv) \n",
    "                h = tf.nn.relu(tf.nn.bias_add(bn_conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID',name=\"pool\")\n",
    "                dropped = tf.nn.dropout(pooled, self.dropout_keep_prob, name=\"dropout\")\n",
    "                outputs.append(dropped)\n",
    "\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h = tf.concat(outputs, 3)\n",
    "        self.h_flat = tf.reshape(self.h, [-1, num_filters_total])\n",
    "        \n",
    "\n",
    "        with tf.name_scope(\"fc-1\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, 1024], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(self.h_flat, W, b), name=\"fc-1-out\")\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"dropout-1\"):\n",
    "            drop_1 = tf.nn.dropout(fc_1_output, self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"fc-2\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024,1024], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            fc_2_output = tf.nn.relu(tf.nn.xw_plus_b(drop_1, W, b), name=\"fc-2-out\")\n",
    "            \n",
    "        with tf.name_scope(\"dropout-2\"):\n",
    "            drop_2 = tf.nn.dropout(fc_2_output, self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"fc-3\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024, num_classes], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(drop_2, W, b, name=\"output\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, \"float\"), name=\"accuracy\") \n",
    "            \n",
    "            \n",
    "    def batch_normalization(self, x):\n",
    "        \"\"\"\n",
    "          x -> γ(x-μ)/√（σ^2-ε）　+ β\n",
    "      \n",
    "          γ : scale\n",
    "          μ: mean (first moment)\n",
    "          σ: variance (second moment)\n",
    "          β: offset\n",
    "          ε: to avoid dividing by 0\n",
    "        \"\"\"\n",
    "        epsilon = 1e-5\n",
    "        dim = x.get_shape()[-1]\n",
    "        scale = tf.Variable(tf.ones([dim]))\n",
    "        offset = tf.Variable(tf.zeros([dim]))\n",
    "        mean, variance = tf.nn.moments(x, [0,1,2])\n",
    "        return tf.nn.batch_normalization(x, mean, variance, offset, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_data_file = \"data/amazon/book_pos.txt\"\n",
    "#negative_data_file = \"data/amazon/book_neg.txt\"\n",
    "\n",
    "positive_data_file = \"data/amazon_ja/pos.txt\"\n",
    "negative_data_file = \"data/amazon_ja/neg.txt\"\n",
    "\n",
    "long_doc =True\n",
    "\n",
    "#positive_data_file = \"data/chABSA/pos.txt\"\n",
    "#negative_data_file = \"data/chABSA/neg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file, level=\"char\"):\n",
    "       \n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    if level == \"char\":\n",
    "        positive_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in positive_examples]\n",
    "        negative_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in negative_examples]\n",
    "    elif level == \"word\":\n",
    "        positive_examples = [s.strip() for s in positive_examples]\n",
    "        negative_examples = [s.strip() for s in negative_examples]\n",
    "    else:\n",
    "        print(\"invaid value of 'level'. ('char' or 'word') \")\n",
    "        \n",
    "    x_text = positive_examples + negative_examples\n",
    "\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    return x_text, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"47739b26-4c7e-4a77-bb6a-41b00375d2a0\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"47739b26-4c7e-4a77-bb6a-41b00375d2a0\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x_text, y = load_data_and_labels(positive_data_file, negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' p s 4 で の ボ イ ス チ ャ ッ ト 、 キ ャ プ チ ャ 用 途 で 購 入 し ま し た マ イ ク は 別 途 持 っ て い る の で u s b デ バ イ ス だ け が 欲 し か っ た ん で す が 、 安 い 割 に ま と も に 使 え ま す 付 属 の u s b デ バ イ ス の ア ナ ロ グ 出 力 の 品 質 に つ い て は 実 際 の s / n 比 も 悪 く な く ノ イ ズ は 気 に な ら な い レ ベ ル で す 以 下 、 ケ ー ブ ル で 外 部 ミ キ サ ー と 接 続 し て い ま す ↓ マ イ ク の 入 力 用 h o s a c m p - 1 0 3 9 1 c m ス テ レ オ ミ ニ プ ラ グ - モ ノ ラ ル フ ォ ン プ ラ グ オ ー デ ィ オ ケ ー ブ ル ↓ 出 力 用 s i l k r o a d y c b - 1 1 5 - 1 . 5 ス テ レ オ ミ ニ プ ラ グ - 1 / 4 フ ォ ン オ ー デ ィ オ ケ ー ブ ル 1 . 5 m 以 上 、 総 じ て 満 足 の い く 買 い 物 で し た p s 4 で の 使 用 に つ い て も 書 き ま す 付 属 の u s b デ バ イ ス を 接 続 す る と ヘ ッ ド セ ッ ト の 扱 い に な り ま す ＜ 接 続 時 ＞ \\u3000 h d m i = シ ス テ ム 、 ゲ ー ム の 音 \\u3000 u s b デ バ イ ス = チ ャ ッ ト 音 声 p s 4 本 体 を t v に h d m i で 接 続 し て い る 場 合 、 t v か ら は 相 手 の チ ャ ッ ト 音 声 が 聞 こ え ま せ ん u s b デ バ イ ス の 「 a u d i o o u t 」 に 出 力 さ れ る の で イ ヤ フ ォ ン を 挿 す こ と に な り ま す そ れ だ と チ ャ ッ ト 音 声 し か 聞 こ え ま せ ん が 、 p s 4 本 体 の 設 定 を 変 え る こ と で チ ャ ッ ト 音 声 、 シ ス テ ム 、 ゲ ー ム の 音 全 て を u s b デ バ イ ス か ら 出 力 さ せ る こ と は 可 能 で す ※ 逆 に 言 う と 「 音 声 と ゲ ー ム 音 を 別 々 に 出 せ る 」 の で 編 集 は や り 易 く な り ま す が ※ h d m i の 出 力 に 含 め ら れ な い の は ち ょ っ と 不 便 と 感 じ ま し た p s 4 本 体 で キ ャ プ チ ャ 、 シ ェ ア 、 配 信 す る 場 合 は 設 定 で チ ャ ッ ト 音 声 を 含 め る よ う に す る だ け で 難 し い こ と は あ り ま せ ん が 、 外 部 デ バ イ ス で 配 信 、 キ ャ プ チ ャ を 行 う 場 合 は 外 部 ミ キ サ ー は 必 須 だ と 思 い ま す \\n '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = np.array([len(r)for r in x_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length\n",
       "0    1339\n",
       "1     533\n",
       "2     321\n",
       "3     183\n",
       "4     153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16995.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>288.150809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>425.152913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>193.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>323.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>531.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17495.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "count  16995.000000\n",
       "mean     288.150809\n",
       "std      425.152913\n",
       "min       27.000000\n",
       "50%      193.000000\n",
       "75%      323.000000\n",
       "80%      373.000000\n",
       "90%      531.000000\n",
       "max    17495.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(percentiles=[0.5,0.75,0.8,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c1f1d4e80>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1f243cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16995.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>218.608650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>117.271831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>193.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>323.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "count  16995.000000\n",
       "mean     218.608650\n",
       "std      117.271831\n",
       "min       27.000000\n",
       "50%      193.000000\n",
       "75%      323.000000\n",
       "80%      373.000000\n",
       "90%      400.000000\n",
       "max      400.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_length = 1000\n",
    "max_length = 400\n",
    "if long_doc:\n",
    "    x_text = [x[:max_length] if len(x) > max_length else x for x in x_text]\n",
    "length_list = np.array([len(r)for r in x_text])\n",
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.describe(percentiles=[0.5,0.75,0.8,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c1efabcc0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1f1fdcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"char\" #\"word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max document length 400\n"
     ]
    }
   ],
   "source": [
    "if level == \"word\":\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "elif level == \"char\":\n",
    "    max_document_length = max([len(x) for x in x_text])\n",
    "print(\"max document length\", max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-ef6c3fae6839>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tdual/anaconda2/envs/py3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tdual/anaconda2/envs/py3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"f49fe533-c21a-4edb-868b-3e8170cc0eb3\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"f49fe533-c21a-4edb-868b-3e8170cc0eb3\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percentage = 0.1#0.0010 #0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2612\n",
      "Train/Test split: 15296/1699\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"72d803d6-9155-4d6d-8e14-734e228e4c72\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"72d803d6-9155-4d6d-8e14-734e228e4c72\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "test_sample_index = -1 * int(test_percentage * float(len(y)))\n",
    "x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]\n",
    "\n",
    "#del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15296, 400)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    print(\"num of epochs: \", num_epochs)\n",
    "    print(\"num of batches: \", num_batches_per_epoch)\n",
    "    print(\"num of step: \", num_batches_per_epoch*num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "vocab_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64     \n",
    "filter_sizes = [2,3,4,5]    \n",
    "num_filters=128               \n",
    "dropout_keep_prob=0.5\n",
    "l2_reg_lambda=0.1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TextCNN at 0x1c273329b0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ./runs/2018_07_01_23_04_49/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_path = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "prefix = \"\"\n",
    "out_dir = os.path.join(os.path.curdir, \"runs\", time_path, prefix)\n",
    "print(\"Writing to {}\\n\".format(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64                  \n",
    "num_epochs = 200            \n",
    "evaluate_every = 20         \n",
    "num_checkpoints = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "allow_soft_placement = True    \n",
    "log_device_placement = False  \n",
    "\n",
    "save_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/Variable:0/grad/hist is illegal; using conv-maxpool-2/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/Variable:0/grad/sparsity is illegal; using conv-maxpool-2/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/Variable_1:0/grad/hist is illegal; using conv-maxpool-2/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-2/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable:0/grad/hist is illegal; using conv-maxpool-3/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable:0/grad/sparsity is illegal; using conv-maxpool-3/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable_1:0/grad/hist is illegal; using conv-maxpool-3/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-3/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable:0/grad/hist is illegal; using conv-maxpool-4/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable:0/grad/sparsity is illegal; using conv-maxpool-4/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable_1:0/grad/hist is illegal; using conv-maxpool-4/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-4/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable:0/grad/hist is illegal; using conv-maxpool-5/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable:0/grad/sparsity is illegal; using conv-maxpool-5/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable_1:0/grad/hist is illegal; using conv-maxpool-5/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-5/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/hist is illegal; using fc-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/sparsity is illegal; using fc-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/hist is illegal; using fc-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/sparsity is illegal; using fc-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/hist is illegal; using fc-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/sparsity is illegal; using fc-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/hist is illegal; using fc-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/sparsity is illegal; using fc-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/hist is illegal; using fc-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/sparsity is illegal; using fc-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/hist is illegal; using fc-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/sparsity is illegal; using fc-3/b_0/grad/sparsity instead.\n",
      "num of epochs:  200\n",
      "num of batches:  239\n",
      "num of step:  47800\n",
      "2018-07-01T23:04:54.968036: step 1, loss 4.51225, acc 0.46875\n",
      "2018-07-01T23:04:55.531588: step 2, loss 3.97333, acc 0.796875\n",
      "2018-07-01T23:04:56.065309: step 3, loss 3.69808, acc 0.75\n",
      "2018-07-01T23:04:56.607958: step 4, loss 3.25712, acc 0.828125\n",
      "2018-07-01T23:04:57.157271: step 5, loss 2.98417, acc 0.796875\n",
      "2018-07-01T23:04:57.703835: step 6, loss 2.76565, acc 0.765625\n",
      "2018-07-01T23:04:58.266303: step 7, loss 2.52651, acc 0.78125\n",
      "2018-07-01T23:04:58.812700: step 8, loss 2.30673, acc 0.78125\n",
      "2018-07-01T23:04:59.335324: step 9, loss 2.13234, acc 0.796875\n",
      "2018-07-01T23:04:59.865163: step 10, loss 1.98143, acc 0.796875\n",
      "2018-07-01T23:05:00.445026: step 11, loss 1.85642, acc 0.78125\n",
      "2018-07-01T23:05:00.956358: step 12, loss 1.71119, acc 0.796875\n",
      "2018-07-01T23:05:01.472523: step 13, loss 1.5367, acc 0.859375\n",
      "2018-07-01T23:05:02.080803: step 14, loss 1.53779, acc 0.828125\n",
      "2018-07-01T23:05:02.651302: step 15, loss 1.44923, acc 0.8125\n",
      "2018-07-01T23:05:03.201377: step 16, loss 1.47434, acc 0.765625\n",
      "2018-07-01T23:05:03.775631: step 17, loss 1.44192, acc 0.765625\n",
      "2018-07-01T23:05:04.356161: step 18, loss 1.29406, acc 0.890625\n",
      "2018-07-01T23:05:04.901074: step 19, loss 1.29843, acc 0.828125\n",
      "2018-07-01T23:05:05.484989: step 20, loss 1.34327, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:05:10.631170: step 20, loss 1.28569, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-20\n",
      "\n",
      "2018-07-01T23:05:11.329595: step 21, loss 1.19542, acc 0.828125\n",
      "2018-07-01T23:05:11.832435: step 22, loss 1.2288, acc 0.796875\n",
      "2018-07-01T23:05:12.346513: step 23, loss 1.27216, acc 0.75\n",
      "2018-07-01T23:05:12.905127: step 24, loss 1.19246, acc 0.796875\n",
      "2018-07-01T23:05:13.407587: step 25, loss 1.21481, acc 0.734375\n",
      "2018-07-01T23:05:13.988496: step 26, loss 1.21969, acc 0.734375\n",
      "2018-07-01T23:05:14.517252: step 27, loss 1.17883, acc 0.734375\n",
      "2018-07-01T23:05:15.130958: step 28, loss 1.1418, acc 0.78125\n",
      "2018-07-01T23:05:15.701307: step 29, loss 1.10927, acc 0.78125\n",
      "2018-07-01T23:05:16.229859: step 30, loss 1.08552, acc 0.765625\n",
      "2018-07-01T23:05:16.748564: step 31, loss 1.17188, acc 0.71875\n",
      "2018-07-01T23:05:17.214914: step 32, loss 1.06536, acc 0.765625\n",
      "2018-07-01T23:05:17.715061: step 33, loss 1.01146, acc 0.796875\n",
      "2018-07-01T23:05:18.248614: step 34, loss 0.997762, acc 0.78125\n",
      "2018-07-01T23:05:18.798644: step 35, loss 1.00521, acc 0.78125\n",
      "2018-07-01T23:05:19.301303: step 36, loss 1.06584, acc 0.71875\n",
      "2018-07-01T23:05:19.801657: step 37, loss 0.882051, acc 0.859375\n",
      "2018-07-01T23:05:20.342264: step 38, loss 0.960508, acc 0.765625\n",
      "2018-07-01T23:05:20.850910: step 39, loss 0.925317, acc 0.78125\n",
      "2018-07-01T23:05:21.358652: step 40, loss 0.971624, acc 0.765625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699\n",
      "2018-07-01T23:05:25.285332: step 40, loss 0.914057, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-40\n",
      "\n",
      "2018-07-01T23:05:25.930062: step 41, loss 0.933815, acc 0.765625\n",
      "2018-07-01T23:05:26.422086: step 42, loss 0.98784, acc 0.71875\n",
      "2018-07-01T23:05:26.890102: step 43, loss 0.900427, acc 0.75\n",
      "2018-07-01T23:05:27.354421: step 44, loss 0.818864, acc 0.84375\n",
      "2018-07-01T23:05:27.836010: step 45, loss 0.86101, acc 0.78125\n",
      "2018-07-01T23:05:28.265477: step 46, loss 0.863441, acc 0.78125\n",
      "2018-07-01T23:05:28.723580: step 47, loss 0.86103, acc 0.765625\n",
      "2018-07-01T23:05:29.187428: step 48, loss 0.793726, acc 0.796875\n",
      "2018-07-01T23:05:29.661890: step 49, loss 0.836736, acc 0.765625\n",
      "2018-07-01T23:05:30.152139: step 50, loss 0.730189, acc 0.828125\n",
      "2018-07-01T23:05:30.597606: step 51, loss 0.84231, acc 0.765625\n",
      "2018-07-01T23:05:31.073168: step 52, loss 0.825181, acc 0.75\n",
      "2018-07-01T23:05:31.541015: step 53, loss 0.859846, acc 0.734375\n",
      "2018-07-01T23:05:32.011558: step 54, loss 0.823341, acc 0.75\n",
      "2018-07-01T23:05:32.460841: step 55, loss 0.790019, acc 0.78125\n",
      "2018-07-01T23:05:32.920099: step 56, loss 0.850587, acc 0.703125\n",
      "2018-07-01T23:05:33.388072: step 57, loss 0.708222, acc 0.875\n",
      "2018-07-01T23:05:33.875393: step 58, loss 0.848612, acc 0.71875\n",
      "2018-07-01T23:05:34.325272: step 59, loss 0.639111, acc 0.875\n",
      "2018-07-01T23:05:34.774429: step 60, loss 0.823317, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:05:37.805990: step 60, loss 0.74904, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-60\n",
      "\n",
      "2018-07-01T23:05:38.444431: step 61, loss 0.788994, acc 0.765625\n",
      "2018-07-01T23:05:38.931450: step 62, loss 0.740477, acc 0.8125\n",
      "2018-07-01T23:05:39.389910: step 63, loss 0.630675, acc 0.859375\n",
      "2018-07-01T23:05:39.890895: step 64, loss 0.68008, acc 0.796875\n",
      "2018-07-01T23:05:40.312929: step 65, loss 0.806281, acc 0.734375\n",
      "2018-07-01T23:05:40.724094: step 66, loss 0.74373, acc 0.75\n",
      "2018-07-01T23:05:41.140847: step 67, loss 0.637734, acc 0.859375\n",
      "2018-07-01T23:05:41.557860: step 68, loss 0.819211, acc 0.6875\n",
      "2018-07-01T23:05:42.041416: step 69, loss 0.690171, acc 0.796875\n",
      "2018-07-01T23:05:42.525167: step 70, loss 0.735811, acc 0.734375\n",
      "2018-07-01T23:05:43.007817: step 71, loss 0.633995, acc 0.828125\n",
      "2018-07-01T23:05:43.500230: step 72, loss 0.644597, acc 0.796875\n",
      "2018-07-01T23:05:44.006502: step 73, loss 0.727793, acc 0.765625\n",
      "2018-07-01T23:05:44.519990: step 74, loss 0.574448, acc 0.84375\n",
      "2018-07-01T23:05:45.034026: step 75, loss 0.587064, acc 0.859375\n",
      "2018-07-01T23:05:45.515832: step 76, loss 0.593426, acc 0.828125\n",
      "2018-07-01T23:05:46.094736: step 77, loss 0.547682, acc 0.875\n",
      "2018-07-01T23:05:46.656147: step 78, loss 0.618423, acc 0.8125\n",
      "2018-07-01T23:05:47.183460: step 79, loss 0.559503, acc 0.859375\n",
      "2018-07-01T23:05:47.697911: step 80, loss 0.734839, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:05:51.537510: step 80, loss 0.647963, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-80\n",
      "\n",
      "2018-07-01T23:05:52.258369: step 81, loss 0.618798, acc 0.8125\n",
      "2018-07-01T23:05:52.778149: step 82, loss 0.626958, acc 0.796875\n",
      "2018-07-01T23:05:53.323725: step 83, loss 0.629636, acc 0.8125\n",
      "2018-07-01T23:05:53.930689: step 84, loss 0.557506, acc 0.84375\n",
      "2018-07-01T23:05:54.418785: step 85, loss 0.53649, acc 0.828125\n",
      "2018-07-01T23:05:54.938194: step 86, loss 0.676861, acc 0.78125\n",
      "2018-07-01T23:05:55.423689: step 87, loss 0.699606, acc 0.75\n",
      "2018-07-01T23:05:55.933867: step 88, loss 0.681496, acc 0.765625\n",
      "2018-07-01T23:05:56.439479: step 89, loss 0.590556, acc 0.828125\n",
      "2018-07-01T23:05:56.919541: step 90, loss 0.664629, acc 0.75\n",
      "2018-07-01T23:05:57.405204: step 91, loss 0.612191, acc 0.8125\n",
      "2018-07-01T23:05:57.908680: step 92, loss 0.630939, acc 0.78125\n",
      "2018-07-01T23:05:58.387814: step 93, loss 0.636356, acc 0.796875\n",
      "2018-07-01T23:05:58.860933: step 94, loss 0.646885, acc 0.765625\n",
      "2018-07-01T23:05:59.331288: step 95, loss 0.550546, acc 0.828125\n",
      "2018-07-01T23:05:59.813898: step 96, loss 0.563983, acc 0.8125\n",
      "2018-07-01T23:06:00.288472: step 97, loss 0.471542, acc 0.875\n",
      "2018-07-01T23:06:00.743673: step 98, loss 0.634959, acc 0.78125\n",
      "2018-07-01T23:06:01.230951: step 99, loss 0.709709, acc 0.71875\n",
      "2018-07-01T23:06:01.731282: step 100, loss 0.624573, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:06:04.889266: step 100, loss 0.60341, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-100\n",
      "\n",
      "2018-07-01T23:06:05.587966: step 101, loss 0.552918, acc 0.828125\n",
      "2018-07-01T23:06:06.054716: step 102, loss 0.5761, acc 0.8125\n",
      "2018-07-01T23:06:06.605793: step 103, loss 0.653152, acc 0.71875\n",
      "2018-07-01T23:06:07.189225: step 104, loss 0.608071, acc 0.796875\n",
      "2018-07-01T23:06:07.674031: step 105, loss 0.57811, acc 0.78125\n",
      "2018-07-01T23:06:08.195131: step 106, loss 0.496119, acc 0.890625\n",
      "2018-07-01T23:06:08.717897: step 107, loss 0.595886, acc 0.796875\n",
      "2018-07-01T23:06:09.262024: step 108, loss 0.609989, acc 0.78125\n",
      "2018-07-01T23:06:09.764691: step 109, loss 0.850602, acc 0.6875\n",
      "2018-07-01T23:06:10.292695: step 110, loss 0.631381, acc 0.75\n",
      "2018-07-01T23:06:10.804260: step 111, loss 0.52043, acc 0.84375\n",
      "2018-07-01T23:06:11.295250: step 112, loss 0.588388, acc 0.78125\n",
      "2018-07-01T23:06:11.773345: step 113, loss 0.593379, acc 0.78125\n",
      "2018-07-01T23:06:12.227123: step 114, loss 0.658482, acc 0.6875\n",
      "2018-07-01T23:06:12.706272: step 115, loss 0.546663, acc 0.8125\n",
      "2018-07-01T23:06:13.202652: step 116, loss 0.50278, acc 0.875\n",
      "2018-07-01T23:06:13.684491: step 117, loss 0.656252, acc 0.71875\n",
      "2018-07-01T23:06:14.165621: step 118, loss 0.506334, acc 0.828125\n",
      "2018-07-01T23:06:14.660958: step 119, loss 0.57527, acc 0.796875\n",
      "2018-07-01T23:06:15.148217: step 120, loss 0.446417, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:06:19.632994: step 120, loss 0.603419, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-120\n",
      "\n",
      "2018-07-01T23:06:20.491932: step 121, loss 0.574787, acc 0.78125\n",
      "2018-07-01T23:06:21.127482: step 122, loss 0.60233, acc 0.796875\n",
      "2018-07-01T23:06:21.785209: step 123, loss 0.681589, acc 0.734375\n",
      "2018-07-01T23:06:22.384027: step 124, loss 0.558338, acc 0.796875\n",
      "2018-07-01T23:06:22.927491: step 125, loss 0.585285, acc 0.78125\n",
      "2018-07-01T23:06:23.385805: step 126, loss 0.505102, acc 0.859375\n",
      "2018-07-01T23:06:23.872797: step 127, loss 0.506059, acc 0.859375\n",
      "2018-07-01T23:06:24.321495: step 128, loss 0.567667, acc 0.796875\n",
      "2018-07-01T23:06:24.827796: step 129, loss 0.590822, acc 0.765625\n",
      "2018-07-01T23:06:25.365216: step 130, loss 0.496846, acc 0.8125\n",
      "2018-07-01T23:06:25.892159: step 131, loss 0.572513, acc 0.78125\n",
      "2018-07-01T23:06:26.354616: step 132, loss 0.704984, acc 0.71875\n",
      "2018-07-01T23:06:26.882914: step 133, loss 0.478649, acc 0.828125\n",
      "2018-07-01T23:06:27.368618: step 134, loss 0.568166, acc 0.8125\n",
      "2018-07-01T23:06:27.825284: step 135, loss 0.566772, acc 0.8125\n",
      "2018-07-01T23:06:28.310159: step 136, loss 0.604763, acc 0.75\n",
      "2018-07-01T23:06:28.728747: step 137, loss 0.576895, acc 0.796875\n",
      "2018-07-01T23:06:29.141294: step 138, loss 0.53569, acc 0.8125\n",
      "2018-07-01T23:06:29.592158: step 139, loss 0.561532, acc 0.8125\n",
      "2018-07-01T23:06:30.051622: step 140, loss 0.656861, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:06:34.137296: step 140, loss 0.562635, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-140\n",
      "\n",
      "2018-07-01T23:06:34.920510: step 141, loss 0.511378, acc 0.828125\n",
      "2018-07-01T23:06:35.522058: step 142, loss 0.459047, acc 0.859375\n",
      "2018-07-01T23:06:36.104672: step 143, loss 0.585005, acc 0.78125\n",
      "2018-07-01T23:06:36.716959: step 144, loss 0.71071, acc 0.71875\n",
      "2018-07-01T23:06:37.240290: step 145, loss 0.762536, acc 0.65625\n",
      "2018-07-01T23:06:37.783066: step 146, loss 0.511747, acc 0.828125\n",
      "2018-07-01T23:06:38.301310: step 147, loss 0.482716, acc 0.875\n",
      "2018-07-01T23:06:38.824485: step 148, loss 0.551861, acc 0.78125\n",
      "2018-07-01T23:06:39.291822: step 149, loss 0.583625, acc 0.75\n",
      "2018-07-01T23:06:39.782416: step 150, loss 0.557735, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-01T23:06:40.333143: step 151, loss 0.535425, acc 0.8125\n",
      "2018-07-01T23:06:40.835483: step 152, loss 0.514675, acc 0.8125\n",
      "2018-07-01T23:06:41.429705: step 153, loss 0.722085, acc 0.6875\n",
      "2018-07-01T23:06:42.007284: step 154, loss 0.509008, acc 0.8125\n",
      "2018-07-01T23:06:42.523131: step 155, loss 0.604248, acc 0.734375\n",
      "2018-07-01T23:06:43.030656: step 156, loss 0.536011, acc 0.8125\n",
      "2018-07-01T23:06:43.526783: step 157, loss 0.619497, acc 0.75\n",
      "2018-07-01T23:06:44.034936: step 158, loss 0.594269, acc 0.75\n",
      "2018-07-01T23:06:44.583282: step 159, loss 0.535089, acc 0.828125\n",
      "2018-07-01T23:06:45.111148: step 160, loss 0.545409, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:06:48.951133: step 160, loss 0.556079, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-160\n",
      "\n",
      "2018-07-01T23:06:49.711816: step 161, loss 0.572708, acc 0.75\n",
      "2018-07-01T23:06:50.273185: step 162, loss 0.562833, acc 0.78125\n",
      "2018-07-01T23:06:50.802612: step 163, loss 0.516943, acc 0.8125\n",
      "2018-07-01T23:06:51.311347: step 164, loss 0.472588, acc 0.828125\n",
      "2018-07-01T23:06:51.862617: step 165, loss 0.485646, acc 0.84375\n",
      "2018-07-01T23:06:52.415874: step 166, loss 0.472569, acc 0.828125\n",
      "2018-07-01T23:06:52.876207: step 167, loss 0.571131, acc 0.78125\n",
      "2018-07-01T23:06:53.384962: step 168, loss 0.575495, acc 0.796875\n",
      "2018-07-01T23:06:53.902074: step 169, loss 0.567877, acc 0.796875\n",
      "2018-07-01T23:06:54.391763: step 170, loss 0.553631, acc 0.765625\n",
      "2018-07-01T23:06:54.930274: step 171, loss 0.561015, acc 0.765625\n",
      "2018-07-01T23:06:55.504500: step 172, loss 0.492001, acc 0.859375\n",
      "2018-07-01T23:06:56.097602: step 173, loss 0.656918, acc 0.703125\n",
      "2018-07-01T23:06:56.564169: step 174, loss 0.417797, acc 0.890625\n",
      "2018-07-01T23:06:57.051181: step 175, loss 0.571581, acc 0.78125\n",
      "2018-07-01T23:06:57.508281: step 176, loss 0.520861, acc 0.828125\n",
      "2018-07-01T23:06:58.017868: step 177, loss 0.474958, acc 0.84375\n",
      "2018-07-01T23:06:58.512081: step 178, loss 0.583613, acc 0.796875\n",
      "2018-07-01T23:06:58.996151: step 179, loss 0.618898, acc 0.734375\n",
      "2018-07-01T23:06:59.506638: step 180, loss 0.527613, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:07:03.333197: step 180, loss 0.555362, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-180\n",
      "\n",
      "2018-07-01T23:07:04.017338: step 181, loss 0.501294, acc 0.84375\n",
      "2018-07-01T23:07:04.529024: step 182, loss 0.578836, acc 0.78125\n",
      "2018-07-01T23:07:05.118700: step 183, loss 0.516063, acc 0.828125\n",
      "2018-07-01T23:07:05.561936: step 184, loss 0.517698, acc 0.796875\n",
      "2018-07-01T23:07:06.021008: step 185, loss 0.685818, acc 0.703125\n",
      "2018-07-01T23:07:06.497819: step 186, loss 0.490827, acc 0.828125\n",
      "2018-07-01T23:07:06.977864: step 187, loss 0.546191, acc 0.796875\n",
      "2018-07-01T23:07:07.475812: step 188, loss 0.607122, acc 0.734375\n",
      "2018-07-01T23:07:08.139253: step 189, loss 0.549448, acc 0.796875\n",
      "2018-07-01T23:07:08.778421: step 190, loss 0.612551, acc 0.75\n",
      "2018-07-01T23:07:09.371751: step 191, loss 0.592944, acc 0.734375\n",
      "2018-07-01T23:07:09.949086: step 192, loss 0.532154, acc 0.8125\n",
      "2018-07-01T23:07:10.478112: step 193, loss 0.599182, acc 0.765625\n",
      "2018-07-01T23:07:11.007667: step 194, loss 0.509747, acc 0.828125\n",
      "2018-07-01T23:07:11.553795: step 195, loss 0.611232, acc 0.765625\n",
      "2018-07-01T23:07:12.153487: step 196, loss 0.546917, acc 0.765625\n",
      "2018-07-01T23:07:12.732317: step 197, loss 0.485511, acc 0.828125\n",
      "2018-07-01T23:07:13.277322: step 198, loss 0.494585, acc 0.8125\n",
      "2018-07-01T23:07:13.840809: step 199, loss 0.478389, acc 0.84375\n",
      "2018-07-01T23:07:14.438549: step 200, loss 0.819753, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "1699\n",
      "2018-07-01T23:07:17.887157: step 200, loss 0.554071, acc 0.778693\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/2018_07_01_23_04_49/checkpoints/model-200\n",
      "\n",
      "2018-07-01T23:07:18.579337: step 201, loss 0.474385, acc 0.8125\n",
      "2018-07-01T23:07:19.065550: step 202, loss 0.602674, acc 0.734375\n",
      "2018-07-01T23:07:19.593791: step 203, loss 0.517882, acc 0.84375\n",
      "2018-07-01T23:07:20.184091: step 204, loss 0.61043, acc 0.734375\n",
      "2018-07-01T23:07:20.778023: step 205, loss 0.600444, acc 0.734375\n",
      "2018-07-01T23:07:21.349580: step 206, loss 0.621808, acc 0.734375\n",
      "2018-07-01T23:07:21.927084: step 207, loss 0.571335, acc 0.78125\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)\n",
    "\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        \n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        test_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "        test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "        if save_checkpoint:\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "  \n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run([train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def test_step(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            predictions, step, summaries, loss, accuracy = sess.run([cnn.predictions, global_step, test_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            \n",
    "            print(len(np.where(predictions==1)[0]))\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                test_step(x_test, y_test, writer=test_summary_writer)\n",
    "                print(\"\")\n",
    "                \n",
    "            if save_checkpoint and current_step % evaluate_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char level cnn \n",
    "- 1525408577  \n",
    "- 1526189751 chABSA  good\n",
    "- 1523936751 amazon good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_neg_posi(text):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "              allow_soft_placement=allow_soft_placement,\n",
    "              log_device_placement=log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(out_dir, \"vocab\"))\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_size,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "        \n",
    "            text = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in [text]]\n",
    "            x = np.array(list(vocab_processor.fit_transform(text)))\n",
    "            feed_dict = {\n",
    "                  cnn.input_x: x,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            feature_2 = sess.run(cnn.scores, feed_dict=feed_dict)\n",
    "    print(feature_2[0])\n",
    "    if np.argmax(feature_2[0]):\n",
    "        print(\"positive\")\n",
    "    else:\n",
    "        print(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_neg_posi(\"伸び率は増加傾向にありました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_neg_posi(\"伸び率は減少傾向にありました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_neg_posi(\"伸び率は加向にありました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_neg_posi(\"伸び率は減向にありました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir=\"runs/1523936751\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(out_dir, \"checkpoints\" )\n",
    "latest_ckpt = tf.train.get_checkpoint_state(ckpt_dir).model_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(out_dir, \"vocab\"))\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_size,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, latest_ckpt)\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        w = sess.run(cnn.W, feed_dict={cnn.input_x: x_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_tensor.tsv','w') as f:\n",
    "    for char_vec in w:\n",
    "        for weight in char_vec:\n",
    "            f.write(str(weight)+ \"\\t\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "vocab_dict = vocab_processor.vocabulary_._reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open('embedding_metadata.tsv' ,'w') as f:\n",
    "    f.write('Titles\\tGenres\\n')\n",
    "    for i,v in enumerate(w):\n",
    "        f.write(\"%s\\t%s\\n\" % (vocab_dict[i], vocab_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(x):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_dim,\n",
    "                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "\n",
    "            feed_dict = {\n",
    "                  cnn.input_x: x,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "            feature_5, feature_2 = sess.run([cnn.f_h, cnn.scores ], feed_dict=feed_dict)\n",
    "    return feature_5, feature_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = list(open(\"data/amazon/rating_5.txt\", \"r\").readlines())\n",
    "review = [s.strip() for s in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []\n",
    "for r in review:\n",
    "    l = r.split(\":::::\")\n",
    "    y.append(float(l[0]))\n",
    "    x.append(l[1].replace(\" \", \"\").replace(\"\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(\"runs/1525408577\", \"vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x)))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5 ,feature_2 = get_feature(x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "chunk_size = 100\n",
    "for i in range(0, len(x) , chunk_size):\n",
    "    feature_5 ,feature_2 = get_feature(x[i:i+chunk_size])\n",
    "    for f, r in zip(feature_5, y[i:i+chunk_size]):\n",
    "        s  += int(np.argmax(f) == r)\n",
    "    print(s/(i+chunk_size))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(feature_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2 [0]  #[neg, pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3.6)",
   "language": "python",
   "name": "conda_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
