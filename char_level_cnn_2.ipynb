{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm is notebook version\n",
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "try:\n",
    "    if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        print(\"tqdm is notebook version\")\n",
    "        from tqdm import tqdm_notebook as tqdm\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "except (NameError, RuntimeError):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_end(char_seq):\n",
    "    if len(char_seq) > 1014:\n",
    "        char_seq = char_seq[-1014:]\n",
    "    return char_seq\n",
    "\n",
    "\n",
    "def pad_sentence(char_seq, padding_char=\" \"):\n",
    "    char_seq_length = 1014\n",
    "    num_padding = char_seq_length - len(char_seq)\n",
    "    new_char_seq = char_seq + [padding_char] * num_padding\n",
    "    return new_char_seq\n",
    "\n",
    "\n",
    "def string_to_int8_conversion(char_seq, alphabet):\n",
    "    x = np.array([alphabet.find(char) for char in char_seq], dtype=np.int8)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_batched_one_hot(char_seqs_indices, labels, start_index, end_index):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\"\n",
    "    x_batch = char_seqs_indices[start_index:end_index]\n",
    "    y_batch = labels[start_index:end_index]\n",
    "    x_batch_one_hot = np.zeros(shape=[len(x_batch), len(alphabet), len(x_batch[0]), 1])\n",
    "    for example_i, char_seq_indices in enumerate(x_batch):\n",
    "        for char_pos_in_seq, char_seq_char_ind in enumerate(char_seq_indices):\n",
    "            if char_seq_char_ind != -1:\n",
    "                x_batch_one_hot[example_i][char_seq_char_ind][char_pos_in_seq][0] = 1\n",
    "    return [x_batch_one_hot, y_batch]\n",
    "\n",
    "\n",
    "def batch_iter(x, y, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    # data = np.array(data)\n",
    "    data_size = len(x)\n",
    "    num_batches_per_epoch = int(data_size/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"In epoch >> \" + str(epoch + 1))\n",
    "        print(\"num batches per epoch is: \" + str(num_batches_per_epoch))\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            x_shuffled = x[shuffle_indices]\n",
    "            y_shuffled = y[shuffle_indices]\n",
    "        else:\n",
    "            x_shuffled = x\n",
    "            y_shuffled = y\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            x_batch, y_batch = get_batched_one_hot(x_shuffled, y_shuffled, start_index, end_index)\n",
    "            batch = list(zip(x_batch, y_batch))\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "evaluate_every = 5000\n",
    "checkpoint_every = 1000\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True #\"Allow device soft device placement\")\n",
    "log_device_placement = False  #, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_file = \"data/amazon/book_pos.txt\"\n",
    "negative_data_file = \"data/amazon/book_neg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sample_percentage = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(positive_file_path, negative_data_file, alphabet):\n",
    "    examples = []\n",
    "    with open(positive_file_path) as f:\n",
    "        i = 0\n",
    "        for line in tqdm(f):\n",
    "            i += 1\n",
    "            text = line\n",
    "            text_end_extracted = extract_end(list(text.lower()))\n",
    "            padded = pad_sentence(text_end_extracted)\n",
    "            text_int8_repr = string_to_int8_conversion(padded, alphabet)           \n",
    "            examples.append(text_int8_repr)\n",
    "    n_posi = i\n",
    "    print(\"# of positive\", n_posi)\n",
    "        \n",
    "    with open(negative_data_file) as f:\n",
    "        i = 0\n",
    "        for line in tqdm(f):\n",
    "            i += 1\n",
    "            text = line\n",
    "            text_end_extracted = extract_end(list(text.lower()))\n",
    "            padded = pad_sentence(text_end_extracted)\n",
    "            text_int8_repr = string_to_int8_conversion(padded, alphabet)           \n",
    "            examples.append(text_int8_repr)\n",
    "    n_neg = i   \n",
    "    print(\"# of negative\", n_neg)\n",
    "    \n",
    "    positive_labels = [[0, 1] for _ in range(n_posi)]\n",
    "    negative_labels = [[1, 0] for _ in range(n_neg)]\n",
    "    labels = positive_labels + negative_labels\n",
    "    return np.array(examples, dtype=np.int8), np.array(labels, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2963a35ed4974e35bdf73a722810cdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of positive 500000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd49ee2e3644cd7abf6ad4c27021d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of negative 500000\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"1b499302-92bb-474d-8255-d5b34a70eb3b\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"1b499302-92bb-474d-8255-d5b34a70eb3b\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x, y = load_data(positive_data_file, negative_data_file, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 14, 21, ..., -1, -1, -1], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1014)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 999500/500\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"03fdd14c-5028-4894-8e57-bb3bf422e6f4\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"03fdd14c-5028-4894-8e57-bb3bf422e6f4\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "#del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    based on the Character-level Convolutional Networks for Text Classification paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, filter_sizes=(7, 7, 3, 3, 3, 3), num_filters_per_size=256,\n",
    "                 l2_reg_lambda=0.0, sequence_max_length=1014, num_quantized_chars=70):\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, num_quantized_chars, sequence_max_length, 1], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "    \n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # ================ Layer 1 ================\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            filter_shape = [num_quantized_chars, filter_sizes[0], 1, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(self.input_x, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv1\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, 1, 3, 1], strides=[1, 1, 3, 1], padding='VALID', name=\"pool1\")\n",
    "\n",
    "        # ================ Layer 2 ================\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            filter_shape = [1, filter_sizes[1], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(pooled, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv2\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, 1, 3, 1], strides=[1, 1, 3, 1], padding='VALID', name=\"pool2\")\n",
    "\n",
    "        # ================ Layer 3 ================\n",
    "        with tf.name_scope(\"conv-3\"):\n",
    "            filter_shape = [1, filter_sizes[2], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(pooled, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv3\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 4 ================\n",
    "        with tf.name_scope(\"conv-4\"):\n",
    "            filter_shape = [1, filter_sizes[3], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv4\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 5 ================\n",
    "        with tf.name_scope(\"conv-5\"):\n",
    "            filter_shape = [1, filter_sizes[4], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv5\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # ================ Layer 6 ================\n",
    "        with tf.name_scope(\"conv-maxpool-6\"):\n",
    "            filter_shape = [1, filter_sizes[5], num_filters_per_size, num_filters_per_size]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters_per_size]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(h, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv6\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, 1, 3, 1], strides=[1, 1, 3, 1], padding='VALID', name=\"pool6\")\n",
    "\n",
    "        # ================ Layer 7 ================\n",
    "        num_features_total = 34 * num_filters_per_size\n",
    "        h_pool_flat = tf.reshape(pooled, [-1, num_features_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout-1\"):\n",
    "            drop1 = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        with tf.name_scope(\"fc-1\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_features_total, 1024], stddev=0.05), name=\"W\")\n",
    "            # W = tf.get_variable(\"W\", shape=[num_features_total, 1024],\n",
    "            #                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            # l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "            fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(drop1, W, b), name=\"fc-1-out\")\n",
    "\n",
    "        # ================ Layer 8 ================\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout-2\"):\n",
    "            drop2 = tf.nn.dropout(fc_1_output, self.dropout_keep_prob)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        with tf.name_scope(\"fc-2\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024, 1024], stddev=0.05), name=\"W\")\n",
    "            # W = tf.get_variable(\"W\", shape=[1024, 1024],\n",
    "            #                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            # l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "            fc_2_output = tf.nn.relu(tf.nn.xw_plus_b(drop2, W, b), name=\"fc-2-out\")\n",
    "\n",
    "        # ================ Layer 9 ================\n",
    "        # Fully connected layer 3\n",
    "        with tf.name_scope(\"fc-3\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024, num_classes], stddev=0.05), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            # l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(fc_2_output, W, b, name=\"output\")\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            \n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-3/W:0/grad/hist is illegal; using conv-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-3/W:0/grad/sparsity is illegal; using conv-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-3/b:0/grad/hist is illegal; using conv-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-3/b:0/grad/sparsity is illegal; using conv-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-4/W:0/grad/hist is illegal; using conv-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-4/W:0/grad/sparsity is illegal; using conv-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-4/b:0/grad/hist is illegal; using conv-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-4/b:0/grad/sparsity is illegal; using conv-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-5/W:0/grad/hist is illegal; using conv-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-5/W:0/grad/sparsity is illegal; using conv-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-5/b:0/grad/hist is illegal; using conv-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-5/b:0/grad/sparsity is illegal; using conv-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/hist is illegal; using conv-maxpool-6/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/sparsity is illegal; using conv-maxpool-6/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/hist is illegal; using conv-maxpool-6/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/sparsity is illegal; using conv-maxpool-6/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/hist is illegal; using fc-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/sparsity is illegal; using fc-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/hist is illegal; using fc-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/sparsity is illegal; using fc-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/hist is illegal; using fc-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/sparsity is illegal; using fc-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/hist is illegal; using fc-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/sparsity is illegal; using fc-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/hist is illegal; using fc-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/sparsity is illegal; using fc-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/hist is illegal; using fc-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/sparsity is illegal; using fc-3/b_0/grad/sparsity instead.\n",
      "Writing to /Users/tdual/Workspace/char_level_cnn/runs/1525351360\n",
      "\n",
      "In epoch >> 1\n",
      "num batches per epoch is: 7809\n",
      "2018-05-03T21:42:45.933734: step 1, loss 1.49588, acc 0.460938\n",
      "2018-05-03T21:42:49.094630: step 2, loss 39.6225, acc 0.492188\n",
      "2018-05-03T21:42:52.208589: step 3, loss 4.15379, acc 0.585938\n",
      "2018-05-03T21:42:55.345397: step 4, loss 3.54859, acc 0.5625\n",
      "2018-05-03T21:42:58.729451: step 5, loss 2.01146, acc 0.484375\n",
      "2018-05-03T21:43:02.010467: step 6, loss 0.937126, acc 0.5\n",
      "2018-05-03T21:43:05.188467: step 7, loss 0.796784, acc 0.476562\n",
      "2018-05-03T21:43:08.401778: step 8, loss 0.7965, acc 0.53125\n",
      "2018-05-03T21:43:11.713226: step 9, loss 0.778664, acc 0.492188\n",
      "2018-05-03T21:43:14.988651: step 10, loss 0.791722, acc 0.429688\n",
      "2018-05-03T21:43:18.243922: step 11, loss 0.816164, acc 0.445312\n",
      "2018-05-03T21:43:21.499617: step 12, loss 0.753534, acc 0.539062\n",
      "2018-05-03T21:43:24.809647: step 13, loss 0.824273, acc 0.554688\n",
      "2018-05-03T21:43:28.206492: step 14, loss 0.783465, acc 0.445312\n",
      "2018-05-03T21:43:31.628713: step 15, loss 0.810654, acc 0.46875\n",
      "2018-05-03T21:43:35.218128: step 16, loss 0.734891, acc 0.492188\n",
      "2018-05-03T21:43:38.771024: step 17, loss 0.719256, acc 0.507812\n",
      "2018-05-03T21:43:41.880184: step 18, loss 0.793451, acc 0.46875\n",
      "2018-05-03T21:43:45.168538: step 19, loss 0.722603, acc 0.539062\n",
      "2018-05-03T21:43:48.309288: step 20, loss 0.695502, acc 0.554688\n",
      "2018-05-03T21:43:51.522349: step 21, loss 0.697814, acc 0.53125\n",
      "2018-05-03T21:43:54.754644: step 22, loss 0.710564, acc 0.492188\n",
      "2018-05-03T21:43:57.994275: step 23, loss 0.786394, acc 0.53125\n",
      "2018-05-03T21:44:01.112442: step 24, loss 0.766756, acc 0.507812\n",
      "2018-05-03T21:44:04.308132: step 25, loss 0.698714, acc 0.546875\n",
      "2018-05-03T21:44:07.615739: step 26, loss 0.724561, acc 0.515625\n",
      "2018-05-03T21:44:10.733882: step 27, loss 0.788165, acc 0.523438\n",
      "2018-05-03T21:44:13.863227: step 28, loss 0.745559, acc 0.546875\n",
      "2018-05-03T21:44:17.006367: step 29, loss 0.709853, acc 0.515625\n",
      "2018-05-03T21:44:20.074219: step 30, loss 0.731191, acc 0.507812\n",
      "2018-05-03T21:44:23.106504: step 31, loss 0.756971, acc 0.46875\n",
      "2018-05-03T21:44:26.369608: step 32, loss 0.704905, acc 0.523438\n",
      "2018-05-03T21:44:29.633666: step 33, loss 0.729038, acc 0.460938\n",
      "2018-05-03T21:44:32.713006: step 34, loss 0.704349, acc 0.507812\n",
      "2018-05-03T21:44:35.917536: step 35, loss 0.752637, acc 0.445312\n",
      "2018-05-03T21:44:39.041376: step 36, loss 0.715769, acc 0.46875\n",
      "2018-05-03T21:44:42.361977: step 37, loss 0.731229, acc 0.414062\n",
      "2018-05-03T21:44:45.684368: step 38, loss 0.718659, acc 0.523438\n",
      "2018-05-03T21:44:48.855549: step 39, loss 0.707752, acc 0.460938\n",
      "2018-05-03T21:44:51.901673: step 40, loss 0.689445, acc 0.539062\n",
      "2018-05-03T21:44:55.067219: step 41, loss 0.713029, acc 0.53125\n",
      "2018-05-03T21:44:58.118519: step 42, loss 0.72679, acc 0.445312\n",
      "2018-05-03T21:45:01.108375: step 43, loss 0.70959, acc 0.5\n",
      "2018-05-03T21:45:04.131574: step 44, loss 0.70183, acc 0.507812\n",
      "2018-05-03T21:45:07.274198: step 45, loss 0.702794, acc 0.5\n",
      "2018-05-03T21:45:10.445129: step 46, loss 0.695308, acc 0.59375\n",
      "2018-05-03T21:45:13.733905: step 47, loss 0.710833, acc 0.492188\n",
      "2018-05-03T21:45:16.783366: step 48, loss 0.70243, acc 0.515625\n",
      "2018-05-03T21:45:19.820986: step 49, loss 0.694472, acc 0.476562\n",
      "2018-05-03T21:45:23.136002: step 50, loss 0.722847, acc 0.484375\n",
      "2018-05-03T21:45:26.401702: step 51, loss 0.726142, acc 0.421875\n",
      "2018-05-03T21:45:29.540510: step 52, loss 0.727069, acc 0.40625\n",
      "2018-05-03T21:45:32.657656: step 53, loss 0.730814, acc 0.445312\n",
      "2018-05-03T21:45:35.881270: step 54, loss 0.724509, acc 0.46875\n",
      "2018-05-03T21:45:38.901190: step 55, loss 0.699674, acc 0.539062\n",
      "2018-05-03T21:45:41.917226: step 56, loss 0.688199, acc 0.585938\n",
      "2018-05-03T21:45:45.016935: step 57, loss 0.697722, acc 0.515625\n",
      "2018-05-03T21:45:48.274627: step 58, loss 0.700489, acc 0.476562\n",
      "2018-05-03T21:45:51.293646: step 59, loss 0.72122, acc 0.5625\n",
      "2018-05-03T21:45:54.268671: step 60, loss 0.705322, acc 0.515625\n",
      "2018-05-03T21:45:57.320388: step 61, loss 0.678357, acc 0.554688\n",
      "2018-05-03T21:46:00.493881: step 62, loss 0.7023, acc 0.492188\n",
      "2018-05-03T21:46:03.678907: step 63, loss 0.696954, acc 0.46875\n",
      "2018-05-03T21:46:07.017689: step 64, loss 0.69808, acc 0.523438\n",
      "2018-05-03T21:46:10.326455: step 65, loss 0.689644, acc 0.570312\n",
      "2018-05-03T21:46:13.468149: step 66, loss 0.691447, acc 0.554688\n",
      "2018-05-03T21:46:16.871141: step 67, loss 0.712076, acc 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-03T21:46:20.121852: step 68, loss 0.704973, acc 0.492188\n",
      "2018-05-03T21:46:23.432265: step 69, loss 0.707782, acc 0.460938\n",
      "2018-05-03T21:46:26.824786: step 70, loss 0.706786, acc 0.523438\n",
      "2018-05-03T21:46:30.104101: step 71, loss 0.714924, acc 0.476562\n",
      "2018-05-03T21:46:33.376833: step 72, loss 0.700051, acc 0.523438\n",
      "2018-05-03T21:46:36.999858: step 73, loss 0.716235, acc 0.453125\n",
      "2018-05-03T21:46:40.369107: step 74, loss 0.698281, acc 0.5\n",
      "2018-05-03T21:46:43.575898: step 75, loss 0.701392, acc 0.445312\n",
      "2018-05-03T21:46:46.908152: step 76, loss 0.731964, acc 0.453125\n",
      "2018-05-03T21:46:50.263346: step 77, loss 0.722428, acc 0.523438\n",
      "2018-05-03T21:46:53.506591: step 78, loss 0.703788, acc 0.515625\n",
      "2018-05-03T21:46:56.788704: step 79, loss 0.70932, acc 0.453125\n",
      "2018-05-03T21:47:00.049595: step 80, loss 0.679639, acc 0.59375\n",
      "2018-05-03T21:47:03.309445: step 81, loss 0.705155, acc 0.515625\n",
      "2018-05-03T21:47:06.474094: step 82, loss 0.696992, acc 0.523438\n",
      "2018-05-03T21:47:09.533868: step 83, loss 0.702915, acc 0.53125\n",
      "2018-05-03T21:47:12.605393: step 84, loss 0.711938, acc 0.484375\n",
      "2018-05-03T21:47:15.929570: step 85, loss 0.697096, acc 0.507812\n",
      "2018-05-03T21:47:18.951268: step 86, loss 0.705852, acc 0.507812\n",
      "2018-05-03T21:47:21.970807: step 87, loss 0.690975, acc 0.5\n",
      "2018-05-03T21:47:25.023566: step 88, loss 0.694316, acc 0.515625\n",
      "2018-05-03T21:47:28.073747: step 89, loss 0.71521, acc 0.445312\n",
      "2018-05-03T21:47:31.055120: step 90, loss 0.726023, acc 0.460938\n",
      "2018-05-03T21:47:34.139010: step 91, loss 0.7168, acc 0.453125\n",
      "2018-05-03T21:47:37.467009: step 92, loss 0.703865, acc 0.523438\n",
      "2018-05-03T21:47:40.542202: step 93, loss 0.707067, acc 0.460938\n",
      "2018-05-03T21:47:43.656311: step 94, loss 0.694183, acc 0.507812\n",
      "2018-05-03T21:47:46.896620: step 95, loss 0.701497, acc 0.523438\n",
      "2018-05-03T21:47:50.175988: step 96, loss 0.708908, acc 0.429688\n",
      "2018-05-03T21:47:53.331127: step 97, loss 0.704489, acc 0.453125\n",
      "2018-05-03T21:47:56.538503: step 98, loss 0.696995, acc 0.507812\n",
      "2018-05-03T21:47:59.652729: step 99, loss 0.715228, acc 0.429688\n",
      "2018-05-03T21:48:02.820901: step 100, loss 0.712234, acc 0.40625\n",
      "2018-05-03T21:48:05.975657: step 101, loss 0.717922, acc 0.453125\n",
      "2018-05-03T21:48:09.119721: step 102, loss 0.697995, acc 0.523438\n",
      "2018-05-03T21:48:12.258830: step 103, loss 0.694382, acc 0.539062\n",
      "2018-05-03T21:48:15.471717: step 104, loss 0.695337, acc 0.546875\n",
      "2018-05-03T21:48:18.678644: step 105, loss 0.692052, acc 0.507812\n",
      "2018-05-03T21:48:21.757304: step 106, loss 0.71205, acc 0.476562\n",
      "2018-05-03T21:48:24.878354: step 107, loss 0.709535, acc 0.453125\n",
      "2018-05-03T21:48:27.959685: step 108, loss 0.700767, acc 0.453125\n",
      "2018-05-03T21:48:31.139689: step 109, loss 0.707997, acc 0.53125\n",
      "2018-05-03T21:48:34.332922: step 110, loss 0.681374, acc 0.609375\n",
      "2018-05-03T21:48:37.580157: step 111, loss 0.702998, acc 0.515625\n",
      "2018-05-03T21:48:40.826216: step 112, loss 0.705733, acc 0.554688\n",
      "2018-05-03T21:48:44.003382: step 113, loss 0.710584, acc 0.484375\n",
      "2018-05-03T21:48:47.207443: step 114, loss 0.723041, acc 0.351562\n",
      "2018-05-03T21:48:50.334639: step 115, loss 0.70507, acc 0.445312\n",
      "2018-05-03T21:48:53.553853: step 116, loss 0.694827, acc 0.515625\n",
      "2018-05-03T21:48:57.080865: step 117, loss 0.706014, acc 0.507812\n",
      "2018-05-03T21:49:00.320663: step 118, loss 0.710273, acc 0.46875\n",
      "2018-05-03T21:49:03.442064: step 119, loss 0.708597, acc 0.460938\n",
      "2018-05-03T21:49:06.549354: step 120, loss 0.698, acc 0.484375\n",
      "2018-05-03T21:49:09.975296: step 121, loss 0.681503, acc 0.609375\n",
      "2018-05-03T21:49:13.150203: step 122, loss 0.700439, acc 0.476562\n",
      "2018-05-03T21:49:16.494850: step 123, loss 0.713138, acc 0.46875\n",
      "2018-05-03T21:49:19.793841: step 124, loss 0.723062, acc 0.4375\n",
      "2018-05-03T21:49:23.109600: step 125, loss 0.706856, acc 0.476562\n",
      "2018-05-03T21:49:26.400404: step 126, loss 0.703025, acc 0.492188\n",
      "2018-05-03T21:49:29.705662: step 127, loss 0.70473, acc 0.484375\n",
      "2018-05-03T21:49:32.977305: step 128, loss 0.683592, acc 0.570312\n",
      "2018-05-03T21:49:36.566608: step 129, loss 0.737499, acc 0.453125\n",
      "2018-05-03T21:49:39.821523: step 130, loss 0.707571, acc 0.507812\n",
      "2018-05-03T21:49:43.111136: step 131, loss 0.713341, acc 0.4375\n",
      "2018-05-03T21:49:46.460303: step 132, loss 0.712917, acc 0.453125\n",
      "2018-05-03T21:49:49.758277: step 133, loss 0.689462, acc 0.539062\n",
      "2018-05-03T21:49:52.963925: step 134, loss 0.715033, acc 0.4375\n",
      "2018-05-03T21:49:56.157833: step 135, loss 0.707184, acc 0.421875\n",
      "2018-05-03T21:49:59.405074: step 136, loss 0.70107, acc 0.460938\n",
      "2018-05-03T21:50:02.815577: step 137, loss 0.697048, acc 0.492188\n",
      "2018-05-03T21:50:06.392469: step 138, loss 0.698107, acc 0.539062\n",
      "2018-05-03T21:50:09.774515: step 139, loss 0.721093, acc 0.460938\n",
      "2018-05-03T21:50:13.011756: step 140, loss 0.73459, acc 0.460938\n",
      "2018-05-03T21:50:16.275940: step 141, loss 0.681266, acc 0.546875\n",
      "2018-05-03T21:50:19.507486: step 142, loss 0.711257, acc 0.46875\n",
      "2018-05-03T21:50:22.703575: step 143, loss 0.703601, acc 0.46875\n",
      "2018-05-03T21:50:26.136543: step 144, loss 0.697798, acc 0.5\n",
      "2018-05-03T21:50:29.241455: step 145, loss 0.684489, acc 0.5625\n",
      "2018-05-03T21:50:32.321449: step 146, loss 0.703383, acc 0.515625\n",
      "2018-05-03T21:50:35.417153: step 147, loss 0.71392, acc 0.476562\n",
      "2018-05-03T21:50:38.448317: step 148, loss 0.732509, acc 0.375\n",
      "2018-05-03T21:50:41.468925: step 149, loss 0.707264, acc 0.46875\n",
      "2018-05-03T21:50:44.484540: step 150, loss 0.70223, acc 0.492188\n",
      "2018-05-03T21:50:47.538441: step 151, loss 0.716755, acc 0.453125\n",
      "2018-05-03T21:50:50.553271: step 152, loss 0.687163, acc 0.546875\n",
      "2018-05-03T21:50:53.548133: step 153, loss 0.680821, acc 0.546875\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=allow_soft_placement, log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = CharCNN()\n",
    "        #cnn = TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "    \n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run([train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            dev_size = len(x_batch)\n",
    "            max_batch_size = 500\n",
    "            num_batches = dev_size/max_batch_size\n",
    "            acc = []\n",
    "            losses = []\n",
    "            print(\"Number of batches in dev set is \" + str(num_batches))\n",
    "            for i in range(num_batches):\n",
    "                x_batch_dev, y_batch_dev = get_batched_one_hot(x_batch, y_batch, i * max_batch_size, (i + 1) * max_batch_size)\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch_dev,\n",
    "                  cnn.input_y: y_batch_dev,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run([global_step, dev_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "                acc.append(accuracy)\n",
    "                losses.append(loss)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"batch \" + str(i + 1) + \" in dev >>\" +\n",
    "                      \" {}: loss {:g}, acc {:g}\".format(time_str, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "            print(\"\\nMean accuracy=\" + str(sum(acc)/len(acc)))\n",
    "            print(\"Mean loss=\" + str(sum(losses)/len(losses)))\n",
    "\n",
    "\n",
    "        for batch in batch_iter(x_train, y_train, batch_size, num_epochs):\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3.6)",
   "language": "python",
   "name": "conda_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
