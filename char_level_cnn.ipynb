{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset  \n",
    "- amazon review\n",
    "http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz\n",
    "\n",
    "- chABSA\n",
    "https://github.com/chakki-works/chABSA-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ShopRunner/jupyter-notify\n",
    "```\n",
    "pip install jupyternotify\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install mecab and neologd if you use Japanese.\n",
    "\n",
    "install mecab on mac\n",
    "```\n",
    "brew install mecab mecab-ipadic  \n",
    "pip install mecab-python3\n",
    "```\n",
    "install neologd\n",
    "```\n",
    "git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
    "cd mecab-ipadic-neologd\n",
    "./bin/install-mecab-ipadic-neologd -n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build Tokenizer\n",
      "get stopwords from the web site.\n",
      "Japanese stopword:  あそこ, あたり, あちら ...\n",
      "English stopword: ... you've, z, zero\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from tensorflow.contrib.learn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from numpy.random import choice, randint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import notebookutil as nbu\n",
    "sys.meta_path.append(nbu.NotebookFinder())\n",
    "from util import load_data_and_labels, get_parser\n",
    "\n",
    "\n",
    " \n",
    "%matplotlib inline\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "\n",
    "    def __init__(\n",
    "        self, sequence_length, num_classes, vocab_size, embedding_size, \n",
    "        filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        print(\"# classes\", num_classes)\n",
    "\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.loss_weight = tf.placeholder(tf.float32, name=\"loss_ratio\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    " \n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded,W,strides=[1, 1, 1, 1],padding=\"VALID\", name=\"conv\")\n",
    "                bn_conv = self.batch_normalization(conv) \n",
    "                h = tf.nn.relu(tf.nn.bias_add(bn_conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID',name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "\n",
    "        with tf.name_scope(\"fc-1\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, 1024], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(self.h_pool_flat, W, b), name=\"fc-1-out\")\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"dropout-1\"):\n",
    "            drop_1 = tf.nn.dropout(fc_1_output, self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"fc-2\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024,1024], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            fc_2_output = tf.nn.relu(tf.nn.xw_plus_b(drop_1, W, b), name=\"fc-2-out\")\n",
    "            \n",
    "        with tf.name_scope(\"dropout-2\"):\n",
    "            drop_2 = tf.nn.dropout(fc_2_output, self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"fc-3\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024, num_classes], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(drop_2, W, b, name=\"output\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "      \n",
    "            #　targets * -log(sigmoid(logits)) * pos_weight +　(1 - targets) * -log(1 - sigmoid(logits))\n",
    "            losses = tf.nn.weighted_cross_entropy_with_logits(logits=self.scores, targets=self.input_y, pos_weight=self.loss_weight)\n",
    "            #losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\") \n",
    "            \n",
    "            \n",
    "    def batch_normalization(self, x):\n",
    "        \"\"\"\n",
    "          x -> γ(x-μ)/√（σ^2-ε）　+ β\n",
    "      \n",
    "          γ : scale\n",
    "          μ: mean (first moment)\n",
    "          σ: variance (second moment)\n",
    "          β: offset\n",
    "          ε: to avoid dividing by 0\n",
    "        \"\"\"\n",
    "        epsilon = 1e-5\n",
    "        dim = x.get_shape()[-1]\n",
    "        scale = tf.Variable(tf.ones([dim]))\n",
    "        offset = tf.Variable(tf.zeros([dim]))\n",
    "        mean, variance = tf.nn.moments(x, [0,1,2])\n",
    "        return tf.nn.batch_normalization(x, mean, variance, offset, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_data_file = \"data/amazon/book_pos.txt\"\n",
    "#negative_data_file = \"data/amazon/book_neg.txt\"\n",
    "\n",
    "#positive_data_file = \"data/chABSA/pos.txt\"\n",
    "#negative_data_file = \"data/chABSA/neg.txt\"\n",
    "\n",
    "positive_data_file = \"data/amazon_ja/pos.txt\"\n",
    "negative_data_file = \"data/amazon_ja/neg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"data/amazon_ja/r_{}.txt\".format(i) for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, parser=None):\n",
    "        if parser:\n",
    "            self.parser = parser\n",
    "        else:\n",
    "            self.parser = get_parser()\n",
    "            \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        l = [line.split(\"\\t\") for line in self.parser(text).split(\"\\n\")]\n",
    "        res = \" \".join([i[2] for i in l if len(i) >=4]) # has POS.)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'認める たい ない もの だ な 。 自分自身 の 若さ故の過ち という もの を 。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.tokenize(\"認めたくないものだな。自分自身の若さ故の過ちというものを。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to load data and labels.\n",
      "# pos:  62402\n",
      "# neg:  9060\n",
      "pos/neg: 6.887637969094922\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"cec2702a-6bfd-407d-9e14-8af20d69bd47\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"cec2702a-6bfd-407d-9e14-8af20d69bd47\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "level= \"char\"\n",
    "x_text, y, ratio = load_data_and_labels(positive_data_file, negative_data_file, level=level, lang=\"Ja\", tokenizer=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_doc = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71462"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 書 き 込 み 、 読 み 出 し 、 転 送 速 度 、 い ず れ も 満 足 で す 。 1 6 0 0 万 画 素 の コ ン パ ク ト カ メ ラ タ イ プ の デ ジ カ メ に 入'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = np.array([len(r)for r in x_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length\n",
       "0     549\n",
       "1      67\n",
       "2     179\n",
       "3     227\n",
       "4     311"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>71462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>251.363746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>328.850754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>289.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>329.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>471.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>669.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17495.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "count  71462.000000\n",
       "mean     251.363746\n",
       "std      328.850754\n",
       "min        7.000000\n",
       "50%      171.000000\n",
       "75%      289.000000\n",
       "80%      329.000000\n",
       "90%      471.000000\n",
       "95%      669.000000\n",
       "max    17495.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(percentiles=[0.5,0.75,0.8,0.9,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.quantile(0.95)[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c550dd2b0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c55106be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut length to  471\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>71462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>210.610730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>128.956019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>289.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>329.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>471.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>471.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>471.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "count  71462.000000\n",
       "mean     210.610730\n",
       "std      128.956019\n",
       "min        7.000000\n",
       "50%      171.000000\n",
       "75%      289.000000\n",
       "80%      329.000000\n",
       "90%      471.000000\n",
       "95%      471.000000\n",
       "max      471.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = int(df.quantile(0.9)[\"length\"]) #1000\n",
    "if long_doc:\n",
    "    print(\"cut length to \", max_length)\n",
    "    x_text = [x[:max_length] if len(x) > max_length else x for x in x_text]\n",
    "length_list = np.array([len(r)for r in x_text])\n",
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.describe(percentiles=[0.5,0.75,0.8,0.9,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c89746f98>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c88b57390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-0ac02ce7bc00>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tdual/anaconda2/envs/py3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71462"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tdual/anaconda2/envs/py3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"802a5c58-0943-47d6-b9a4-9d08e77b85e5\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"802a5c58-0943-47d6-b9a4-9d08e77b85e5\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percentage = 0.03 #0.0010 #0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3215\n",
      "Train/Test split: 69319/2143\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"790ac98d-ea49-4d79-a8f4-f5d228a00d58\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"790ac98d-ea49-4d79-a8f4-f5d228a00d58\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "test_sample_index = -1 * int(test_percentage * float(len(y)))\n",
    "x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]\n",
    "\n",
    "#del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69319, 471)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    print(\"num of epochs: \", num_epochs)\n",
    "    print(\"num of batches: \", num_batches_per_epoch)\n",
    "    print(\"num of step: \", num_batches_per_epoch*num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "vocab_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128    \n",
    "filter_sizes = [2,3,4,5,6]    \n",
    "num_filters =128               \n",
    "dropout_keep_prob = 0.5 \n",
    "l2_reg_lambda = 0.01       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TextCNN at 0x1c89053c88>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64                  \n",
    "num_epochs = 30            \n",
    "evaluate_every = 20         \n",
    "num_checkpoints = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "allow_soft_placement = True    \n",
    "log_device_placement = False  \n",
    "\n",
    "save_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ./runs/char_cnn_2/2018_07_22_22_48_46/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_path = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "prefix = \"\"\n",
    "out_dir = os.path.join(os.path.curdir, \"runs\", \"{}_cnn_{}\".format(level,num_classes), time_path, prefix)\n",
    "print(\"Writing to {}\\n\".format(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 2\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/Variable:0/grad/hist is illegal; using conv-maxpool-2/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/Variable:0/grad/sparsity is illegal; using conv-maxpool-2/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/Variable_1:0/grad/hist is illegal; using conv-maxpool-2/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-2/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable:0/grad/hist is illegal; using conv-maxpool-3/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable:0/grad/sparsity is illegal; using conv-maxpool-3/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable_1:0/grad/hist is illegal; using conv-maxpool-3/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-3/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable:0/grad/hist is illegal; using conv-maxpool-4/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable:0/grad/sparsity is illegal; using conv-maxpool-4/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable_1:0/grad/hist is illegal; using conv-maxpool-4/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-4/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable:0/grad/hist is illegal; using conv-maxpool-5/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable:0/grad/sparsity is illegal; using conv-maxpool-5/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable_1:0/grad/hist is illegal; using conv-maxpool-5/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-5/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/hist is illegal; using conv-maxpool-6/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/sparsity is illegal; using conv-maxpool-6/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/hist is illegal; using conv-maxpool-6/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/sparsity is illegal; using conv-maxpool-6/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/Variable:0/grad/hist is illegal; using conv-maxpool-6/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/Variable:0/grad/sparsity is illegal; using conv-maxpool-6/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/Variable_1:0/grad/hist is illegal; using conv-maxpool-6/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-6/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/hist is illegal; using fc-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/sparsity is illegal; using fc-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/hist is illegal; using fc-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/sparsity is illegal; using fc-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/hist is illegal; using fc-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/sparsity is illegal; using fc-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/hist is illegal; using fc-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/sparsity is illegal; using fc-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/hist is illegal; using fc-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/sparsity is illegal; using fc-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/hist is illegal; using fc-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/sparsity is illegal; using fc-3/b_0/grad/sparsity instead.\n",
      "num of epochs:  30\n",
      "num of batches:  1084\n",
      "num of step:  32520\n",
      "2018-07-22T22:49:12.299410: step 20, loss 0.607867, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:49:28.373154: step 20, loss 0.650109, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-20\n",
      "\n",
      "2018-07-22T22:49:49.939899: step 40, loss 0.505531, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:50:03.792413: step 40, loss 0.518366, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-40\n",
      "\n",
      "2018-07-22T22:50:24.757966: step 60, loss 0.500103, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:50:38.473340: step 60, loss 0.470843, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-60\n",
      "\n",
      "2018-07-22T22:50:58.530053: step 80, loss 0.32073, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:51:12.484491: step 80, loss 0.415383, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-80\n",
      "\n",
      "2018-07-22T22:51:31.866038: step 100, loss 0.378351, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:51:45.154810: step 100, loss 0.389676, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-100\n",
      "\n",
      "2018-07-22T22:52:04.309967: step 120, loss 0.34813, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:52:15.776613: step 120, loss 0.376753, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-120\n",
      "\n",
      "2018-07-22T22:52:36.212045: step 140, loss 0.253402, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:52:52.278050: step 140, loss 0.374959, acc 0.875875\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-140\n",
      "\n",
      "2018-07-22T22:53:13.960903: step 160, loss 0.393938, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:53:28.267615: step 160, loss 0.366669, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-160\n",
      "\n",
      "2018-07-22T22:53:48.069683: step 180, loss 0.440399, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:53:58.730515: step 180, loss 0.373926, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-180\n",
      "\n",
      "2018-07-22T22:54:19.422966: step 200, loss 0.386632, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:54:35.159030: step 200, loss 0.364038, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-200\n",
      "\n",
      "2018-07-22T22:54:56.863546: step 220, loss 0.349556, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:55:10.787605: step 220, loss 0.355801, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-220\n",
      "\n",
      "2018-07-22T22:55:32.823844: step 240, loss 0.487685, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:55:45.307984: step 240, loss 0.360466, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-240\n",
      "\n",
      "2018-07-22T22:56:06.258316: step 260, loss 0.37945, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:56:18.525501: step 260, loss 0.332224, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-260\n",
      "\n",
      "2018-07-22T22:56:38.989398: step 280, loss 0.306544, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:56:53.064056: step 280, loss 0.320672, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-280\n",
      "\n",
      "2018-07-22T22:57:15.197013: step 300, loss 0.38388, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:57:28.280761: step 300, loss 0.30698, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-300\n",
      "\n",
      "2018-07-22T22:57:49.059036: step 320, loss 0.371379, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:58:00.479391: step 320, loss 0.31338, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-320\n",
      "\n",
      "2018-07-22T22:58:21.758855: step 340, loss 0.271398, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:58:36.634150: step 340, loss 0.30793, acc 0.875875\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-340\n",
      "\n",
      "2018-07-22T22:58:57.752361: step 360, loss 0.252544, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:59:10.796542: step 360, loss 0.299882, acc 0.887541\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-360\n",
      "\n",
      "2018-07-22T22:59:31.526546: step 380, loss 0.437156, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T22:59:45.479870: step 380, loss 0.29494, acc 0.882408\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-380\n",
      "\n",
      "2018-07-22T23:00:05.761345: step 400, loss 0.270073, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-07-22T23:00:21.820894: step 400, loss 0.288, acc 0.887074\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-400\n",
      "\n",
      "2018-07-22T23:00:43.444640: step 420, loss 0.229153, acc 0.921875\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)\n",
    "\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        \n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        test_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "        test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "        if save_checkpoint:\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "  \n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob,\n",
    "              cnn.loss_weight: 1.0#ratio\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run([train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % 20 == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def test_step(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0,\n",
    "              cnn.loss_weight: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run([global_step, test_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                test_step(x_test, y_test, writer=test_summary_writer)\n",
    "                print(\"\")\n",
    "                \n",
    "            if save_checkpoint and current_step % evaluate_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##char level cnn \n",
    "- 1525408577  \n",
    "- 1526189751 chABSA  good\n",
    "- 1523936751 amazon good\n",
    "- 2018_05_17_12_10_17 chABSA max_length 300\n",
    "- 2018_07_07_10_13_49 amazon_ja\n",
    "\n",
    "##word level cnn \n",
    "- 2018_05_17_12_46_51chABSA\n",
    "- 2018_07_07_13_04_25 amazon_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir = os.path.join(os.path.curdir, \"runs\", \"2018_05_17_12_10_17\", prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(out_dir, \"checkpoints\" )\n",
    "latest_ckpt = tf.train.get_checkpoint_state(ckpt_dir).model_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_neg_posi(text):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "              allow_soft_placement=allow_soft_placement,\n",
    "              log_device_placement=log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(out_dir, \"vocab\"))\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=max_length,\n",
    "                num_classes=2,\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_size=embedding_size,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "        \n",
    "            text = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in [text]]\n",
    "            x = np.array(list(vocab_processor.fit_transform(text)))\n",
    "            feed_dict = {\n",
    "                  cnn.input_x: x,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            feature_2 = sess.run(cnn.scores, feed_dict=feed_dict)\n",
    "    print(feature_2[0])\n",
    "    if np.argmax(feature_2[0]):\n",
    "        print(\"positive\")\n",
    "    else:\n",
    "        print(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_neg_posi(\"当四半期連結累計期間の営業利益は、前年同期比5,184億円増加し、7,127億円となりました。この大幅な増益は、主に半導体分野の大幅な損益改善及び前年同期に映画分野の営業権の減損損失を計上していたことによるものです。 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_neg_posi(\"伸び率は減少傾向にありました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_neg_posi(\"伸び率は加向にありました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_neg_posi(\"伸び率は減向にありました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(x, y, max_length):\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=max_length,\n",
    "                num_classes=2,\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_size=embedding_size,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            saver.restore(sess, latest_ckpt)\n",
    "        \n",
    "            feed_dict = {\n",
    "              cnn.input_x: x,\n",
    "              cnn.input_y: y,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            acc, scores = sess.run([cnn.accuracy, cnn.scores], feed_dict=feed_dict)\n",
    "    return acc, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_char(x_test ,n):\n",
    "    drop_x_test = []\n",
    "    for x in  x_test:\n",
    "        x_c = x.copy()\n",
    "        for i in choice(range(max_length), n,replace=False):\n",
    "            x_c[i] = randint(max_length)\n",
    "        drop_x_test.append(x_c)\n",
    "    return drop_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(x_test, y_test, max_length=max_length)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(drop_char(x_test, 100), y_test, max_length=max_length)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for i in tqdm(range(max_length)):\n",
    "    a,s = eval_acc(drop_char(x_test, i), y_test, max_length=max_length)\n",
    "    print(a)\n",
    "    r.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(max_length), r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drop_doc(positive_data_file, negative_data_file, n):\n",
    "       \n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    po = []\n",
    "    ne = []\n",
    "    t = Tokenizer()\n",
    "    for e in positive_examples:\n",
    "        if len(e) < n:\n",
    "            e_ = \"\"\n",
    "            for _ in  range(len(e)):\n",
    "                e_ += chr(randint(12354, 20000)) + \" \"\n",
    "            e = e_\n",
    "        else:\n",
    "            max_n = n\n",
    "            for i in choice(range(len(e)), max_n, replace=False):\n",
    "                e = e.replace(e[i], chr(randint(12354, 20000)))\n",
    "        po.append(t.tokenize(e))\n",
    "        \n",
    "    for e in negative_examples:\n",
    "        if len(e) < n:\n",
    "            e = \"{} \".format(chr(randint(12354, 20000))) * len(e)\n",
    "        else:\n",
    "            max_n = n\n",
    "            for i in choice(range(len(e)), max_n, replace=False):\n",
    "                e = e.replace(e[i], chr(randint(12354, 20000)))\n",
    "        ne.append(t.tokenize(e))\n",
    "\n",
    "   \n",
    "    positive_examples = po\n",
    "    negative_examples = ne\n",
    "        \n",
    "    x_text = positive_examples + negative_examples\n",
    "\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    #return po, y\n",
    "    return x_text, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = create_drop_doc(positive_data_file, negative_data_file, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[shuffle_indices][test_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(x_test, y_test, max_length=max_length)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(x, y_test, max_length=max_length)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for i in tqdm(range(max_length)):\n",
    "    x_, y_ = create_drop_doc(positive_data_file, negative_data_file, i)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_)))\n",
    "    x = x[shuffle_indices][test_sample_index:]\n",
    "    a,s = eval_acc(x, y_test,max_length=max_length)\n",
    "    print(a)\n",
    "    r.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(max_length), r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(drop_char(), y_test, max_length=max_length)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir=\"runs/1523936751\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(out_dir, \"vocab\"))\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_size,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, latest_ckpt)\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        w = sess.run(cnn.W, feed_dict={cnn.input_x: x_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_tensor.tsv','w') as f:\n",
    "    for char_vec in w:\n",
    "        for weight in char_vec:\n",
    "            f.write(str(weight)+ \"\\t\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "vocab_dict = vocab_processor.vocabulary_._reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open('embedding_metadata.tsv' ,'w') as f:\n",
    "    f.write('Titles\\tGenres\\n')\n",
    "    for i,v in enumerate(w):\n",
    "        f.write(\"%s\\t%s\\n\" % (vocab_dict[i], vocab_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(x):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_dim,\n",
    "                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "\n",
    "            feed_dict = {\n",
    "                  cnn.input_x: x,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "            feature_5, feature_2 = sess.run([cnn.f_h, cnn.scores ], feed_dict=feed_dict)\n",
    "    return feature_5, feature_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = list(open(\"data/amazon/rating_5.txt\", \"r\").readlines())\n",
    "review = [s.strip() for s in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []\n",
    "for r in review:\n",
    "    l = r.split(\":::::\")\n",
    "    y.append(float(l[0]))\n",
    "    x.append(l[1].replace(\" \", \"\").replace(\"\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(\"runs/1525408577\", \"vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x)))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5 ,feature_2 = get_feature(x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "chunk_size = 100\n",
    "for i in range(0, len(x) , chunk_size):\n",
    "    feature_5 ,feature_2 = get_feature(x[i:i+chunk_size])\n",
    "    for f, r in zip(feature_5, y[i:i+chunk_size]):\n",
    "        s  += int(np.argmax(f) == r)\n",
    "    print(s/(i+chunk_size))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(feature_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2 [0]  #[neg, pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3.6)",
   "language": "python",
   "name": "conda_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
