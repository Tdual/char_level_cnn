{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset  \n",
    "- amazon review\n",
    "http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz\n",
    "\n",
    "- chABSA\n",
    "https://github.com/chakki-works/chABSA-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ShopRunner/jupyter-notify\n",
    "```\n",
    "pip install jupyternotify\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install mecab and neologd if you use Japanese.\n",
    "\n",
    "install mecab on mac\n",
    "```\n",
    "brew install mecab mecab-ipadic  \n",
    "pip install mecab-python3\n",
    "```\n",
    "install neologd\n",
    "```\n",
    "git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
    "cd mecab-ipadic-neologd\n",
    "./bin/install-mecab-ipadic-neologd -n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build Tokenizer\n",
      "get stopwords from the web site.\n",
      "Japanese stopword:  あそこ, あたり, あちら ...\n",
      "English stopword: ... you've, z, zero\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from tensorflow.contrib.learn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from numpy.random import choice, randint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import notebookutil as nbu\n",
    "sys.meta_path.append(nbu.NotebookFinder())\n",
    "from util import load_data_and_labels, get_parser\n",
    "\n",
    "\n",
    " \n",
    "%matplotlib inline\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "\n",
    "    def __init__(\n",
    "        self, sequence_length, num_classes, vocab_size, embedding_size, \n",
    "        filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        print(\"# classes\", num_classes)\n",
    "\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.loss_weight = tf.placeholder(tf.float32, name=\"loss_ratio\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    " \n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded,W,strides=[1, 1, 1, 1],padding=\"VALID\", name=\"conv\")\n",
    "                bn_conv = self.batch_normalization(conv) \n",
    "                h = tf.nn.relu(tf.nn.bias_add(bn_conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID',name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "\n",
    "        with tf.name_scope(\"fc-1\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, 1024], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(self.h_pool_flat, W, b), name=\"fc-1-out\")\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"dropout-1\"):\n",
    "            drop_1 = tf.nn.dropout(fc_1_output, self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"fc-2\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024,1024], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            fc_2_output = tf.nn.relu(tf.nn.xw_plus_b(drop_1, W, b), name=\"fc-2-out\")\n",
    "            \n",
    "        with tf.name_scope(\"dropout-2\"):\n",
    "            drop_2 = tf.nn.dropout(fc_2_output, self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"fc-3\"):\n",
    "            W = tf.Variable(tf.truncated_normal([1024, num_classes], stddev=0.01), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(drop_2, W, b, name=\"output\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "      \n",
    "            #　targets * -log(sigmoid(logits)) * pos_weight +　(1 - targets) * -log(1 - sigmoid(logits))\n",
    "            losses = tf.nn.weighted_cross_entropy_with_logits(logits=self.scores, targets=self.input_y, pos_weight=self.loss_weight)\n",
    "            #losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\") \n",
    "            \n",
    "            \n",
    "    def batch_normalization(self, x):\n",
    "        \"\"\"\n",
    "          x -> γ(x-μ)/√（σ^2-ε）　+ β\n",
    "      \n",
    "          γ : scale\n",
    "          μ: mean (first moment)\n",
    "          σ: variance (second moment)\n",
    "          β: offset\n",
    "          ε: to avoid dividing by 0\n",
    "        \"\"\"\n",
    "        epsilon = 1e-5\n",
    "        dim = x.get_shape()[-1]\n",
    "        scale = tf.Variable(tf.ones([dim]))\n",
    "        offset = tf.Variable(tf.zeros([dim]))\n",
    "        mean, variance = tf.nn.moments(x, [0,1,2])\n",
    "        return tf.nn.batch_normalization(x, mean, variance, offset, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_data_file = \"data/amazon/book_pos.txt\"\n",
    "#negative_data_file = \"data/amazon/book_neg.txt\"\n",
    "\n",
    "#positive_data_file = \"data/chABSA/pos.txt\"\n",
    "#negative_data_file = \"data/chABSA/neg.txt\"\n",
    "\n",
    "positive_data_file = \"data/amazon_ja/pos.txt\"\n",
    "negative_data_file = \"data/amazon_ja/neg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"data/amazon_ja/r_{}.txt\".format(i) for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, parser=None):\n",
    "        if parser:\n",
    "            self.parser = parser\n",
    "        else:\n",
    "            self.parser = get_parser()\n",
    "            \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        l = [line.split(\"\\t\") for line in self.parser(text).split(\"\\n\")]\n",
    "        res = \" \".join([i[2] for i in l if len(i) >=4]) # has POS.)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'認める たい ない もの だ な 。 自分自身 の 若さ故の過ち という もの を 。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.tokenize(\"認めたくないものだな。自分自身の若さ故の過ちというものを。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to load data and labels.\n",
      "# pos:  62240\n",
      "# neg:  9026\n",
      "pos/neg: 6.895634832705517\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"552aed8d-a4a7-4ef3-b98f-7dc25ceb1153\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"552aed8d-a4a7-4ef3-b98f-7dc25ceb1153\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "level= \"word\"#\"char\"\n",
    "x_text, y, ratio = load_data_and_labels(positive_data_file, negative_data_file, level=level, lang=\"Ja\", tokenizer=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_doc = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71266"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'書き込む 、 読み出し 、 転送速度 、 いずれ も 満足 です 。 1600 万 画素 の コンパクトカメラ タイプ の デジカメ に 入れる て 撮影 に 使う 、 撮影後 は カード リーダ に'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = np.array([len(r)for r in x_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length\n",
       "0     430\n",
       "1      51\n",
       "2     145\n",
       "3     178\n",
       "4     258"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>71266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>197.862038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>261.041825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>228.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>261.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>373.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>535.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13936.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "count  71266.000000\n",
       "mean     197.862038\n",
       "std      261.041825\n",
       "min        4.000000\n",
       "50%      134.000000\n",
       "75%      228.000000\n",
       "80%      261.000000\n",
       "90%      373.500000\n",
       "95%      535.000000\n",
       "max    13936.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(percentiles=[0.5,0.75,0.8,0.9,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.quantile(0.95)[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x18888dac8>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18888d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = int(df.quantile(0.9)[\"length\"]) #1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_length = 471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut length to  373\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>71266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>165.416959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>103.157737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>228.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>261.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "count  71266.000000\n",
       "mean     165.416959\n",
       "std      103.157737\n",
       "min        4.000000\n",
       "50%      134.000000\n",
       "75%      228.000000\n",
       "80%      261.000000\n",
       "90%      373.000000\n",
       "95%      373.000000\n",
       "max      373.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if long_doc:\n",
    "    print(\"cut length to \", max_length)\n",
    "    x_text = [x[:max_length] if len(x) > max_length else x for x in x_text]\n",
    "length_list = np.array([len(r)for r in x_text])\n",
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.describe(percentiles=[0.5,0.75,0.8,0.9,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1888533c8>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187aa9780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-0ac02ce7bc00>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71266"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"068909e4-80e8-499a-b994-53b7e0a02a54\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"068909e4-80e8-499a-b994-53b7e0a02a54\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percentage = 0.03 #0.0010 #0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 53640\n",
      "Train/Test split: 69129/2137\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"52073bdc-a0b1-4e37-93aa-46699619771a\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"52073bdc-a0b1-4e37-93aa-46699619771a\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "test_sample_index = -1 * int(test_percentage * float(len(y)))\n",
    "x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]\n",
    "\n",
    "#del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69129, 373)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    print(\"num of epochs: \", num_epochs)\n",
    "    print(\"num of batches: \", num_batches_per_epoch)\n",
    "    print(\"num of step: \", num_batches_per_epoch*num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "vocab_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128    \n",
    "filter_sizes = [2,3,4,5,6]    \n",
    "num_filters =128               \n",
    "dropout_keep_prob = 0.5 \n",
    "l2_reg_lambda = 0.01       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TextCNN at 0x18a1067f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64                  \n",
    "num_epochs = 30            \n",
    "evaluate_every = 20         \n",
    "num_checkpoints = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "allow_soft_placement = True    \n",
    "log_device_placement = False  \n",
    "\n",
    "save_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_path = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "prefix = \"\"\n",
    "out_dir = os.path.join(os.path.curdir, \"runs\", \"{}_cnn_{}\".format(level,num_classes), time_path, prefix)\n",
    "print(\"Writing to {}\\n\".format(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)\n",
    "\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        \n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        test_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "        test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "        if save_checkpoint:\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "  \n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob,\n",
    "              cnn.loss_weight: 1.0#ratio\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run([train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % 20 == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def test_step(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0,\n",
    "              cnn.loss_weight: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run([global_step, test_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                test_step(x_test, y_test, writer=test_summary_writer)\n",
    "                print(\"\")\n",
    "                \n",
    "            if save_checkpoint and current_step % evaluate_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##char level cnn \n",
    "- 1525408577  \n",
    "- 1526189751 chABSA  good\n",
    "- 1523936751 amazon good\n",
    "- 2018_05_17_12_10_17 chABSA max_length 300\n",
    "- 2018_07_07_10_13_49 amazon_ja\n",
    "\n",
    "##word level cnn \n",
    "- 2018_05_17_12_46_51chABSA\n",
    "- 2018_07_07_13_04_25 amazon_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char level cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(\"runs\", \"char_cnn_2\",\"2018_07_22_22_48_46\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(out_dir, \"checkpoints\" )\n",
    "latest_ckpt = tf.train.get_checkpoint_state(ckpt_dir).model_checkpoint_path\n",
    "latest_ckpt  = latest_ckpt.split(\"char_level_cnn/\")[1]\n",
    "latest_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 471\n",
    "vocab_size = 3215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_ckpt  = \"runs/char_cnn_2/2018_07_22_22_48_46/checkpoints/model-3940\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TextCNN(\n",
    "    sequence_length=max_length,\n",
    "    num_classes=2,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    filter_sizes=filter_sizes,\n",
    "    num_filters=num_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_neg_posi(text):\n",
    "    with tf.Graph().as_default():\n",
    "        sess = tf.Session()\n",
    "        vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(out_dir, \"vocab\"))\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                    sequence_length=max_length,\n",
    "                    num_classes=2,\n",
    "                    vocab_size=vocab_size,\n",
    "                    embedding_size=embedding_size,\n",
    "                    filter_sizes=filter_sizes,\n",
    "                    num_filters=num_filters)\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "        \n",
    "            text = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in [text]]\n",
    "            x = np.array(list(vocab_processor.fit_transform(text)))\n",
    "            feed_dict = {\n",
    "                  cnn.input_x: x,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "            feature_2 = sess.run(cnn.scores, feed_dict=feed_dict)\n",
    "    print(feature_2[0])\n",
    "    if np.argmax(feature_2[0]):\n",
    "        print(\"positive\")\n",
    "    else:\n",
    "        print(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = \"\"\"\n",
    "これは全然ダメです。使えません。\n",
    "こんな商品を買うんじゃなかった・・・\n",
    "後悔しています。皆さんも買わない方が良いです。\n",
    "\"\"\"\n",
    "check_neg_posi(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = \"\"\"\n",
    "サイコーです！今まで多くの類似商品を買ってきましたが、これほど良いものには出会ったことありません。\n",
    "これからも使い続けたいと思います(*ﾟ▽ﾟ*)\n",
    "\"\"\"\n",
    "check_neg_posi(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(x, y, max_length):\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=max_length,\n",
    "                num_classes=2,\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_size=embedding_size,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "        \n",
    "            feed_dict = {\n",
    "              cnn.input_x: x,\n",
    "              cnn.input_y: y,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            acc, scores = sess.run([cnn.accuracy, cnn.scores], feed_dict=feed_dict)\n",
    "    return acc, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_char(x_test ,n):\n",
    "    drop_x_test = []\n",
    "    for x in  x_test:\n",
    "        x_c = x.copy()\n",
    "        for i in choice(range(max_length), n,replace=False):\n",
    "            x_c[i] = randint(max_length)\n",
    "        drop_x_test.append(x_c)\n",
    "    return drop_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(x_test, y_test, max_length=max_length)\n",
    "print(\"acc: \", a)\n",
    "print(classification_report([v.argmax() for v in y_test], [v.argmax() for v in s], digits=4 ,target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(drop_char(x_test, 10), y_test, max_length=max_length)\n",
    "print(\"acc: \", a)\n",
    "print(classification_report([v.argmax() for v in y_test], [v.argmax() for v in s], digits=4 ,target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_change_leng = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for i in tqdm(range(max_change_leng)):\n",
    "    a,s = eval_acc(drop_char(x_test, i), y_test, max_length=max_length)\n",
    "    print(a)\n",
    "    r.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(max_change_leng), r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(\"runs\", \"word_cnn_2\",\"2018_07_25_14_46_48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(out_dir, \"checkpoints\" )\n",
    "latest_ckpt = tf.train.get_checkpoint_state(ckpt_dir).model_checkpoint_path\n",
    "latest_ckpt  = latest_ckpt.split(\"char_level_cnn/\")[1]\n",
    "latest_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 373\n",
    "vocab_size = 53640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "acc:  0.9405709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg     0.8465    0.6404    0.7292       267\n",
      "        pos     0.9504    0.9834    0.9666      1870\n",
      "\n",
      "avg / total     0.9374    0.9406    0.9370      2137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a,s = eval_acc(x_test, y_test, max_length=max_length)\n",
    "print(\"acc: \", a)\n",
    "print(classification_report([v.argmax() for v in y_test], [v.argmax() for v in s], digits=4 ,target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drop_doc(positive_data_file, negative_data_file, n):\n",
    "       \n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    po = []\n",
    "    ne = []\n",
    "    t = Tokenizer()\n",
    "    for e in positive_examples:\n",
    "        if len(e) < n:\n",
    "            e_ = \"\"\n",
    "            for _ in  range(len(e)):\n",
    "                e_ += chr(randint(12354, 20000)) + \" \"\n",
    "            e = e_\n",
    "        else:\n",
    "            max_n = n\n",
    "            for i in choice(range(len(e)), max_n, replace=False):\n",
    "                e = e.replace(e[i], chr(randint(12354, 20000)))\n",
    "        po.append(t.tokenize(e))\n",
    "        \n",
    "    for e in negative_examples:\n",
    "        if len(e) < n:\n",
    "            e = \"{} \".format(chr(randint(12354, 20000))) * len(e)\n",
    "        else:\n",
    "            max_n = n\n",
    "            for i in choice(range(len(e)), max_n, replace=False):\n",
    "                e = e.replace(e[i], chr(randint(12354, 20000)))\n",
    "        ne.append(t.tokenize(e))\n",
    "\n",
    "   \n",
    "    positive_examples = po\n",
    "    negative_examples = ne\n",
    "        \n",
    "    x_text = positive_examples + negative_examples\n",
    "\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    #return po, y\n",
    "    return x_text, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = create_drop_doc(positive_data_file, negative_data_file, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'䄏 ㇖ 㕲 ㇗ 䯿 䯄 䧝 䭻 䡕 䞑 䍦 㥹 㔆 ㌲ 䊂 䑹 ㋃ 㿋 䮈 䤭 䝞 㢊 㢂 ㄟ 䴿 㜍 䆸 㷐 䵫 䱅 䉄 䈀 䛮'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[shuffle_indices][test_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "acc:  0.9405709\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg     0.8465    0.6404    0.7292       267\n",
      "        pos     0.9504    0.9834    0.9666      1870\n",
      "\n",
      "avg / total     0.9374    0.9406    0.9370      2137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a,s = eval_acc(x_test, y_test, max_length=max_length)\n",
    "print(\"acc: \", a)\n",
    "print(classification_report([v.argmax() for v in y_test], [v.argmax() for v in s], digits=4 ,target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(x, y_test, max_length=max_length)\n",
    "print(\"acc: \", a)\n",
    "print(classification_report([v.argmax() for v in y_test], [v.argmax() for v in s], digits=4 ,target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_change_leng = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e115b3b3b28480d980c8887c7bd495f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [2137] vs. [71266]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fc-3/predictions, accuracy/ArgMax)]]\n\nCaused by op 'accuracy/Equal', defined at:\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-52-990e0a9714ea>\", line 12, in <module>\n    l2_reg_lambda=l2_reg_lambda)\n  File \"<ipython-input-5-a205570fab05>\", line 82, in __init__\n    correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2529, in equal\n    \"Equal\", x=x, y=y, name=name)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [2137] vs. [71266]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fc-3/predictions, accuracy/ArgMax)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [2137] vs. [71266]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fc-3/predictions, accuracy/ArgMax)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-990e0a9714ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m               \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             }\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [2137] vs. [71266]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fc-3/predictions, accuracy/ArgMax)]]\n\nCaused by op 'accuracy/Equal', defined at:\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-52-990e0a9714ea>\", line 12, in <module>\n    l2_reg_lambda=l2_reg_lambda)\n  File \"<ipython-input-5-a205570fab05>\", line 82, in __init__\n    correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2529, in equal\n    \"Equal\", x=x, y=y, name=name)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [2137] vs. [71266]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fc-3/predictions, accuracy/ArgMax)]]\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "for i in tqdm(range(max_change_leng)):\n",
    "    x_, y_ = create_drop_doc(positive_data_file, negative_data_file, i)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_)))\n",
    "    x = x[shuffle_indices][test_sample_index:]\n",
    "    a,s = eval_acc(x, y_test,max_length=max_length)\n",
    "    print(a)\n",
    "    r.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857a3727ac72466abbecadd163ede98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.94337857\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.9405709\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.931212\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.9279364\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.92185307\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.91576976\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.9110903\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.905007\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.90407115\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.9017314\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.90407115\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8975199\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8937763\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.89518017\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8937763\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8886289\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.89190453\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.88909686\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8806738\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8811418\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.88394946\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.87880206\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.87786615\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8741226\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8745906\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.86523163\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8755264\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8689752\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.86663544\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.870847\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8699111\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8628919\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.86335987\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.86523163\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8642957\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8591483\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8596163\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.86242396\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.85212916\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8568086\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8525971\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8479176\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n",
      "0.8479176\n",
      "# classes 2\n",
      "INFO:tensorflow:Restoring parameters from runs/word_cnn_2/2018_07_25_14_46_48/checkpoints/model-2040\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        cnn = TextCNN(\n",
    "                sequence_length=max_length,\n",
    "                num_classes=2,\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_size=embedding_size,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, latest_ckpt)\n",
    "        for i in tqdm(range(max_change_leng)):\n",
    "            x_, y_ = create_drop_doc(positive_data_file, negative_data_file, i)\n",
    "            x = np.array(list(vocab_processor.fit_transform(x_)))\n",
    "            x = x[shuffle_indices][test_sample_index:]\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x,\n",
    "              cnn.input_y: y_test,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            acc, scores = sess.run([cnn.accuracy, cnn.scores], feed_dict=feed_dict)\n",
    "            a,s = eval_acc(x, y_test,max_length=max_length)\n",
    "            print(a)\n",
    "            r.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(max_change_leng), r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = eval_acc(drop_char(), y_test, max_length=max_length)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir=\"runs/1523936751\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(out_dir, \"vocab\"))\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_size,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "        saver = tf.train.Saver()\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, latest_ckpt)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        w = sess.run(cnn.W, feed_dict={cnn.input_x: x_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_tensor.tsv','w') as f:\n",
    "    for char_vec in w:\n",
    "        for weight in char_vec:\n",
    "            f.write(str(weight)+ \"\\t\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "vocab_dict = vocab_processor.vocabulary_._reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open('embedding_metadata.tsv' ,'w') as f:\n",
    "    f.write('Titles\\tGenres\\n')\n",
    "    for i,v in enumerate(w):\n",
    "        f.write(\"%s\\t%s\\n\" % (vocab_dict[i], vocab_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(x):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_dim,\n",
    "                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "\n",
    "            feed_dict = {\n",
    "                  cnn.input_x: x,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "            feature_5, feature_2 = sess.run([cnn.f_h, cnn.scores ], feed_dict=feed_dict)\n",
    "    return feature_5, feature_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = list(open(\"data/amazon/rating_5.txt\", \"r\").readlines())\n",
    "review = [s.strip() for s in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []\n",
    "for r in review:\n",
    "    l = r.split(\":::::\")\n",
    "    y.append(float(l[0]))\n",
    "    x.append(l[1].replace(\" \", \"\").replace(\"\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(\"runs/1525408577\", \"vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x)))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5 ,feature_2 = get_feature(x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "chunk_size = 100\n",
    "for i in range(0, len(x) , chunk_size):\n",
    "    feature_5 ,feature_2 = get_feature(x[i:i+chunk_size])\n",
    "    for f, r in zip(feature_5, y[i:i+chunk_size]):\n",
    "        s  += int(np.argmax(f) == r)\n",
    "    print(s/(i+chunk_size))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(feature_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2 [0]  #[neg, pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36)",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
