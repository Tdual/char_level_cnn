{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ShopRunner/jupyter-notify\n",
    "```\n",
    "pip install jupyternotify\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from tensorflow.contrib import learn\n",
    "import pickle\n",
    "import pandas as pd\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded,W,strides=[1, 1, 1, 1],padding=\"VALID\", name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID',name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        print(num_filters_total)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "                            # add@@@@@@@@\n",
    "            features_size = 5\n",
    "            self.features = tf.Variable(tf.truncated_normal([num_filters_total, features_size], stddev=0.1))\n",
    "            # 384 * 5\n",
    "            self.f_h = tf.matmul(self.h_drop, self.features, )\n",
    "              # (None * 383) *(384 * 5) \n",
    "            print(self.h_drop) #None * 384 \n",
    "            print(self.f_h) # Nonw * 5\n",
    "\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\"W\", shape=[features_size, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            #W = tf.get_variable(\"W\", shape=[num_filters_total, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "\n",
    "            \n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            #self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.f_h, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "dev_sample_percentage = 0.001\n",
    "positive_data_file = \"data/amazon/book_pos.txt\"#\"./data/rt-polarity.pos\"\n",
    "negative_data_file = \"data/amazon/book_neg.txt\"#\"./data/rt-polarity.neg\" #\"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 128 #, \"Dimensionality of character embedding (default: 128)\")\n",
    "filter_sizes = \"3,4,5\"#, \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "num_filters=128#, \"Number of filters per filter size (default: 128)\")\n",
    "dropout_keep_prob=0.5#, \"Dropout keep probability (default: 0.5)\")\n",
    "l2_reg_lambda=0.0#, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size=64#, \"Batch Size (default: 64)\")\n",
    "num_epochs=200#, \"Number of training epochs (default: 200)\")\n",
    "evaluate_every=100#, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "num_checkpoints=5#, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "allow_soft_placement=True#, \"Allow device soft device placement\")\n",
    "log_device_placement=False#, \"Log placement of ops on devices\")\n",
    "\n",
    "#FLAGS = tf.flags.FLAGS\n",
    "#FLAGS._parse_flags()\n",
    "#print(\"\\nParameters:\")\n",
    "#for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#    print(\"{}={}\".format(attr.upper(), value))\n",
    "#print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-533795485ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_data_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Workspace/Python/char_level_cnn/data_helpers.py\u001b[0m in \u001b[0;36mload_data_and_labels\u001b[0;34m(positive_data_file, negative_data_file)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Split by words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_examples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mx_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Generate labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpositive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/Python/char_level_cnn/data_helpers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Split by words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_examples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mx_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Generate labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpositive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/Python/char_level_cnn/data_helpers.py\u001b[0m in \u001b[0;36mclean_str\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mOriginal\u001b[0m \u001b[0mtaken\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0myoonkim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mCNN_sentence\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^A-Za-z0-9(),!?\\'\\`]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\'s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \\'s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\'ve\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \\'ve\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"b4e78b6a-6f77-4cd5-ba3c-81f544fcaf04\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"b4e78b6a-6f77-4cd5-ba3c-81f544fcaf04\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love the characters love the location ! i cannot wait for the next book in the series i will certainly read it'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"787a7423-2491-45d3-b6f0-144a6695a94d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"787a7423-2491-45d3-b6f0-144a6695a94d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 203998\n",
      "Train/Dev split: 999000/1000\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"acb7b370-e99d-48f8-bbec-01729d105331\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"acb7b370-e99d-48f8-bbec-01729d105331\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99900\n",
      "0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f3ab406a0bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/x_train_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "chunk_size =  int(len(x_train)/10)\n",
    "print(chunk_size)\n",
    "for i in range(0, len(x_train), chunck_size):\n",
    "    print(i)\n",
    "    end = i+chunck_size\n",
    "    if end < len(x_train):\n",
    "        chunk = x_train[i:  end]\n",
    "    else:\n",
    "        chunk = x_train[i:]\n",
    "        end = len(x_train)\n",
    "    with open(\"data/x_train_{}.pkl\".format(end), \"wb\") as f:\n",
    "        pickle.dump(chunk, f, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "99900\n",
      "199800\n",
      "299700\n",
      "399600\n",
      "499500\n",
      "599400\n",
      "699300\n",
      "799200\n",
      "899100\n"
     ]
    }
   ],
   "source": [
    "all_size = 999000\n",
    "chunk_size = 99900\n",
    "x_train =[]\n",
    "for i in range(0, all_size, chunck_size):\n",
    "    print(i)\n",
    "    end = i+chunk_size\n",
    "    if end > len(reviews):\n",
    "        end = all_size\n",
    "    with open(\"data/x_train_{}.pkl\".format(end), \"rb\") as f:\n",
    "        x_train += pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/y_train.pkl\", \"wb\") as f:\n",
    "        pickle.dump(y_train, f, protocol = 4)\n",
    "with open(\"data/x_dev.pkl\", \"wb\") as f:\n",
    "        pickle.dump(x_dev, f, protocol = 4)\n",
    "with open(\"data/y_dev.pkl\", \"wb\") as f:\n",
    "        pickle.dump(x_dev, f, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "Tensor(\"dropout/dropout/mul:0\", shape=(?, 384), dtype=float32)\n",
      "Tensor(\"dropout/MatMul:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "with sess.as_default():\n",
    "        \n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "Tensor(\"dropout/dropout/mul:0\", shape=(?, 384), dtype=float32)\n",
      "Tensor(\"dropout/MatMul:0\", shape=(?, 5), dtype=float32)\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dropout/Variable:0/grad/hist is illegal; using dropout/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dropout/Variable:0/grad/sparsity is illegal; using dropout/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/tamoto.yoshifumi/Workspace/Python/char_level_cnn/runs/1523608254\n",
      "\n",
      "2018-04-13T17:31:13.484266: step 1, loss 5.62956, acc 0.46875\n",
      "2018-04-13T17:31:24.406824: step 2, loss 4.17347, acc 0.453125\n",
      "2018-04-13T17:31:34.833523: step 3, loss 3.60351, acc 0.390625\n",
      "2018-04-13T17:31:45.178271: step 4, loss 3.3713, acc 0.453125\n",
      "2018-04-13T17:31:55.647836: step 5, loss 2.23772, acc 0.625\n",
      "2018-04-13T17:32:05.824302: step 6, loss 3.50954, acc 0.46875\n",
      "2018-04-13T17:32:15.788386: step 7, loss 3.34221, acc 0.484375\n",
      "2018-04-13T17:32:26.278673: step 8, loss 2.98432, acc 0.5\n",
      "2018-04-13T17:32:36.204588: step 9, loss 3.76438, acc 0.484375\n",
      "2018-04-13T17:32:45.615785: step 10, loss 3.63778, acc 0.515625\n",
      "2018-04-13T17:32:55.133767: step 11, loss 3.35901, acc 0.453125\n",
      "2018-04-13T17:33:04.648039: step 12, loss 2.38077, acc 0.5625\n",
      "2018-04-13T17:33:14.166477: step 13, loss 2.1176, acc 0.546875\n",
      "2018-04-13T17:33:23.687213: step 14, loss 2.17512, acc 0.546875\n",
      "2018-04-13T17:33:33.973854: step 15, loss 2.08281, acc 0.546875\n",
      "2018-04-13T17:33:45.231499: step 16, loss 2.68919, acc 0.421875\n",
      "2018-04-13T17:33:55.010682: step 17, loss 2.5813, acc 0.53125\n",
      "2018-04-13T17:34:04.664014: step 18, loss 2.92964, acc 0.484375\n",
      "2018-04-13T17:34:14.297640: step 19, loss 2.30256, acc 0.578125\n",
      "2018-04-13T17:34:23.884470: step 20, loss 1.92763, acc 0.546875\n",
      "2018-04-13T17:34:33.373091: step 21, loss 3.04179, acc 0.40625\n",
      "2018-04-13T17:34:43.481443: step 22, loss 3.08378, acc 0.484375\n",
      "2018-04-13T17:34:53.136154: step 23, loss 1.90715, acc 0.546875\n",
      "2018-04-13T17:35:03.397053: step 24, loss 2.06354, acc 0.5625\n",
      "2018-04-13T17:35:13.872107: step 25, loss 2.43122, acc 0.5625\n",
      "2018-04-13T17:35:25.305337: step 26, loss 2.83161, acc 0.46875\n",
      "2018-04-13T17:35:35.057857: step 27, loss 2.1507, acc 0.5\n",
      "2018-04-13T17:35:45.543034: step 28, loss 2.15862, acc 0.59375\n",
      "2018-04-13T17:35:55.861267: step 29, loss 1.97446, acc 0.5\n",
      "2018-04-13T17:36:06.216811: step 30, loss 2.08324, acc 0.546875\n",
      "2018-04-13T17:36:16.484478: step 31, loss 1.90772, acc 0.625\n",
      "2018-04-13T17:36:26.671985: step 32, loss 1.44994, acc 0.65625\n",
      "2018-04-13T17:36:36.927619: step 33, loss 1.8531, acc 0.625\n",
      "2018-04-13T17:36:48.029141: step 34, loss 1.62689, acc 0.59375\n",
      "2018-04-13T17:36:57.964729: step 35, loss 1.92814, acc 0.546875\n",
      "2018-04-13T17:37:07.878573: step 36, loss 2.16512, acc 0.515625\n",
      "2018-04-13T17:37:17.721487: step 37, loss 2.65857, acc 0.484375\n",
      "2018-04-13T17:37:27.371112: step 38, loss 1.65389, acc 0.640625\n",
      "2018-04-13T17:37:37.207625: step 39, loss 2.11187, acc 0.5\n",
      "2018-04-13T17:37:47.312025: step 40, loss 2.32971, acc 0.484375\n",
      "2018-04-13T17:37:57.348898: step 41, loss 1.68607, acc 0.640625\n",
      "2018-04-13T17:38:06.857460: step 42, loss 2.06484, acc 0.5625\n",
      "2018-04-13T17:38:16.445817: step 43, loss 2.38457, acc 0.46875\n",
      "2018-04-13T17:38:28.020983: step 44, loss 1.4953, acc 0.578125\n",
      "2018-04-13T17:38:38.705091: step 45, loss 1.60411, acc 0.546875\n",
      "2018-04-13T17:38:48.700458: step 46, loss 1.56705, acc 0.53125\n",
      "2018-04-13T17:38:58.915768: step 47, loss 1.70015, acc 0.609375\n",
      "2018-04-13T17:39:09.428626: step 48, loss 1.72984, acc 0.609375\n",
      "2018-04-13T17:39:19.736510: step 49, loss 1.28803, acc 0.609375\n",
      "2018-04-13T17:39:30.116873: step 50, loss 1.57581, acc 0.578125\n",
      "2018-04-13T17:39:40.309825: step 51, loss 1.26681, acc 0.65625\n",
      "2018-04-13T17:39:50.611507: step 52, loss 1.63026, acc 0.53125\n",
      "2018-04-13T17:40:00.589413: step 53, loss 1.48914, acc 0.640625\n",
      "2018-04-13T17:40:10.704438: step 54, loss 2.37185, acc 0.5\n",
      "2018-04-13T17:40:20.493178: step 55, loss 2.03824, acc 0.46875\n",
      "2018-04-13T17:40:30.195435: step 56, loss 1.36683, acc 0.59375\n",
      "2018-04-13T17:40:40.267895: step 57, loss 1.47648, acc 0.640625\n",
      "2018-04-13T17:40:50.528334: step 58, loss 2.64234, acc 0.546875\n",
      "2018-04-13T17:41:01.315359: step 59, loss 1.93027, acc 0.5\n",
      "2018-04-13T17:41:11.695834: step 60, loss 1.75172, acc 0.5625\n",
      "2018-04-13T17:41:22.738292: step 61, loss 1.38576, acc 0.65625\n",
      "2018-04-13T17:41:34.304686: step 62, loss 2.24125, acc 0.546875\n",
      "2018-04-13T17:41:45.199658: step 63, loss 1.40293, acc 0.5625\n",
      "2018-04-13T17:41:55.138960: step 64, loss 1.95669, acc 0.546875\n",
      "2018-04-13T17:42:06.118064: step 65, loss 1.5038, acc 0.6875\n",
      "2018-04-13T17:42:15.971555: step 66, loss 1.77029, acc 0.53125\n",
      "2018-04-13T17:42:25.947462: step 67, loss 1.55557, acc 0.5625\n",
      "2018-04-13T17:42:36.776605: step 68, loss 1.53111, acc 0.59375\n",
      "2018-04-13T17:42:47.213622: step 69, loss 1.82103, acc 0.59375\n",
      "2018-04-13T17:42:58.149592: step 70, loss 1.58566, acc 0.59375\n",
      "2018-04-13T17:43:09.444635: step 71, loss 2.00684, acc 0.46875\n",
      "2018-04-13T17:43:19.725808: step 72, loss 1.56752, acc 0.640625\n",
      "2018-04-13T17:43:30.208334: step 73, loss 1.60418, acc 0.609375\n",
      "2018-04-13T17:43:40.118957: step 74, loss 1.84841, acc 0.59375\n",
      "2018-04-13T17:43:50.999780: step 75, loss 1.46171, acc 0.53125\n",
      "2018-04-13T17:44:01.714957: step 76, loss 1.55515, acc 0.625\n",
      "2018-04-13T17:44:12.027132: step 77, loss 1.86596, acc 0.59375\n",
      "2018-04-13T17:44:22.674030: step 78, loss 1.46698, acc 0.609375\n",
      "2018-04-13T17:44:32.714876: step 79, loss 1.8795, acc 0.609375\n",
      "2018-04-13T17:44:42.692920: step 80, loss 1.39786, acc 0.578125\n",
      "2018-04-13T17:44:52.586155: step 81, loss 1.32744, acc 0.640625\n",
      "2018-04-13T17:45:02.358195: step 82, loss 1.70877, acc 0.625\n",
      "2018-04-13T17:45:12.884518: step 83, loss 1.24335, acc 0.671875\n",
      "2018-04-13T17:45:23.879132: step 84, loss 1.76558, acc 0.578125\n",
      "2018-04-13T17:45:34.341291: step 85, loss 1.63454, acc 0.6875\n",
      "2018-04-13T17:45:43.903882: step 86, loss 1.46073, acc 0.609375\n",
      "2018-04-13T17:45:54.272618: step 87, loss 1.39993, acc 0.625\n",
      "2018-04-13T17:46:04.645724: step 88, loss 1.99662, acc 0.578125\n",
      "2018-04-13T17:46:14.190489: step 89, loss 1.57785, acc 0.625\n",
      "2018-04-13T17:46:24.097089: step 90, loss 1.44153, acc 0.640625\n",
      "2018-04-13T17:46:33.694042: step 91, loss 1.66975, acc 0.609375\n",
      "2018-04-13T17:46:44.031436: step 92, loss 1.01915, acc 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T17:46:54.503401: step 93, loss 1.54335, acc 0.546875\n",
      "2018-04-13T17:47:05.170367: step 94, loss 0.864733, acc 0.671875\n",
      "2018-04-13T17:47:15.346336: step 95, loss 1.92825, acc 0.46875\n",
      "2018-04-13T17:47:26.632422: step 96, loss 1.21186, acc 0.640625\n",
      "2018-04-13T17:47:37.104371: step 97, loss 1.16195, acc 0.703125\n",
      "2018-04-13T17:47:47.626065: step 98, loss 1.5543, acc 0.59375\n",
      "2018-04-13T17:47:58.326876: step 99, loss 1.90125, acc 0.53125\n",
      "2018-04-13T17:48:09.051032: step 100, loss 1.35579, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T17:56:46.564649: step 100, loss 0.451258, acc 0.774\n",
      "\n",
      "2018-04-13T17:56:57.670711: step 101, loss 1.51227, acc 0.578125\n",
      "2018-04-13T17:57:08.404039: step 102, loss 1.59222, acc 0.625\n",
      "2018-04-13T17:57:19.020346: step 103, loss 1.56445, acc 0.53125\n",
      "2018-04-13T17:57:29.867039: step 104, loss 0.780058, acc 0.78125\n",
      "2018-04-13T17:57:40.757858: step 105, loss 1.67472, acc 0.53125\n",
      "2018-04-13T17:57:52.043071: step 106, loss 0.911877, acc 0.65625\n",
      "2018-04-13T17:58:02.591407: step 107, loss 0.804395, acc 0.703125\n",
      "2018-04-13T17:58:12.977520: step 108, loss 1.25075, acc 0.609375\n",
      "2018-04-13T17:58:23.196839: step 109, loss 1.50326, acc 0.640625\n",
      "2018-04-13T17:58:32.989347: step 110, loss 1.34009, acc 0.703125\n",
      "2018-04-13T17:58:42.744825: step 111, loss 1.33538, acc 0.65625\n",
      "2018-04-13T17:58:52.331073: step 112, loss 1.37543, acc 0.578125\n",
      "2018-04-13T17:59:01.845045: step 113, loss 1.46684, acc 0.609375\n",
      "2018-04-13T17:59:11.511643: step 114, loss 1.37375, acc 0.609375\n",
      "2018-04-13T17:59:21.610389: step 115, loss 1.15585, acc 0.671875\n",
      "2018-04-13T17:59:31.475518: step 116, loss 1.40976, acc 0.640625\n",
      "2018-04-13T17:59:41.035999: step 117, loss 1.03848, acc 0.609375\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "        batches = data_helpers.batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            #if current_step % checkpoint_every == 0:\n",
    "            #    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            #    print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36)",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
