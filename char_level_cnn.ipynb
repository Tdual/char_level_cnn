{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset  \n",
    "http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ShopRunner/jupyter-notify\n",
    "```\n",
    "pip install jupyternotify\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib.learn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    " \n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded,W,strides=[1, 1, 1, 1],padding=\"VALID\", name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID',name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        print(num_filters_total)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "\n",
    "        with tf.name_scope(\"fc-1\"):\n",
    "            size = 1024\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, size], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[size]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            fc_1_output = tf.nn.relu(tf.nn.xw_plus_b(self.h_pool_flat, W, b), name=\"fc-1-out\")\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"dropout-1\"):\n",
    "            drop_1 = tf.nn.dropout(fc_1_output, self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"fc-2\"):\n",
    "            W = tf.Variable(tf.truncated_normal([size, size], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[size]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            fc_2_output = tf.nn.relu(tf.nn.xw_plus_b(drop_1, W, b), name=\"fc-2-out\")\n",
    "            \n",
    "        with tf.name_scope(\"dropout-2\"):\n",
    "            drop_2 = tf.nn.dropout(fc_2_output, self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"fc-3\"):\n",
    "            W = tf.Variable(tf.truncated_normal([size, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(drop_2, W, b, name=\"output\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "dev_sample_percentage = 0.0010\n",
    "\n",
    "positive_data_file = \"data/amazon/book_pos.txt\"#\"./data/rt-polarity.pos\"\n",
    "negative_data_file = \"data/amazon/book_neg.txt\"#\"./data/rt-polarity.neg\" #\"Data source for the negative data.\")\n",
    "#positive_data_file = \"data/chABSA/pos.txt\"\n",
    "#negative_data_file = \"data/chABSA/neg.txt\"\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 32     #, \"Dimensionality of character embedding (default: 128)\")\n",
    "filter_sizes = \"2,3,4,5\"        #, \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "num_filters=128               #, \"Number of filters per filter size (default: 128)\")\n",
    "dropout_keep_prob=0.5 #, \"Dropout keep probability (default: 0.5)\")\n",
    "l2_reg_lambda=0.0          #, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size=64                    #, \"Batch Size (default: 64)\")\n",
    "num_epochs=200              #, \"Number of training epochs (default: 200)\")\n",
    "evaluate_every=100         #, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "num_checkpoints=5          #, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement=True    #, \"Allow device soft device placement\")\n",
    "log_device_placement=False  #, \"Log placement of ops on devices\")\n",
    "\n",
    "#FLAGS = tf.flags.FLAGS\n",
    "#FLAGS._parse_flags()\n",
    "#print(\"\\nParameters:\")\n",
    "#for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#    print(\"{}={}\".format(attr.upper(), value))\n",
    "#print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file, level=\"char\"):\n",
    "       \n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    if level == \"char\":\n",
    "        positive_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in positive_examples]\n",
    "        negative_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in negative_examples]\n",
    "    elif level == \"word\":\n",
    "        positive_examples = [s.strip() for s in positive_examples]\n",
    "        negative_examples = [s.strip() for s in negative_examples]\n",
    "    else:\n",
    "        print(\"invaid value of 'level'. ('char' or 'word') \")\n",
    "        \n",
    "    x_text = positive_examples + negative_examples\n",
    "\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    return x_text, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"6812d283-0de7-4dd3-a3b1-822b3222d1a7\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"6812d283-0de7-4dd3-a3b1-822b3222d1a7\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x_text, y = load_data_and_labels(positive_data_file, negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' l o v e t h e c h a r a c t e r s . l o v e t h e l o c a t i o n ! i c a n n o t w a i t f o r t h e n e x t b o o k i n t h e s e r i e s . i w i l l c e r t a i n l y r e a d i t . \\n '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = np.array([len(r)for r in x_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length\n",
       "0     187\n",
       "1     399\n",
       "2     621\n",
       "3     215\n",
       "4     225"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>842.351232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1119.140102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>931.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48939.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               length\n",
       "count  1000000.000000\n",
       "mean       842.351232\n",
       "std       1119.140102\n",
       "min          3.000000\n",
       "25%        255.000000\n",
       "50%        447.000000\n",
       "75%        931.000000\n",
       "max      48939.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c282870f0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2dd9f5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = [x[:2000] if len(x) > 2000 else x for x in x_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>694.104355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>584.037875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>931.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               length\n",
       "count  1000000.000000\n",
       "mean       694.104355\n",
       "std        584.037875\n",
       "min          3.000000\n",
       "25%        255.000000\n",
       "50%        447.000000\n",
       "75%        931.000000\n",
       "max       2000.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_list = np.array([len(r)for r in x_text])\n",
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c304e7ef0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c28254cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"Ja\" #\"En\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max document length 2000\n"
     ]
    }
   ],
   "source": [
    "if lang == \"En\":\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "elif lang == \"Ja\":\n",
    "    max_document_length = max([len(x) for x in x_text])\n",
    "print(\"max document length\", max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-ef6c3fae6839>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tdual/anaconda2/envs/py3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tdual/anaconda2/envs/py3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"f752827c-db9f-4538-80eb-357d33dc0b7f\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"f752827c-db9f-4538-80eb-357d33dc0b7f\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 40\n",
      "Train/Dev split: 999000/1000\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"757ea960-5d1a-40e4-a3cb-04022a78799e\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"757ea960-5d1a-40e4-a3cb-04022a78799e\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999000, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle():\n",
    "    chunk_size =  int(len(x_train)/10)\n",
    "    print(chunk_size)\n",
    "    for i in range(0, len(x_train), chunck_size):\n",
    "        print(i)\n",
    "        end = i+chunck_size\n",
    "        if end < len(x_train):\n",
    "            chunk = x_train[i:  end]\n",
    "        else:\n",
    "            chunk = x_train[i:]\n",
    "            end = len(x_train)\n",
    "        with open(\"data/x_train_{}.pkl\".format(end), \"wb\") as f:\n",
    "            pickle.dump(chunk, f, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x_train():\n",
    "    all_size = 999000\n",
    "    chunk_size = 99900\n",
    "    x_train =[]\n",
    "    for i in range(0, all_size, chunck_size):\n",
    "        print(i)\n",
    "        end = i+chunk_size\n",
    "        if end > len(reviews):\n",
    "            end = all_size\n",
    "        with open(\"data/x_train_{}.pkl\".format(end), \"rb\") as f:\n",
    "            x_train += pickle.load(f)\n",
    "        return x_train\n",
    "#x_train = load_x_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "embedding_size = embedding_dim\n",
    "filter_sizes = [2,3,4,5]\n",
    "num_filters = num_filters\n",
    "l2_reg_lambda = l2_reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TextCNN at 0x1c398646d8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/hist is illegal; using fc-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/W:0/grad/sparsity is illegal; using fc-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/hist is illegal; using fc-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-1/b:0/grad/sparsity is illegal; using fc-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/hist is illegal; using fc-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/W:0/grad/sparsity is illegal; using fc-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/hist is illegal; using fc-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-2/b:0/grad/sparsity is illegal; using fc-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/hist is illegal; using fc-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/W:0/grad/sparsity is illegal; using fc-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/hist is illegal; using fc-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fc-3/b:0/grad/sparsity is illegal; using fc-3/b_0/grad/sparsity instead.\n",
      "Writing to /Users/tdual/Workspace/char_level_cnn/runs/1525609926/+3fcs\n",
      "\n",
      "2018-05-07T00:41:37.278086: step 1, loss 13.4497, acc 0.5\n",
      "2018-05-07T00:41:38.606820: step 2, loss 30.9108, acc 0.515625\n",
      "2018-05-07T00:41:39.724703: step 3, loss 12.0458, acc 0.5625\n",
      "2018-05-07T00:41:40.750114: step 4, loss 22.0784, acc 0.484375\n",
      "2018-05-07T00:41:41.776150: step 5, loss 11.6287, acc 0.609375\n",
      "2018-05-07T00:41:42.871008: step 6, loss 9.88043, acc 0.5625\n",
      "2018-05-07T00:41:43.935204: step 7, loss 13.9979, acc 0.5\n",
      "2018-05-07T00:41:44.935527: step 8, loss 14.6122, acc 0.484375\n",
      "2018-05-07T00:41:45.945695: step 9, loss 7.32111, acc 0.5\n",
      "2018-05-07T00:41:46.968094: step 10, loss 7.38713, acc 0.53125\n",
      "2018-05-07T00:41:47.975128: step 11, loss 11.0117, acc 0.46875\n",
      "2018-05-07T00:41:49.195890: step 12, loss 8.49258, acc 0.53125\n",
      "2018-05-07T00:41:50.245516: step 13, loss 5.74491, acc 0.625\n",
      "2018-05-07T00:41:51.326671: step 14, loss 7.85786, acc 0.5\n",
      "2018-05-07T00:41:52.357869: step 15, loss 11.1147, acc 0.53125\n",
      "2018-05-07T00:41:53.519927: step 16, loss 10.3832, acc 0.515625\n",
      "2018-05-07T00:41:54.555334: step 17, loss 10.8276, acc 0.46875\n",
      "2018-05-07T00:41:55.528286: step 18, loss 6.37308, acc 0.5\n",
      "2018-05-07T00:41:56.541983: step 19, loss 6.79822, acc 0.4375\n",
      "2018-05-07T00:41:57.784728: step 20, loss 5.12682, acc 0.609375\n",
      "2018-05-07T00:41:58.805378: step 21, loss 5.80403, acc 0.640625\n",
      "2018-05-07T00:42:00.054092: step 22, loss 5.69993, acc 0.484375\n",
      "2018-05-07T00:42:01.151384: step 23, loss 6.32319, acc 0.390625\n",
      "2018-05-07T00:42:02.457101: step 24, loss 6.08229, acc 0.390625\n",
      "2018-05-07T00:42:03.528435: step 25, loss 7.77894, acc 0.5\n",
      "2018-05-07T00:42:04.592845: step 26, loss 6.87988, acc 0.421875\n",
      "2018-05-07T00:42:05.640461: step 27, loss 7.16065, acc 0.5\n",
      "2018-05-07T00:42:06.621225: step 28, loss 3.18669, acc 0.53125\n",
      "2018-05-07T00:42:07.649460: step 29, loss 4.96388, acc 0.40625\n",
      "2018-05-07T00:42:08.664539: step 30, loss 7.22734, acc 0.359375\n",
      "2018-05-07T00:42:09.881236: step 31, loss 4.99098, acc 0.390625\n",
      "2018-05-07T00:42:10.923713: step 32, loss 3.38238, acc 0.578125\n",
      "2018-05-07T00:42:12.127262: step 33, loss 3.13906, acc 0.5\n",
      "2018-05-07T00:42:13.158331: step 34, loss 4.18316, acc 0.453125\n",
      "2018-05-07T00:42:14.268611: step 35, loss 2.91658, acc 0.5625\n",
      "2018-05-07T00:42:15.317563: step 36, loss 4.1623, acc 0.484375\n",
      "2018-05-07T00:42:16.312141: step 37, loss 4.15894, acc 0.453125\n",
      "2018-05-07T00:42:17.297144: step 38, loss 4.30553, acc 0.453125\n",
      "2018-05-07T00:42:18.265857: step 39, loss 2.49947, acc 0.5625\n",
      "2018-05-07T00:42:19.374695: step 40, loss 2.63619, acc 0.5\n",
      "2018-05-07T00:42:20.391581: step 41, loss 3.14054, acc 0.453125\n",
      "2018-05-07T00:42:21.440989: step 42, loss 3.12565, acc 0.421875\n",
      "2018-05-07T00:42:22.471237: step 43, loss 3.36304, acc 0.484375\n",
      "2018-05-07T00:42:23.746461: step 44, loss 2.7626, acc 0.59375\n",
      "2018-05-07T00:42:24.798160: step 45, loss 2.81055, acc 0.5\n",
      "2018-05-07T00:42:25.794872: step 46, loss 2.59392, acc 0.40625\n",
      "2018-05-07T00:42:26.953705: step 47, loss 2.70611, acc 0.40625\n",
      "2018-05-07T00:42:28.023651: step 48, loss 2.47026, acc 0.46875\n",
      "2018-05-07T00:42:29.085957: step 49, loss 1.63725, acc 0.453125\n",
      "2018-05-07T00:42:30.200499: step 50, loss 1.82557, acc 0.578125\n",
      "2018-05-07T00:42:31.301510: step 51, loss 2.161, acc 0.5\n",
      "2018-05-07T00:42:32.360737: step 52, loss 1.60168, acc 0.59375\n",
      "2018-05-07T00:42:33.394587: step 53, loss 1.63225, acc 0.5625\n",
      "2018-05-07T00:42:34.435156: step 54, loss 1.64108, acc 0.5625\n",
      "2018-05-07T00:42:35.456746: step 55, loss 1.90848, acc 0.53125\n",
      "2018-05-07T00:42:36.466248: step 56, loss 1.56402, acc 0.640625\n",
      "2018-05-07T00:42:37.527289: step 57, loss 2.18717, acc 0.5625\n",
      "2018-05-07T00:42:38.567876: step 58, loss 1.53595, acc 0.5625\n",
      "2018-05-07T00:42:39.569345: step 59, loss 2.54742, acc 0.546875\n",
      "2018-05-07T00:42:40.698109: step 60, loss 1.71775, acc 0.546875\n",
      "2018-05-07T00:42:42.006055: step 61, loss 1.67154, acc 0.5\n",
      "2018-05-07T00:42:43.313470: step 62, loss 1.79761, acc 0.453125\n",
      "2018-05-07T00:42:44.368043: step 63, loss 1.92336, acc 0.5\n",
      "2018-05-07T00:42:45.452438: step 64, loss 1.82752, acc 0.453125\n",
      "2018-05-07T00:42:46.465507: step 65, loss 1.47225, acc 0.5\n",
      "2018-05-07T00:42:47.516662: step 66, loss 1.22899, acc 0.578125\n",
      "2018-05-07T00:42:48.569027: step 67, loss 1.35317, acc 0.609375\n",
      "2018-05-07T00:42:49.618441: step 68, loss 1.1351, acc 0.5625\n",
      "2018-05-07T00:42:50.670210: step 69, loss 0.987899, acc 0.625\n",
      "2018-05-07T00:42:51.657295: step 70, loss 1.27168, acc 0.5625\n",
      "2018-05-07T00:42:52.704185: step 71, loss 1.11521, acc 0.5\n",
      "2018-05-07T00:42:53.782445: step 72, loss 1.52704, acc 0.5\n",
      "2018-05-07T00:42:54.886469: step 73, loss 1.24881, acc 0.484375\n",
      "2018-05-07T00:42:55.892248: step 74, loss 1.45383, acc 0.453125\n",
      "2018-05-07T00:42:56.950356: step 75, loss 1.39579, acc 0.484375\n",
      "2018-05-07T00:42:58.033358: step 76, loss 1.03643, acc 0.484375\n",
      "2018-05-07T00:42:59.085537: step 77, loss 1.11859, acc 0.546875\n",
      "2018-05-07T00:43:00.195780: step 78, loss 0.885903, acc 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-07T00:43:01.529110: step 79, loss 1.28685, acc 0.4375\n",
      "2018-05-07T00:43:02.540072: step 80, loss 1.0638, acc 0.5\n",
      "2018-05-07T00:43:03.526762: step 81, loss 0.914253, acc 0.53125\n",
      "2018-05-07T00:43:04.600274: step 82, loss 0.917852, acc 0.625\n",
      "2018-05-07T00:43:05.700495: step 83, loss 1.00399, acc 0.46875\n",
      "2018-05-07T00:43:06.659504: step 84, loss 1.12431, acc 0.390625\n",
      "2018-05-07T00:43:07.654555: step 85, loss 0.808793, acc 0.578125\n",
      "2018-05-07T00:43:08.741191: step 86, loss 0.791646, acc 0.515625\n",
      "2018-05-07T00:43:09.802072: step 87, loss 0.778515, acc 0.546875\n",
      "2018-05-07T00:43:10.871559: step 88, loss 0.83095, acc 0.546875\n",
      "2018-05-07T00:43:12.071191: step 89, loss 0.671955, acc 0.609375\n",
      "2018-05-07T00:43:13.062238: step 90, loss 0.943105, acc 0.421875\n",
      "2018-05-07T00:43:14.179066: step 91, loss 0.694029, acc 0.5625\n",
      "2018-05-07T00:43:15.163868: step 92, loss 0.822833, acc 0.5625\n",
      "2018-05-07T00:43:16.181676: step 93, loss 1.09055, acc 0.453125\n",
      "2018-05-07T00:43:17.195229: step 94, loss 0.665658, acc 0.640625\n",
      "2018-05-07T00:43:18.216142: step 95, loss 0.763734, acc 0.5625\n",
      "2018-05-07T00:43:19.276127: step 96, loss 0.754424, acc 0.59375\n",
      "2018-05-07T00:43:20.299613: step 97, loss 0.721514, acc 0.59375\n",
      "2018-05-07T00:43:21.330610: step 98, loss 0.717337, acc 0.59375\n",
      "2018-05-07T00:43:22.364319: step 99, loss 0.932917, acc 0.40625\n",
      "2018-05-07T00:43:23.359404: step 100, loss 0.853898, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-07T00:43:31.219557: step 100, loss 0.618925, acc 0.661\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/1525609926/+3fcs/checkpoints/model-100\n",
      "\n",
      "2018-05-07T00:43:32.356771: step 101, loss 0.713934, acc 0.59375\n",
      "2018-05-07T00:43:33.318329: step 102, loss 0.947178, acc 0.375\n",
      "2018-05-07T00:43:34.232648: step 103, loss 0.771365, acc 0.578125\n",
      "2018-05-07T00:43:35.211859: step 104, loss 0.757864, acc 0.625\n",
      "2018-05-07T00:43:36.177876: step 105, loss 0.732279, acc 0.59375\n",
      "2018-05-07T00:43:37.130647: step 106, loss 0.745598, acc 0.46875\n",
      "2018-05-07T00:43:38.105843: step 107, loss 0.847943, acc 0.546875\n",
      "2018-05-07T00:43:39.087293: step 108, loss 0.692337, acc 0.609375\n",
      "2018-05-07T00:43:40.065816: step 109, loss 0.686143, acc 0.578125\n",
      "2018-05-07T00:43:41.044344: step 110, loss 0.701912, acc 0.546875\n",
      "2018-05-07T00:43:42.066178: step 111, loss 0.606281, acc 0.6875\n",
      "2018-05-07T00:43:43.003417: step 112, loss 0.847414, acc 0.421875\n",
      "2018-05-07T00:43:44.022488: step 113, loss 0.723061, acc 0.484375\n",
      "2018-05-07T00:43:45.262258: step 114, loss 0.740845, acc 0.5\n",
      "2018-05-07T00:43:46.249691: step 115, loss 0.721539, acc 0.578125\n",
      "2018-05-07T00:43:47.257936: step 116, loss 0.681789, acc 0.5625\n",
      "2018-05-07T00:43:48.244758: step 117, loss 0.664269, acc 0.546875\n",
      "2018-05-07T00:43:49.205926: step 118, loss 0.643474, acc 0.65625\n",
      "2018-05-07T00:43:50.180559: step 119, loss 0.675144, acc 0.59375\n",
      "2018-05-07T00:43:51.144222: step 120, loss 0.71654, acc 0.53125\n",
      "2018-05-07T00:43:52.116979: step 121, loss 0.685152, acc 0.59375\n",
      "2018-05-07T00:43:53.121935: step 122, loss 0.710246, acc 0.5\n",
      "2018-05-07T00:43:54.128597: step 123, loss 0.607636, acc 0.703125\n",
      "2018-05-07T00:43:55.126785: step 124, loss 0.68239, acc 0.53125\n",
      "2018-05-07T00:43:56.102236: step 125, loss 0.748022, acc 0.515625\n",
      "2018-05-07T00:43:57.065178: step 126, loss 0.628296, acc 0.625\n",
      "2018-05-07T00:43:58.068342: step 127, loss 0.673549, acc 0.59375\n",
      "2018-05-07T00:43:59.063100: step 128, loss 0.66597, acc 0.515625\n",
      "2018-05-07T00:43:59.981125: step 129, loss 0.688675, acc 0.578125\n",
      "2018-05-07T00:44:01.246676: step 130, loss 0.704722, acc 0.578125\n",
      "2018-05-07T00:44:02.198835: step 131, loss 0.75571, acc 0.578125\n",
      "2018-05-07T00:44:03.184738: step 132, loss 0.702965, acc 0.5625\n",
      "2018-05-07T00:44:04.160437: step 133, loss 0.61494, acc 0.65625\n",
      "2018-05-07T00:44:05.090393: step 134, loss 0.702044, acc 0.578125\n",
      "2018-05-07T00:44:05.993766: step 135, loss 0.614541, acc 0.609375\n",
      "2018-05-07T00:44:07.090691: step 136, loss 0.613455, acc 0.625\n",
      "2018-05-07T00:44:08.099602: step 137, loss 0.730328, acc 0.5\n",
      "2018-05-07T00:44:09.102718: step 138, loss 0.723604, acc 0.5625\n",
      "2018-05-07T00:44:10.098946: step 139, loss 0.677078, acc 0.5625\n",
      "2018-05-07T00:44:11.126277: step 140, loss 0.75711, acc 0.5\n",
      "2018-05-07T00:44:12.093107: step 141, loss 0.71216, acc 0.59375\n",
      "2018-05-07T00:44:13.049102: step 142, loss 0.700461, acc 0.53125\n",
      "2018-05-07T00:44:14.103602: step 143, loss 0.669614, acc 0.609375\n",
      "2018-05-07T00:44:15.097491: step 144, loss 0.668876, acc 0.578125\n",
      "2018-05-07T00:44:16.121542: step 145, loss 0.704619, acc 0.515625\n",
      "2018-05-07T00:44:17.130049: step 146, loss 0.651686, acc 0.640625\n",
      "2018-05-07T00:44:18.145965: step 147, loss 0.735314, acc 0.515625\n",
      "2018-05-07T00:44:19.163732: step 148, loss 0.674458, acc 0.609375\n",
      "2018-05-07T00:44:20.159327: step 149, loss 0.661094, acc 0.625\n",
      "2018-05-07T00:44:21.184061: step 150, loss 0.685132, acc 0.515625\n",
      "2018-05-07T00:44:22.148188: step 151, loss 0.685713, acc 0.53125\n",
      "2018-05-07T00:44:23.139743: step 152, loss 0.713662, acc 0.578125\n",
      "2018-05-07T00:44:24.167943: step 153, loss 0.672859, acc 0.59375\n",
      "2018-05-07T00:44:25.154599: step 154, loss 0.662679, acc 0.640625\n",
      "2018-05-07T00:44:26.196157: step 155, loss 0.641228, acc 0.578125\n",
      "2018-05-07T00:44:27.170605: step 156, loss 0.68441, acc 0.484375\n",
      "2018-05-07T00:44:28.115666: step 157, loss 0.673402, acc 0.703125\n",
      "2018-05-07T00:44:29.168129: step 158, loss 0.630677, acc 0.671875\n",
      "2018-05-07T00:44:30.154087: step 159, loss 0.64224, acc 0.609375\n",
      "2018-05-07T00:44:31.428976: step 160, loss 0.653917, acc 0.6875\n",
      "2018-05-07T00:44:32.615610: step 161, loss 0.646306, acc 0.625\n",
      "2018-05-07T00:44:33.624061: step 162, loss 0.599416, acc 0.59375\n",
      "2018-05-07T00:44:34.574077: step 163, loss 0.570258, acc 0.703125\n",
      "2018-05-07T00:44:35.538753: step 164, loss 0.695539, acc 0.484375\n",
      "2018-05-07T00:44:36.561567: step 165, loss 0.693088, acc 0.59375\n",
      "2018-05-07T00:44:37.675437: step 166, loss 0.672141, acc 0.59375\n",
      "2018-05-07T00:44:38.654617: step 167, loss 0.671265, acc 0.59375\n",
      "2018-05-07T00:44:39.611605: step 168, loss 0.733057, acc 0.59375\n",
      "2018-05-07T00:44:40.606324: step 169, loss 0.674458, acc 0.609375\n",
      "2018-05-07T00:44:41.612444: step 170, loss 0.662454, acc 0.59375\n",
      "2018-05-07T00:44:42.598906: step 171, loss 0.707523, acc 0.5625\n",
      "2018-05-07T00:44:43.572902: step 172, loss 0.694193, acc 0.609375\n",
      "2018-05-07T00:44:44.588576: step 173, loss 0.78694, acc 0.5\n",
      "2018-05-07T00:44:45.573114: step 174, loss 0.741255, acc 0.46875\n",
      "2018-05-07T00:44:46.509885: step 175, loss 0.707407, acc 0.59375\n",
      "2018-05-07T00:44:47.504515: step 176, loss 0.648127, acc 0.59375\n",
      "2018-05-07T00:44:48.448931: step 177, loss 0.745327, acc 0.515625\n",
      "2018-05-07T00:44:49.684052: step 178, loss 0.617507, acc 0.65625\n",
      "2018-05-07T00:44:50.670914: step 179, loss 0.680232, acc 0.625\n",
      "2018-05-07T00:44:51.635299: step 180, loss 0.625431, acc 0.609375\n",
      "2018-05-07T00:44:52.768937: step 181, loss 0.673547, acc 0.609375\n",
      "2018-05-07T00:44:53.877782: step 182, loss 0.627983, acc 0.625\n",
      "2018-05-07T00:44:54.883992: step 183, loss 0.686095, acc 0.625\n",
      "2018-05-07T00:44:55.924732: step 184, loss 0.739663, acc 0.5625\n",
      "2018-05-07T00:44:57.014981: step 185, loss 0.63728, acc 0.609375\n",
      "2018-05-07T00:44:58.016145: step 186, loss 0.609235, acc 0.671875\n",
      "2018-05-07T00:44:59.051468: step 187, loss 0.655124, acc 0.59375\n",
      "2018-05-07T00:45:00.031702: step 188, loss 0.635687, acc 0.65625\n",
      "2018-05-07T00:45:01.046209: step 189, loss 0.609791, acc 0.65625\n",
      "2018-05-07T00:45:02.157781: step 190, loss 0.660492, acc 0.578125\n",
      "2018-05-07T00:45:03.193859: step 191, loss 0.595666, acc 0.671875\n",
      "2018-05-07T00:45:04.110308: step 192, loss 0.789018, acc 0.5\n",
      "2018-05-07T00:45:05.210401: step 193, loss 0.654666, acc 0.640625\n",
      "2018-05-07T00:45:06.180555: step 194, loss 0.678435, acc 0.65625\n",
      "2018-05-07T00:45:07.140852: step 195, loss 0.577926, acc 0.703125\n",
      "2018-05-07T00:45:08.264423: step 196, loss 0.515272, acc 0.765625\n",
      "2018-05-07T00:45:09.232812: step 197, loss 0.73998, acc 0.546875\n",
      "2018-05-07T00:45:10.474218: step 198, loss 0.692239, acc 0.59375\n",
      "2018-05-07T00:45:11.450253: step 199, loss 0.90962, acc 0.515625\n",
      "2018-05-07T00:45:12.464682: step 200, loss 0.718114, acc 0.59375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-07T00:45:20.307578: step 200, loss 0.609945, acc 0.706\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/1525609926/+3fcs/checkpoints/model-200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-07T00:45:21.443939: step 201, loss 0.77577, acc 0.53125\n",
      "2018-05-07T00:45:22.441440: step 202, loss 0.644888, acc 0.65625\n",
      "2018-05-07T00:45:23.571694: step 203, loss 0.635331, acc 0.65625\n",
      "2018-05-07T00:45:24.608007: step 204, loss 0.637824, acc 0.640625\n",
      "2018-05-07T00:45:25.564779: step 205, loss 0.716511, acc 0.59375\n",
      "2018-05-07T00:45:26.628782: step 206, loss 0.759977, acc 0.515625\n",
      "2018-05-07T00:45:27.600026: step 207, loss 0.748999, acc 0.5\n",
      "2018-05-07T00:45:28.571565: step 208, loss 0.664481, acc 0.609375\n",
      "2018-05-07T00:45:29.628209: step 209, loss 0.689093, acc 0.578125\n",
      "2018-05-07T00:45:30.622885: step 210, loss 0.695882, acc 0.5625\n",
      "2018-05-07T00:45:31.603547: step 211, loss 0.692946, acc 0.609375\n",
      "2018-05-07T00:45:32.562674: step 212, loss 0.681626, acc 0.640625\n",
      "2018-05-07T00:45:33.586893: step 213, loss 0.741713, acc 0.53125\n",
      "2018-05-07T00:45:34.608549: step 214, loss 0.713838, acc 0.5625\n",
      "2018-05-07T00:45:35.596340: step 215, loss 0.759107, acc 0.453125\n",
      "2018-05-07T00:45:36.558688: step 216, loss 0.605201, acc 0.671875\n",
      "2018-05-07T00:45:37.842202: step 217, loss 0.659898, acc 0.625\n",
      "2018-05-07T00:45:39.021526: step 218, loss 0.675727, acc 0.53125\n",
      "2018-05-07T00:45:39.999654: step 219, loss 0.666105, acc 0.625\n",
      "2018-05-07T00:45:40.947307: step 220, loss 0.731139, acc 0.515625\n",
      "2018-05-07T00:45:41.973846: step 221, loss 0.660334, acc 0.625\n",
      "2018-05-07T00:45:42.963344: step 222, loss 0.812978, acc 0.546875\n",
      "2018-05-07T00:45:43.894227: step 223, loss 0.647578, acc 0.59375\n",
      "2018-05-07T00:45:44.871838: step 224, loss 0.665775, acc 0.59375\n",
      "2018-05-07T00:45:45.799664: step 225, loss 0.729097, acc 0.578125\n",
      "2018-05-07T00:45:46.795537: step 226, loss 0.617531, acc 0.6875\n",
      "2018-05-07T00:45:47.767228: step 227, loss 0.626065, acc 0.625\n",
      "2018-05-07T00:45:48.721924: step 228, loss 0.703509, acc 0.546875\n",
      "2018-05-07T00:45:49.644500: step 229, loss 0.609115, acc 0.671875\n",
      "2018-05-07T00:45:50.626278: step 230, loss 0.673732, acc 0.65625\n",
      "2018-05-07T00:45:51.589796: step 231, loss 0.606118, acc 0.671875\n",
      "2018-05-07T00:45:52.600147: step 232, loss 0.680199, acc 0.5625\n",
      "2018-05-07T00:45:53.583914: step 233, loss 0.723132, acc 0.515625\n",
      "2018-05-07T00:45:54.637625: step 234, loss 0.681253, acc 0.59375\n",
      "2018-05-07T00:45:55.612133: step 235, loss 0.671388, acc 0.640625\n",
      "2018-05-07T00:45:56.534049: step 236, loss 0.712133, acc 0.5625\n",
      "2018-05-07T00:45:57.594970: step 237, loss 0.646534, acc 0.609375\n",
      "2018-05-07T00:45:58.538013: step 238, loss 0.61947, acc 0.640625\n",
      "2018-05-07T00:45:59.511136: step 239, loss 0.670832, acc 0.578125\n",
      "2018-05-07T00:46:00.491003: step 240, loss 0.632924, acc 0.671875\n",
      "2018-05-07T00:46:01.450533: step 241, loss 0.667343, acc 0.609375\n",
      "2018-05-07T00:46:02.486393: step 242, loss 0.79259, acc 0.59375\n",
      "2018-05-07T00:46:03.488093: step 243, loss 0.691795, acc 0.515625\n",
      "2018-05-07T00:46:04.478271: step 244, loss 0.692112, acc 0.53125\n",
      "2018-05-07T00:46:05.724331: step 245, loss 0.662444, acc 0.65625\n",
      "2018-05-07T00:46:06.749189: step 246, loss 0.665124, acc 0.671875\n",
      "2018-05-07T00:46:07.710516: step 247, loss 0.750258, acc 0.5\n",
      "2018-05-07T00:46:08.748327: step 248, loss 0.657979, acc 0.5625\n",
      "2018-05-07T00:46:10.106924: step 249, loss 0.684373, acc 0.578125\n",
      "2018-05-07T00:46:11.357529: step 250, loss 0.68836, acc 0.546875\n",
      "2018-05-07T00:46:12.737103: step 251, loss 0.629021, acc 0.6875\n",
      "2018-05-07T00:46:14.017837: step 252, loss 0.60992, acc 0.703125\n",
      "2018-05-07T00:46:15.049003: step 253, loss 0.650791, acc 0.640625\n",
      "2018-05-07T00:46:16.043276: step 254, loss 0.659792, acc 0.625\n",
      "2018-05-07T00:46:17.022105: step 255, loss 0.729634, acc 0.5\n",
      "2018-05-07T00:46:18.022171: step 256, loss 0.705427, acc 0.46875\n",
      "2018-05-07T00:46:19.291392: step 257, loss 0.691529, acc 0.453125\n",
      "2018-05-07T00:46:20.325155: step 258, loss 0.649936, acc 0.546875\n",
      "2018-05-07T00:46:21.339027: step 259, loss 0.804623, acc 0.40625\n",
      "2018-05-07T00:46:22.342600: step 260, loss 0.774177, acc 0.546875\n",
      "2018-05-07T00:46:23.353469: step 261, loss 0.605668, acc 0.6875\n",
      "2018-05-07T00:46:24.353433: step 262, loss 0.680557, acc 0.5625\n",
      "2018-05-07T00:46:25.383356: step 263, loss 0.731031, acc 0.515625\n",
      "2018-05-07T00:46:26.387073: step 264, loss 0.726119, acc 0.53125\n",
      "2018-05-07T00:46:27.387445: step 265, loss 0.732401, acc 0.59375\n",
      "2018-05-07T00:46:28.365697: step 266, loss 0.751033, acc 0.515625\n",
      "2018-05-07T00:46:29.369103: step 267, loss 0.70553, acc 0.5\n",
      "2018-05-07T00:46:30.428930: step 268, loss 0.668404, acc 0.65625\n",
      "2018-05-07T00:46:31.408389: step 269, loss 0.704346, acc 0.625\n",
      "2018-05-07T00:46:32.765157: step 270, loss 0.638691, acc 0.625\n",
      "2018-05-07T00:46:34.232496: step 271, loss 0.660391, acc 0.640625\n",
      "2018-05-07T00:46:35.505816: step 272, loss 0.648145, acc 0.5625\n",
      "2018-05-07T00:46:36.532580: step 273, loss 0.643123, acc 0.640625\n",
      "2018-05-07T00:46:37.576389: step 274, loss 0.686556, acc 0.546875\n",
      "2018-05-07T00:46:38.780734: step 275, loss 0.715615, acc 0.515625\n",
      "2018-05-07T00:46:40.037274: step 276, loss 0.735912, acc 0.5\n",
      "2018-05-07T00:46:41.310933: step 277, loss 0.71208, acc 0.5625\n",
      "2018-05-07T00:46:42.341273: step 278, loss 0.670998, acc 0.59375\n",
      "2018-05-07T00:46:43.432343: step 279, loss 0.645362, acc 0.546875\n",
      "2018-05-07T00:46:44.464618: step 280, loss 0.676665, acc 0.578125\n",
      "2018-05-07T00:46:45.500114: step 281, loss 0.721566, acc 0.5625\n",
      "2018-05-07T00:46:46.727606: step 282, loss 0.641311, acc 0.609375\n",
      "2018-05-07T00:46:47.994516: step 283, loss 0.510615, acc 0.78125\n",
      "2018-05-07T00:46:49.235351: step 284, loss 0.586383, acc 0.71875\n",
      "2018-05-07T00:46:50.233581: step 285, loss 0.633509, acc 0.6875\n",
      "2018-05-07T00:46:51.253681: step 286, loss 0.594154, acc 0.703125\n",
      "2018-05-07T00:46:52.243185: step 287, loss 0.745028, acc 0.578125\n",
      "2018-05-07T00:46:53.278597: step 288, loss 0.630322, acc 0.625\n",
      "2018-05-07T00:46:54.368128: step 289, loss 0.662453, acc 0.609375\n",
      "2018-05-07T00:46:55.461039: step 290, loss 0.663994, acc 0.625\n",
      "2018-05-07T00:46:56.486417: step 291, loss 0.653681, acc 0.546875\n",
      "2018-05-07T00:46:57.478679: step 292, loss 0.679841, acc 0.484375\n",
      "2018-05-07T00:46:58.457343: step 293, loss 0.688179, acc 0.546875\n",
      "2018-05-07T00:46:59.440706: step 294, loss 0.652528, acc 0.609375\n",
      "2018-05-07T00:47:00.633501: step 295, loss 0.628705, acc 0.65625\n",
      "2018-05-07T00:47:01.594407: step 296, loss 0.696262, acc 0.5625\n",
      "2018-05-07T00:47:02.561052: step 297, loss 0.649452, acc 0.578125\n",
      "2018-05-07T00:47:03.567017: step 298, loss 0.613879, acc 0.609375\n",
      "2018-05-07T00:47:04.548409: step 299, loss 0.677112, acc 0.59375\n",
      "2018-05-07T00:47:05.735152: step 300, loss 0.674878, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-07T00:47:12.649398: step 300, loss 0.577631, acc 0.758\n",
      "\n",
      "Saved model checkpoint to /Users/tdual/Workspace/char_level_cnn/runs/1525609926/+3fcs/checkpoints/model-300\n",
      "\n",
      "2018-05-07T00:47:13.787091: step 301, loss 0.685495, acc 0.578125\n",
      "2018-05-07T00:47:14.812366: step 302, loss 0.595448, acc 0.71875\n",
      "2018-05-07T00:47:16.084268: step 303, loss 0.645209, acc 0.625\n",
      "2018-05-07T00:47:17.107126: step 304, loss 0.65021, acc 0.5625\n",
      "2018-05-07T00:47:18.126419: step 305, loss 0.672709, acc 0.59375\n",
      "2018-05-07T00:47:19.112292: step 306, loss 0.657506, acc 0.640625\n",
      "2018-05-07T00:47:20.085744: step 307, loss 0.629513, acc 0.671875\n",
      "2018-05-07T00:47:21.145774: step 308, loss 0.683611, acc 0.515625\n",
      "2018-05-07T00:47:22.102974: step 309, loss 0.644388, acc 0.546875\n",
      "2018-05-07T00:47:23.172527: step 310, loss 0.665094, acc 0.609375\n",
      "2018-05-07T00:47:24.174640: step 311, loss 0.653324, acc 0.59375\n",
      "2018-05-07T00:47:25.191188: step 312, loss 0.716259, acc 0.46875\n",
      "2018-05-07T00:47:26.118437: step 313, loss 0.684557, acc 0.578125\n",
      "2018-05-07T00:47:27.115927: step 314, loss 0.648736, acc 0.65625\n",
      "2018-05-07T00:47:28.125477: step 315, loss 0.634261, acc 0.578125\n",
      "2018-05-07T00:47:29.097878: step 316, loss 0.664896, acc 0.59375\n",
      "2018-05-07T00:47:30.109413: step 317, loss 0.651529, acc 0.578125\n",
      "2018-05-07T00:47:31.122870: step 318, loss 0.604008, acc 0.625\n",
      "2018-05-07T00:47:32.093475: step 319, loss 0.625539, acc 0.65625\n",
      "2018-05-07T00:47:33.064274: step 320, loss 0.66364, acc 0.609375\n",
      "2018-05-07T00:47:34.100216: step 321, loss 0.572478, acc 0.65625\n",
      "2018-05-07T00:47:35.157383: step 322, loss 0.626399, acc 0.671875\n",
      "2018-05-07T00:47:36.176388: step 323, loss 0.596463, acc 0.640625\n",
      "2018-05-07T00:47:37.189923: step 324, loss 0.700645, acc 0.546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-07T00:47:38.226329: step 325, loss 0.698235, acc 0.5625\n",
      "2018-05-07T00:47:39.219266: step 326, loss 0.652078, acc 0.671875\n",
      "2018-05-07T00:47:40.182491: step 327, loss 0.626211, acc 0.640625\n",
      "2018-05-07T00:47:41.164248: step 328, loss 0.665267, acc 0.59375\n",
      "2018-05-07T00:47:42.161769: step 329, loss 0.816607, acc 0.453125\n",
      "2018-05-07T00:47:43.156508: step 330, loss 0.628378, acc 0.640625\n",
      "2018-05-07T00:47:44.113926: step 331, loss 0.644645, acc 0.578125\n",
      "2018-05-07T00:47:45.078321: step 332, loss 0.691945, acc 0.59375\n",
      "2018-05-07T00:47:46.068627: step 333, loss 0.681883, acc 0.5625\n",
      "2018-05-07T00:47:47.070578: step 334, loss 0.704452, acc 0.5625\n",
      "2018-05-07T00:47:47.991525: step 335, loss 0.661097, acc 0.640625\n",
      "2018-05-07T00:47:48.977532: step 336, loss 0.603087, acc 0.578125\n",
      "2018-05-07T00:47:49.929460: step 337, loss 0.676692, acc 0.578125\n",
      "2018-05-07T00:47:50.914265: step 338, loss 0.647991, acc 0.640625\n",
      "2018-05-07T00:47:51.839659: step 339, loss 0.603966, acc 0.640625\n",
      "2018-05-07T00:47:52.825654: step 340, loss 0.648906, acc 0.65625\n",
      "2018-05-07T00:47:53.772183: step 341, loss 0.698763, acc 0.578125\n",
      "2018-05-07T00:47:54.767298: step 342, loss 0.684512, acc 0.578125\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)\n",
    "\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "\n",
    "        #timestamp = str(int(time.time()))\n",
    "        timestamp = \"1525609926\"\n",
    "        prefix = \"+3fcs\"\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp, prefix))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        \n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        if save_checkpoint:\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "  \n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run([train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run([global_step, dev_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "                \n",
    "            if save_checkpoint and current_step % evaluate_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char level cnn \n",
    "- 1525408577  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using clean_str  \n",
    "```\n",
    "Evaluation:\n",
    "2018-04-17T10:40:20.162299: step 5000, loss 0.161694, acc 0.942\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('./runs/1525408577/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(x):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_dim,\n",
    "                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            saver.restore(sess, \"runs/1525408577/checkpoints/model-98400\")\n",
    "\n",
    "            feed_dict = {\n",
    "                  cnn.input_x: x,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "            feature_5, feature_2 = sess.run([cnn.f_h, cnn.scores ], feed_dict=feed_dict)\n",
    "    return feature_5, feature_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = list(open(\"data/amazon/rating_5.txt\", \"r\").readlines())\n",
    "review = [s.strip() for s in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []\n",
    "for r in review:\n",
    "    l = r.split(\":::::\")\n",
    "    y.append(float(l[0]))\n",
    "    x.append(l[1].replace(\" \", \"\").replace(\"\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(\"runs/1525408577\", \"vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x)))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5 ,feature_2 = get_feature(x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "chunk_size = 100\n",
    "for i in range(0, len(x) , chunk_size):\n",
    "    feature_5 ,feature_2 = get_feature(x[i:i+chunk_size])\n",
    "    for f, r in zip(feature_5, y[i:i+chunk_size]):\n",
    "        s  += int(np.argmax(f) == r)\n",
    "    print(s/(i+chunk_size))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(feature_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2 [0]  #[neg, pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3.6)",
   "language": "python",
   "name": "conda_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
