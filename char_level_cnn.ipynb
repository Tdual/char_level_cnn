{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset  \n",
    "http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ShopRunner/jupyter-notify\n",
    "```\n",
    "pip install jupyternotify\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib.learn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    " \n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded,W,strides=[1, 1, 1, 1],padding=\"VALID\", name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID',name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        print(num_filters_total)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "                            # add@@@@@@@@\n",
    "            features_size = 5\n",
    "            self.features = tf.Variable(tf.truncated_normal([num_filters_total, features_size], stddev=0.1))\n",
    "            # 384 * 5\n",
    "            self.f_h = tf.matmul(self.h_drop, self.features, )\n",
    "              # (None * 383) *(384 * 5) \n",
    "            #print(self.h_drop) #None * 384 \n",
    "            #print(self.f_h) # None * 5\n",
    "\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\"W\", shape=[features_size, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            #W = tf.get_variable(\"W\", shape=[num_filters_total, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "\n",
    "            \n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            #self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.f_h, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "dev_sample_percentage = 0.0005\n",
    "\n",
    "positive_data_file = \"data/amazon/book_pos.txt\"#\"./data/rt-polarity.pos\"\n",
    "negative_data_file = \"data/amazon/book_neg.txt\"#\"./data/rt-polarity.neg\" #\"Data source for the negative data.\")\n",
    "#positive_data_file = \"data/chABSA/pos.txt\"\n",
    "#negative_data_file = \"data/chABSA/neg.txt\"\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 32     #, \"Dimensionality of character embedding (default: 128)\")\n",
    "filter_sizes = \"2,3,4,5\"        #, \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "num_filters=128               #, \"Number of filters per filter size (default: 128)\")\n",
    "dropout_keep_prob=0.5 #, \"Dropout keep probability (default: 0.5)\")\n",
    "l2_reg_lambda=0.0          #, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size=64                    #, \"Batch Size (default: 64)\")\n",
    "num_epochs=200              #, \"Number of training epochs (default: 200)\")\n",
    "evaluate_every=100         #, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "num_checkpoints=5          #, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement=True    #, \"Allow device soft device placement\")\n",
    "log_device_placement=False  #, \"Log placement of ops on devices\")\n",
    "\n",
    "#FLAGS = tf.flags.FLAGS\n",
    "#FLAGS._parse_flags()\n",
    "#print(\"\\nParameters:\")\n",
    "#for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#    print(\"{}={}\".format(attr.upper(), value))\n",
    "#print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "       \n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    #positive_examples = [s.strip() for s in positive_examples]\n",
    "    positive_examples = [s.replace(\" \", \"\").replace(\"\", \" \") for s in positive_examples]\n",
    "    \n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.replace(\" \", \"\").replace(\"\", \" \") for s in negative_examples]\n",
    "    #negative_examples = [s.strip() for s in negative_examples]\n",
    "\n",
    "    x_text = positive_examples + negative_examples\n",
    "    #x_text = [clean_str(sent) for sent in x_text]\n",
    "\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"a5a6d49b-62bb-48b5-909d-f508d994d887\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"a5a6d49b-62bb-48b5-909d-f508d994d887\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x_text, y = load_data_and_labels(positive_data_file, negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' L o v e t h e c h a r a c t e r s . L o v e t h e l o c a t i o n ! I c a n n o t w a i t f o r t h e n e x t b o o k i n t h e s e r i e s . I w i l l c e r t a i n l y r e a d i t . \\n '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = np.array([len(r)for r in x_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length\n",
       "0     187\n",
       "1     399\n",
       "2     621\n",
       "3     215\n",
       "4     225"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>842.351232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1119.140102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>931.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48939.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               length\n",
       "count  1000000.000000\n",
       "mean       842.351232\n",
       "std       1119.140102\n",
       "min          3.000000\n",
       "25%        255.000000\n",
       "50%        447.000000\n",
       "75%        931.000000\n",
       "max      48939.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c1d5174e0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2e7764e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = [x[:2000] if len(x) > 2000 else x for x in x_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>694.104355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>584.037875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>931.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               length\n",
       "count  1000000.000000\n",
       "mean       694.104355\n",
       "std        584.037875\n",
       "min          3.000000\n",
       "25%        255.000000\n",
       "50%        447.000000\n",
       "75%        931.000000\n",
       "max       2000.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_list = np.array([len(r)for r in x_text])\n",
    "df = pd.DataFrame(length_list, columns=[\"length\"])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c28cbf1d0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1d517d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_text = x_text[:20000]\n",
    "#y = y[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"Ja\" #\"En\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max document length 2000\n"
     ]
    }
   ],
   "source": [
    "if lang == \"En\":\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "elif lang == \"Ja\":\n",
    "    max_document_length = max([len(x) for x in x_text])\n",
    "print(\"max document length\", max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-ef6c3fae6839>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tdual/anaconda2/envs/py3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tdual/anaconda2/envs/py3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"75a680e8-0e35-4b38-8a0e-dd487ffd515f\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"75a680e8-0e35-4b38-8a0e-dd487ffd515f\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 66\n",
      "Train/Dev split: 999500/500\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"d77a6734-38d1-4190-b703-26da8bb862b5\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"d77a6734-38d1-4190-b703-26da8bb862b5\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle():\n",
    "    chunk_size =  int(len(x_train)/10)\n",
    "    print(chunk_size)\n",
    "    for i in range(0, len(x_train), chunck_size):\n",
    "        print(i)\n",
    "        end = i+chunck_size\n",
    "        if end < len(x_train):\n",
    "            chunk = x_train[i:  end]\n",
    "        else:\n",
    "            chunk = x_train[i:]\n",
    "            end = len(x_train)\n",
    "        with open(\"data/x_train_{}.pkl\".format(end), \"wb\") as f:\n",
    "            pickle.dump(chunk, f, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x_train():\n",
    "    all_size = 999000\n",
    "    chunk_size = 99900\n",
    "    x_train =[]\n",
    "    for i in range(0, all_size, chunck_size):\n",
    "        print(i)\n",
    "        end = i+chunk_size\n",
    "        if end > len(reviews):\n",
    "            end = all_size\n",
    "        with open(\"data/x_train_{}.pkl\".format(end), \"rb\") as f:\n",
    "            x_train += pickle.load(f)\n",
    "        return x_train\n",
    "#x_train = load_x_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "embedding_size = embedding_dim\n",
    "filter_sizes = list(map(int, filter_sizes.split(\",\")))\n",
    "num_filters = num_filters\n",
    "l2_reg_lambda = l2_reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dropout/Variable:0/grad/hist is illegal; using dropout/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dropout/Variable:0/grad/sparsity is illegal; using dropout/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/tdual/Workspace/char_level_cnn/runs/1525408577/2-3-4-5\n",
      "\n",
      "2018-05-06T20:15:09.784545: step 1, loss 1.19122, acc 0.53125\n",
      "2018-05-06T20:15:10.902646: step 2, loss 1.18837, acc 0.515625\n",
      "2018-05-06T20:15:11.981712: step 3, loss 1.02522, acc 0.578125\n",
      "2018-05-06T20:15:13.203445: step 4, loss 1.27616, acc 0.515625\n",
      "2018-05-06T20:15:14.334388: step 5, loss 1.54433, acc 0.46875\n",
      "2018-05-06T20:15:15.358074: step 6, loss 1.06444, acc 0.5625\n",
      "2018-05-06T20:15:16.368979: step 7, loss 1.47794, acc 0.40625\n",
      "2018-05-06T20:15:17.394182: step 8, loss 1.06087, acc 0.5\n",
      "2018-05-06T20:15:18.417519: step 9, loss 1.02513, acc 0.578125\n",
      "2018-05-06T20:15:19.426315: step 10, loss 1.23816, acc 0.5625\n",
      "2018-05-06T20:15:20.461781: step 11, loss 1.17861, acc 0.546875\n",
      "2018-05-06T20:15:21.516737: step 12, loss 1.3575, acc 0.5\n",
      "2018-05-06T20:15:22.619583: step 13, loss 1.26741, acc 0.5\n",
      "2018-05-06T20:15:23.727138: step 14, loss 1.51739, acc 0.453125\n",
      "2018-05-06T20:15:24.784461: step 15, loss 1.2746, acc 0.546875\n",
      "2018-05-06T20:15:25.795543: step 16, loss 1.53086, acc 0.421875\n",
      "2018-05-06T20:15:26.796879: step 17, loss 1.35759, acc 0.453125\n",
      "2018-05-06T20:15:27.833400: step 18, loss 1.29659, acc 0.5625\n",
      "2018-05-06T20:15:28.942500: step 19, loss 1.18347, acc 0.546875\n",
      "2018-05-06T20:15:30.304436: step 20, loss 1.28156, acc 0.5\n",
      "2018-05-06T20:15:31.449939: step 21, loss 1.63097, acc 0.359375\n",
      "2018-05-06T20:15:32.577070: step 22, loss 1.31958, acc 0.484375\n",
      "2018-05-06T20:15:33.810029: step 23, loss 0.897267, acc 0.671875\n",
      "2018-05-06T20:15:34.999611: step 24, loss 1.21702, acc 0.484375\n",
      "2018-05-06T20:15:36.048396: step 25, loss 0.918775, acc 0.609375\n",
      "2018-05-06T20:15:37.177568: step 26, loss 1.31323, acc 0.5625\n",
      "2018-05-06T20:15:38.195773: step 27, loss 1.16248, acc 0.5625\n",
      "2018-05-06T20:15:39.425335: step 28, loss 1.01214, acc 0.515625\n",
      "2018-05-06T20:15:40.381920: step 29, loss 1.20743, acc 0.484375\n",
      "2018-05-06T20:15:41.483408: step 30, loss 1.08305, acc 0.5625\n",
      "2018-05-06T20:15:42.504119: step 31, loss 1.21189, acc 0.53125\n",
      "2018-05-06T20:15:43.533307: step 32, loss 1.04995, acc 0.59375\n",
      "2018-05-06T20:15:44.554885: step 33, loss 0.98885, acc 0.578125\n",
      "2018-05-06T20:15:45.525029: step 34, loss 1.02657, acc 0.546875\n",
      "2018-05-06T20:15:46.581185: step 35, loss 1.37895, acc 0.484375\n",
      "2018-05-06T20:15:47.569291: step 36, loss 0.916853, acc 0.53125\n",
      "2018-05-06T20:15:48.566266: step 37, loss 1.08902, acc 0.59375\n",
      "2018-05-06T20:15:49.504260: step 38, loss 1.11335, acc 0.578125\n",
      "2018-05-06T20:15:50.485030: step 39, loss 1.23715, acc 0.484375\n",
      "2018-05-06T20:15:51.525983: step 40, loss 0.954429, acc 0.53125\n",
      "2018-05-06T20:15:52.643266: step 41, loss 0.76433, acc 0.625\n",
      "2018-05-06T20:15:53.648252: step 42, loss 1.03882, acc 0.484375\n",
      "2018-05-06T20:15:54.656394: step 43, loss 0.902992, acc 0.578125\n",
      "2018-05-06T20:15:55.710708: step 44, loss 0.914443, acc 0.5625\n",
      "2018-05-06T20:15:56.773009: step 45, loss 1.22635, acc 0.5\n",
      "2018-05-06T20:15:57.822843: step 46, loss 0.935236, acc 0.609375\n",
      "2018-05-06T20:15:58.831811: step 47, loss 1.08188, acc 0.484375\n",
      "2018-05-06T20:15:59.868073: step 48, loss 0.977875, acc 0.609375\n",
      "2018-05-06T20:16:00.841531: step 49, loss 1.04058, acc 0.53125\n",
      "2018-05-06T20:16:01.844836: step 50, loss 1.07822, acc 0.546875\n",
      "2018-05-06T20:16:02.881130: step 51, loss 0.759803, acc 0.6875\n",
      "2018-05-06T20:16:03.899427: step 52, loss 0.978781, acc 0.5\n",
      "2018-05-06T20:16:04.841336: step 53, loss 1.11729, acc 0.484375\n",
      "2018-05-06T20:16:05.765513: step 54, loss 0.818774, acc 0.59375\n",
      "2018-05-06T20:16:07.021897: step 55, loss 0.773693, acc 0.609375\n",
      "2018-05-06T20:16:08.047827: step 56, loss 1.12188, acc 0.5\n",
      "2018-05-06T20:16:09.022186: step 57, loss 1.36789, acc 0.453125\n",
      "2018-05-06T20:16:09.984773: step 58, loss 0.915247, acc 0.53125\n",
      "2018-05-06T20:16:10.972711: step 59, loss 1.03499, acc 0.5625\n",
      "2018-05-06T20:16:12.379889: step 60, loss 0.792915, acc 0.515625\n",
      "2018-05-06T20:16:13.667659: step 61, loss 0.807775, acc 0.578125\n",
      "2018-05-06T20:16:14.851052: step 62, loss 1.10688, acc 0.515625\n",
      "2018-05-06T20:16:15.871043: step 63, loss 0.929318, acc 0.546875\n",
      "2018-05-06T20:16:16.895937: step 64, loss 1.06037, acc 0.53125\n",
      "2018-05-06T20:16:17.879060: step 65, loss 0.957099, acc 0.53125\n",
      "2018-05-06T20:16:18.807282: step 66, loss 0.901265, acc 0.5625\n",
      "2018-05-06T20:16:19.915048: step 67, loss 0.978309, acc 0.53125\n",
      "2018-05-06T20:16:21.096465: step 68, loss 1.06749, acc 0.5\n",
      "2018-05-06T20:16:22.061969: step 69, loss 0.861506, acc 0.59375\n",
      "2018-05-06T20:16:23.065270: step 70, loss 0.834359, acc 0.609375\n",
      "2018-05-06T20:16:24.243841: step 71, loss 0.933873, acc 0.484375\n",
      "2018-05-06T20:16:25.369888: step 72, loss 0.839159, acc 0.546875\n",
      "2018-05-06T20:16:26.349661: step 73, loss 1.00791, acc 0.578125\n",
      "2018-05-06T20:16:27.532377: step 74, loss 1.10609, acc 0.5625\n",
      "2018-05-06T20:16:28.592783: step 75, loss 0.831827, acc 0.59375\n",
      "2018-05-06T20:16:29.614988: step 76, loss 0.854496, acc 0.546875\n",
      "2018-05-06T20:16:30.594987: step 77, loss 0.820598, acc 0.609375\n",
      "2018-05-06T20:16:31.744219: step 78, loss 0.957152, acc 0.5625\n",
      "2018-05-06T20:16:32.751726: step 79, loss 1.06499, acc 0.59375\n",
      "2018-05-06T20:16:33.754435: step 80, loss 0.902043, acc 0.59375\n",
      "2018-05-06T20:16:34.719290: step 81, loss 0.854299, acc 0.578125\n",
      "2018-05-06T20:16:35.686207: step 82, loss 0.778428, acc 0.625\n",
      "2018-05-06T20:16:36.616471: step 83, loss 0.930653, acc 0.546875\n",
      "2018-05-06T20:16:37.558533: step 84, loss 0.669374, acc 0.609375\n",
      "2018-05-06T20:16:38.524802: step 85, loss 0.828361, acc 0.59375\n",
      "2018-05-06T20:16:39.514021: step 86, loss 0.930304, acc 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-06T20:16:40.536709: step 87, loss 1.04111, acc 0.515625\n",
      "2018-05-06T20:16:41.558761: step 88, loss 0.691799, acc 0.625\n",
      "2018-05-06T20:16:42.579191: step 89, loss 0.709892, acc 0.640625\n",
      "2018-05-06T20:16:43.711315: step 90, loss 0.822177, acc 0.546875\n",
      "2018-05-06T20:16:45.106489: step 91, loss 0.799831, acc 0.59375\n",
      "2018-05-06T20:16:46.068470: step 92, loss 0.934839, acc 0.546875\n",
      "2018-05-06T20:16:47.071626: step 93, loss 0.736761, acc 0.671875\n",
      "2018-05-06T20:16:48.050021: step 94, loss 0.693646, acc 0.625\n",
      "2018-05-06T20:16:49.030526: step 95, loss 0.921909, acc 0.546875\n",
      "2018-05-06T20:16:49.954532: step 96, loss 0.895175, acc 0.546875\n",
      "2018-05-06T20:16:50.931127: step 97, loss 0.804281, acc 0.578125\n",
      "2018-05-06T20:16:51.918382: step 98, loss 0.838438, acc 0.59375\n",
      "2018-05-06T20:16:53.229597: step 99, loss 0.888956, acc 0.59375\n",
      "2018-05-06T20:16:54.502343: step 100, loss 0.903383, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-06T20:16:58.137069: step 100, loss 0.603543, acc 0.606\n",
      "\n",
      "2018-05-06T20:16:59.142639: step 101, loss 0.786789, acc 0.5625\n",
      "2018-05-06T20:17:00.175872: step 102, loss 0.809248, acc 0.640625\n",
      "2018-05-06T20:17:01.168878: step 103, loss 0.818843, acc 0.578125\n",
      "2018-05-06T20:17:02.152202: step 104, loss 0.962846, acc 0.53125\n",
      "2018-05-06T20:17:03.182079: step 105, loss 0.756325, acc 0.65625\n",
      "2018-05-06T20:17:04.520873: step 106, loss 0.938192, acc 0.46875\n",
      "2018-05-06T20:17:05.543362: step 107, loss 0.778974, acc 0.609375\n",
      "2018-05-06T20:17:06.629999: step 108, loss 0.848689, acc 0.484375\n",
      "2018-05-06T20:17:07.694169: step 109, loss 0.665107, acc 0.65625\n",
      "2018-05-06T20:17:08.642324: step 110, loss 0.935446, acc 0.53125\n",
      "2018-05-06T20:17:09.646903: step 111, loss 0.961094, acc 0.5\n",
      "2018-05-06T20:17:10.629765: step 112, loss 0.731792, acc 0.703125\n",
      "2018-05-06T20:17:11.902999: step 113, loss 0.889937, acc 0.5625\n",
      "2018-05-06T20:17:13.145967: step 114, loss 0.874973, acc 0.578125\n",
      "2018-05-06T20:17:14.371162: step 115, loss 0.697453, acc 0.625\n",
      "2018-05-06T20:17:15.542554: step 116, loss 0.689876, acc 0.671875\n",
      "2018-05-06T20:17:16.483841: step 117, loss 0.937702, acc 0.546875\n",
      "2018-05-06T20:17:17.455580: step 118, loss 0.692574, acc 0.703125\n",
      "2018-05-06T20:17:18.457108: step 119, loss 0.958465, acc 0.625\n",
      "2018-05-06T20:17:19.471091: step 120, loss 0.747338, acc 0.640625\n",
      "2018-05-06T20:17:20.482971: step 121, loss 0.770863, acc 0.609375\n",
      "2018-05-06T20:17:21.780080: step 122, loss 0.833047, acc 0.5\n",
      "2018-05-06T20:17:22.842129: step 123, loss 0.827869, acc 0.5625\n",
      "2018-05-06T20:17:23.879561: step 124, loss 0.878174, acc 0.53125\n",
      "2018-05-06T20:17:25.064056: step 125, loss 0.903172, acc 0.546875\n",
      "2018-05-06T20:17:26.081941: step 126, loss 0.766634, acc 0.671875\n",
      "2018-05-06T20:17:27.065749: step 127, loss 0.841736, acc 0.640625\n",
      "2018-05-06T20:17:28.133495: step 128, loss 0.673977, acc 0.640625\n",
      "2018-05-06T20:17:29.161373: step 129, loss 0.869724, acc 0.59375\n",
      "2018-05-06T20:17:30.153014: step 130, loss 0.646237, acc 0.640625\n",
      "2018-05-06T20:17:31.165441: step 131, loss 0.761141, acc 0.578125\n",
      "2018-05-06T20:17:32.173662: step 132, loss 0.654026, acc 0.609375\n",
      "2018-05-06T20:17:33.241962: step 133, loss 0.752052, acc 0.640625\n",
      "2018-05-06T20:17:34.560120: step 134, loss 0.706798, acc 0.640625\n",
      "2018-05-06T20:17:35.795650: step 135, loss 0.667961, acc 0.65625\n",
      "2018-05-06T20:17:36.871475: step 136, loss 0.735536, acc 0.625\n",
      "2018-05-06T20:17:37.964650: step 137, loss 0.611843, acc 0.71875\n",
      "2018-05-06T20:17:39.034351: step 138, loss 0.635382, acc 0.640625\n",
      "2018-05-06T20:17:40.080926: step 139, loss 0.975176, acc 0.46875\n",
      "2018-05-06T20:17:41.185763: step 140, loss 0.874502, acc 0.5\n",
      "2018-05-06T20:17:42.337403: step 141, loss 0.71836, acc 0.65625\n",
      "2018-05-06T20:17:43.538061: step 142, loss 0.717191, acc 0.640625\n",
      "2018-05-06T20:17:44.624629: step 143, loss 0.625427, acc 0.703125\n",
      "2018-05-06T20:17:45.743928: step 144, loss 0.777715, acc 0.5625\n",
      "2018-05-06T20:17:46.890204: step 145, loss 0.778508, acc 0.5625\n",
      "2018-05-06T20:17:48.001042: step 146, loss 0.752315, acc 0.59375\n",
      "2018-05-06T20:17:49.128279: step 147, loss 0.911309, acc 0.484375\n",
      "2018-05-06T20:17:50.347355: step 148, loss 0.712313, acc 0.640625\n",
      "2018-05-06T20:17:51.534584: step 149, loss 0.758612, acc 0.640625\n",
      "2018-05-06T20:17:53.010759: step 150, loss 0.595756, acc 0.671875\n",
      "2018-05-06T20:17:54.458347: step 151, loss 0.714823, acc 0.671875\n",
      "2018-05-06T20:17:55.675104: step 152, loss 0.581583, acc 0.671875\n",
      "2018-05-06T20:17:57.053592: step 153, loss 0.647479, acc 0.640625\n",
      "2018-05-06T20:17:58.390134: step 154, loss 0.860731, acc 0.5625\n",
      "2018-05-06T20:17:59.589397: step 155, loss 0.559151, acc 0.71875\n",
      "2018-05-06T20:18:00.758279: step 156, loss 0.707555, acc 0.625\n",
      "2018-05-06T20:18:01.946316: step 157, loss 0.719627, acc 0.609375\n",
      "2018-05-06T20:18:03.202011: step 158, loss 0.654467, acc 0.578125\n",
      "2018-05-06T20:18:04.648815: step 159, loss 0.735153, acc 0.65625\n",
      "2018-05-06T20:18:05.886026: step 160, loss 0.68799, acc 0.640625\n",
      "2018-05-06T20:18:07.264895: step 161, loss 0.62815, acc 0.671875\n",
      "2018-05-06T20:18:08.594985: step 162, loss 0.576752, acc 0.671875\n",
      "2018-05-06T20:18:09.900013: step 163, loss 0.846288, acc 0.515625\n",
      "2018-05-06T20:18:11.242660: step 164, loss 0.602185, acc 0.703125\n",
      "2018-05-06T20:18:12.403011: step 165, loss 0.609697, acc 0.703125\n",
      "2018-05-06T20:18:13.640709: step 166, loss 0.584474, acc 0.75\n",
      "2018-05-06T20:18:14.809601: step 167, loss 0.632683, acc 0.734375\n",
      "2018-05-06T20:18:16.227980: step 168, loss 0.828996, acc 0.59375\n",
      "2018-05-06T20:18:17.474587: step 169, loss 0.833463, acc 0.5625\n",
      "2018-05-06T20:18:18.714132: step 170, loss 0.745998, acc 0.59375\n",
      "2018-05-06T20:18:19.935385: step 171, loss 0.738889, acc 0.609375\n",
      "2018-05-06T20:18:21.252318: step 172, loss 0.596404, acc 0.703125\n",
      "2018-05-06T20:18:22.502952: step 173, loss 0.693851, acc 0.609375\n",
      "2018-05-06T20:18:23.993870: step 174, loss 0.715397, acc 0.671875\n",
      "2018-05-06T20:18:25.202523: step 175, loss 0.682811, acc 0.59375\n",
      "2018-05-06T20:18:26.432793: step 176, loss 0.719699, acc 0.609375\n",
      "2018-05-06T20:18:27.643779: step 177, loss 0.716662, acc 0.578125\n",
      "2018-05-06T20:18:28.942856: step 178, loss 0.770027, acc 0.609375\n",
      "2018-05-06T20:18:30.158092: step 179, loss 0.673335, acc 0.625\n",
      "2018-05-06T20:18:31.363456: step 180, loss 0.640702, acc 0.640625\n",
      "2018-05-06T20:18:32.590266: step 181, loss 0.638135, acc 0.640625\n",
      "2018-05-06T20:18:33.878532: step 182, loss 0.671916, acc 0.71875\n",
      "2018-05-06T20:18:35.139398: step 183, loss 0.692446, acc 0.609375\n",
      "2018-05-06T20:18:36.374569: step 184, loss 0.753816, acc 0.546875\n",
      "2018-05-06T20:18:37.611134: step 185, loss 0.685145, acc 0.6875\n",
      "2018-05-06T20:18:38.876508: step 186, loss 0.879067, acc 0.625\n",
      "2018-05-06T20:18:40.185724: step 187, loss 0.643466, acc 0.640625\n",
      "2018-05-06T20:18:41.566396: step 188, loss 0.842543, acc 0.578125\n",
      "2018-05-06T20:18:43.102921: step 189, loss 0.715964, acc 0.65625\n",
      "2018-05-06T20:18:44.877130: step 190, loss 0.772084, acc 0.59375\n",
      "2018-05-06T20:18:46.325358: step 191, loss 0.774273, acc 0.640625\n",
      "2018-05-06T20:18:47.933127: step 192, loss 0.668628, acc 0.65625\n",
      "2018-05-06T20:18:49.619139: step 193, loss 0.532771, acc 0.71875\n",
      "2018-05-06T20:18:51.130026: step 194, loss 0.816664, acc 0.53125\n",
      "2018-05-06T20:18:52.738835: step 195, loss 0.716944, acc 0.65625\n",
      "2018-05-06T20:18:54.494181: step 196, loss 0.742403, acc 0.578125\n",
      "2018-05-06T20:18:56.112628: step 197, loss 0.65909, acc 0.75\n",
      "2018-05-06T20:18:57.521691: step 198, loss 0.607872, acc 0.625\n",
      "2018-05-06T20:18:58.929487: step 199, loss 0.701059, acc 0.640625\n",
      "2018-05-06T20:19:00.310981: step 200, loss 0.455566, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-06T20:19:06.164645: step 200, loss 0.506959, acc 0.798\n",
      "\n",
      "2018-05-06T20:19:07.574031: step 201, loss 0.589682, acc 0.65625\n",
      "2018-05-06T20:19:09.106594: step 202, loss 0.667025, acc 0.671875\n",
      "2018-05-06T20:19:10.576229: step 203, loss 0.685341, acc 0.59375\n",
      "2018-05-06T20:19:12.060365: step 204, loss 0.555457, acc 0.734375\n",
      "2018-05-06T20:19:13.545683: step 205, loss 0.58349, acc 0.6875\n",
      "2018-05-06T20:19:15.056732: step 206, loss 0.790545, acc 0.59375\n",
      "2018-05-06T20:19:16.528263: step 207, loss 0.6369, acc 0.71875\n",
      "2018-05-06T20:19:17.901656: step 208, loss 0.6885, acc 0.5625\n",
      "2018-05-06T20:19:19.241159: step 209, loss 0.555777, acc 0.75\n",
      "2018-05-06T20:19:20.667506: step 210, loss 0.525217, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-06T20:19:21.964240: step 211, loss 0.750633, acc 0.53125\n",
      "2018-05-06T20:19:23.429715: step 212, loss 0.679052, acc 0.671875\n",
      "2018-05-06T20:19:24.819751: step 213, loss 0.540523, acc 0.765625\n",
      "2018-05-06T20:19:26.141782: step 214, loss 0.637638, acc 0.6875\n",
      "2018-05-06T20:19:27.482647: step 215, loss 0.594585, acc 0.6875\n",
      "2018-05-06T20:19:28.858795: step 216, loss 0.603624, acc 0.703125\n",
      "2018-05-06T20:19:30.087268: step 217, loss 0.635791, acc 0.703125\n",
      "2018-05-06T20:19:31.336683: step 218, loss 0.715521, acc 0.640625\n",
      "2018-05-06T20:19:32.651273: step 219, loss 0.629052, acc 0.6875\n",
      "2018-05-06T20:19:34.029938: step 220, loss 0.532921, acc 0.703125\n",
      "2018-05-06T20:19:35.169113: step 221, loss 0.541975, acc 0.75\n",
      "2018-05-06T20:19:36.267420: step 222, loss 0.632615, acc 0.625\n",
      "2018-05-06T20:19:37.406142: step 223, loss 0.627492, acc 0.71875\n",
      "2018-05-06T20:19:38.545377: step 224, loss 0.63651, acc 0.671875\n",
      "2018-05-06T20:19:39.839852: step 225, loss 0.548624, acc 0.703125\n",
      "2018-05-06T20:19:41.089068: step 226, loss 0.764281, acc 0.5625\n",
      "2018-05-06T20:19:42.102013: step 227, loss 0.624068, acc 0.625\n",
      "2018-05-06T20:19:43.208330: step 228, loss 0.546715, acc 0.71875\n",
      "2018-05-06T20:19:44.232933: step 229, loss 0.557413, acc 0.71875\n",
      "2018-05-06T20:19:45.301674: step 230, loss 0.558854, acc 0.734375\n",
      "2018-05-06T20:19:46.436019: step 231, loss 0.580453, acc 0.671875\n",
      "2018-05-06T20:19:47.479871: step 232, loss 0.663595, acc 0.640625\n",
      "2018-05-06T20:19:48.507935: step 233, loss 0.491587, acc 0.796875\n",
      "2018-05-06T20:19:49.518977: step 234, loss 0.515666, acc 0.765625\n",
      "2018-05-06T20:19:50.572690: step 235, loss 0.624928, acc 0.671875\n",
      "2018-05-06T20:19:51.698283: step 236, loss 0.557737, acc 0.734375\n",
      "2018-05-06T20:19:53.000483: step 237, loss 0.68725, acc 0.609375\n",
      "2018-05-06T20:19:54.324643: step 238, loss 0.640275, acc 0.6875\n",
      "2018-05-06T20:19:55.293739: step 239, loss 0.555864, acc 0.71875\n",
      "2018-05-06T20:19:56.492210: step 240, loss 0.558567, acc 0.71875\n",
      "2018-05-06T20:19:57.587052: step 241, loss 0.756497, acc 0.609375\n",
      "2018-05-06T20:19:58.684622: step 242, loss 0.600542, acc 0.71875\n",
      "2018-05-06T20:19:59.872091: step 243, loss 0.704309, acc 0.59375\n",
      "2018-05-06T20:20:00.874524: step 244, loss 0.644008, acc 0.578125\n",
      "2018-05-06T20:20:01.907036: step 245, loss 0.789458, acc 0.59375\n",
      "2018-05-06T20:20:02.986042: step 246, loss 0.692173, acc 0.671875\n",
      "2018-05-06T20:20:04.061652: step 247, loss 0.640489, acc 0.6875\n",
      "2018-05-06T20:20:05.117782: step 248, loss 0.600867, acc 0.671875\n",
      "2018-05-06T20:20:06.122990: step 249, loss 0.643662, acc 0.625\n",
      "2018-05-06T20:20:07.111524: step 250, loss 0.623468, acc 0.671875\n",
      "2018-05-06T20:20:08.158338: step 251, loss 0.759148, acc 0.578125\n",
      "2018-05-06T20:20:09.390136: step 252, loss 0.639901, acc 0.609375\n",
      "2018-05-06T20:20:10.424372: step 253, loss 0.600877, acc 0.734375\n",
      "2018-05-06T20:20:11.439498: step 254, loss 0.552787, acc 0.703125\n",
      "2018-05-06T20:20:12.459469: step 255, loss 0.61397, acc 0.65625\n",
      "2018-05-06T20:20:13.518246: step 256, loss 0.518939, acc 0.734375\n",
      "2018-05-06T20:20:14.576089: step 257, loss 0.634031, acc 0.625\n",
      "2018-05-06T20:20:15.579778: step 258, loss 0.614567, acc 0.703125\n",
      "2018-05-06T20:20:16.591526: step 259, loss 0.570937, acc 0.6875\n",
      "2018-05-06T20:20:17.655586: step 260, loss 0.63357, acc 0.640625\n",
      "2018-05-06T20:20:18.695140: step 261, loss 0.512189, acc 0.75\n",
      "2018-05-06T20:20:19.745735: step 262, loss 0.506352, acc 0.78125\n",
      "2018-05-06T20:20:20.850522: step 263, loss 0.671541, acc 0.59375\n",
      "2018-05-06T20:20:21.933373: step 264, loss 0.573126, acc 0.6875\n",
      "2018-05-06T20:20:23.035680: step 265, loss 0.715011, acc 0.578125\n",
      "2018-05-06T20:20:24.108145: step 266, loss 0.581032, acc 0.671875\n",
      "2018-05-06T20:20:25.089591: step 267, loss 0.700725, acc 0.578125\n",
      "2018-05-06T20:20:26.175505: step 268, loss 0.677565, acc 0.640625\n",
      "2018-05-06T20:20:27.178520: step 269, loss 0.715129, acc 0.703125\n",
      "2018-05-06T20:20:28.217099: step 270, loss 0.607258, acc 0.671875\n",
      "2018-05-06T20:20:29.242643: step 271, loss 0.596521, acc 0.6875\n",
      "2018-05-06T20:20:30.299994: step 272, loss 0.589045, acc 0.640625\n",
      "2018-05-06T20:20:31.324270: step 273, loss 0.626819, acc 0.625\n",
      "2018-05-06T20:20:32.529027: step 274, loss 0.563858, acc 0.65625\n",
      "2018-05-06T20:20:33.760627: step 275, loss 0.738616, acc 0.609375\n",
      "2018-05-06T20:20:34.749738: step 276, loss 0.743852, acc 0.578125\n",
      "2018-05-06T20:20:35.775095: step 277, loss 0.597967, acc 0.65625\n",
      "2018-05-06T20:20:36.844035: step 278, loss 0.63219, acc 0.65625\n",
      "2018-05-06T20:20:37.954825: step 279, loss 0.731714, acc 0.609375\n",
      "2018-05-06T20:20:38.940727: step 280, loss 0.676449, acc 0.625\n",
      "2018-05-06T20:20:40.116536: step 281, loss 0.625784, acc 0.71875\n",
      "2018-05-06T20:20:41.136302: step 282, loss 0.562454, acc 0.71875\n",
      "2018-05-06T20:20:42.197043: step 283, loss 0.755576, acc 0.625\n",
      "2018-05-06T20:20:43.256609: step 284, loss 0.607858, acc 0.71875\n",
      "2018-05-06T20:20:44.293238: step 285, loss 0.55058, acc 0.734375\n",
      "2018-05-06T20:20:45.302784: step 286, loss 0.688262, acc 0.703125\n",
      "2018-05-06T20:20:46.276224: step 287, loss 0.598234, acc 0.734375\n",
      "2018-05-06T20:20:47.266137: step 288, loss 0.526162, acc 0.734375\n",
      "2018-05-06T20:20:48.277734: step 289, loss 0.650265, acc 0.6875\n",
      "2018-05-06T20:20:49.345901: step 290, loss 0.630762, acc 0.703125\n",
      "2018-05-06T20:20:50.490945: step 291, loss 0.535594, acc 0.75\n",
      "2018-05-06T20:20:51.663150: step 292, loss 0.580345, acc 0.6875\n",
      "2018-05-06T20:20:52.983379: step 293, loss 0.551297, acc 0.671875\n",
      "2018-05-06T20:20:54.337941: step 294, loss 0.601117, acc 0.703125\n",
      "2018-05-06T20:20:55.536517: step 295, loss 0.625745, acc 0.65625\n",
      "2018-05-06T20:20:56.683410: step 296, loss 0.601285, acc 0.71875\n",
      "2018-05-06T20:20:57.873220: step 297, loss 0.643003, acc 0.578125\n",
      "2018-05-06T20:20:59.016985: step 298, loss 0.595671, acc 0.6875\n",
      "2018-05-06T20:21:00.264084: step 299, loss 0.591031, acc 0.6875\n",
      "2018-05-06T20:21:01.476128: step 300, loss 0.612981, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-06T20:21:04.652665: step 300, loss 0.498744, acc 0.782\n",
      "\n",
      "2018-05-06T20:21:06.265999: step 301, loss 0.607063, acc 0.703125\n",
      "2018-05-06T20:21:07.588692: step 302, loss 0.571157, acc 0.75\n",
      "2018-05-06T20:21:08.867857: step 303, loss 0.52627, acc 0.703125\n",
      "2018-05-06T20:21:10.203206: step 304, loss 0.624311, acc 0.640625\n",
      "2018-05-06T20:21:11.590321: step 305, loss 0.576106, acc 0.6875\n",
      "2018-05-06T20:21:12.943734: step 306, loss 0.619619, acc 0.6875\n",
      "2018-05-06T20:21:14.241682: step 307, loss 0.713335, acc 0.671875\n",
      "2018-05-06T20:21:15.539761: step 308, loss 0.692839, acc 0.625\n",
      "2018-05-06T20:21:16.787511: step 309, loss 0.54964, acc 0.75\n",
      "2018-05-06T20:21:18.072033: step 310, loss 0.69156, acc 0.609375\n",
      "2018-05-06T20:21:19.473738: step 311, loss 0.533027, acc 0.65625\n",
      "2018-05-06T20:21:20.741125: step 312, loss 0.543541, acc 0.703125\n",
      "2018-05-06T20:21:21.930547: step 313, loss 0.643796, acc 0.609375\n",
      "2018-05-06T20:21:23.265255: step 314, loss 0.561409, acc 0.71875\n",
      "2018-05-06T20:21:24.607511: step 315, loss 0.579854, acc 0.65625\n",
      "2018-05-06T20:21:26.099892: step 316, loss 0.556373, acc 0.703125\n",
      "2018-05-06T20:21:27.404435: step 317, loss 0.487871, acc 0.78125\n",
      "2018-05-06T20:21:28.609096: step 318, loss 0.572423, acc 0.671875\n",
      "2018-05-06T20:21:29.937183: step 319, loss 0.545037, acc 0.703125\n",
      "2018-05-06T20:21:31.335731: step 320, loss 0.531535, acc 0.75\n",
      "2018-05-06T20:21:32.645993: step 321, loss 0.633743, acc 0.640625\n",
      "2018-05-06T20:21:34.102390: step 322, loss 0.715376, acc 0.59375\n",
      "2018-05-06T20:21:35.401256: step 323, loss 0.537538, acc 0.6875\n",
      "2018-05-06T20:21:36.802837: step 324, loss 0.538715, acc 0.71875\n",
      "2018-05-06T20:21:38.156259: step 325, loss 0.615199, acc 0.65625\n",
      "2018-05-06T20:21:39.419080: step 326, loss 0.567502, acc 0.6875\n",
      "2018-05-06T20:21:40.696788: step 327, loss 0.583658, acc 0.671875\n",
      "2018-05-06T20:21:42.031064: step 328, loss 0.618951, acc 0.671875\n",
      "2018-05-06T20:21:43.349234: step 329, loss 0.594127, acc 0.6875\n",
      "2018-05-06T20:21:44.429072: step 330, loss 0.635356, acc 0.65625\n",
      "2018-05-06T20:21:45.556320: step 331, loss 0.639862, acc 0.65625\n",
      "2018-05-06T20:21:46.632871: step 332, loss 0.670485, acc 0.609375\n",
      "2018-05-06T20:21:47.767954: step 333, loss 0.695406, acc 0.59375\n",
      "2018-05-06T20:21:49.013090: step 334, loss 0.58542, acc 0.65625\n",
      "2018-05-06T20:21:50.100241: step 335, loss 0.661, acc 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-06T20:21:51.146074: step 336, loss 0.565287, acc 0.6875\n",
      "2018-05-06T20:21:52.409568: step 337, loss 0.503581, acc 0.765625\n",
      "2018-05-06T20:21:53.443897: step 338, loss 0.594657, acc 0.6875\n",
      "2018-05-06T20:21:54.461533: step 339, loss 0.531223, acc 0.71875\n",
      "2018-05-06T20:21:55.515223: step 340, loss 0.572277, acc 0.75\n",
      "2018-05-06T20:21:56.573092: step 341, loss 0.570218, acc 0.703125\n",
      "2018-05-06T20:21:57.639109: step 342, loss 0.632905, acc 0.65625\n",
      "2018-05-06T20:21:58.776989: step 343, loss 0.571768, acc 0.671875\n",
      "2018-05-06T20:21:59.888116: step 344, loss 0.559507, acc 0.796875\n",
      "2018-05-06T20:22:01.036257: step 345, loss 0.687712, acc 0.609375\n",
      "2018-05-06T20:22:02.125055: step 346, loss 0.63622, acc 0.640625\n",
      "2018-05-06T20:22:03.243270: step 347, loss 0.633123, acc 0.640625\n",
      "2018-05-06T20:22:04.323284: step 348, loss 0.528626, acc 0.734375\n",
      "2018-05-06T20:22:05.384860: step 349, loss 0.564728, acc 0.6875\n",
      "2018-05-06T20:22:06.556763: step 350, loss 0.591627, acc 0.703125\n",
      "2018-05-06T20:22:07.800876: step 351, loss 0.529195, acc 0.796875\n",
      "2018-05-06T20:22:08.848706: step 352, loss 0.686573, acc 0.59375\n",
      "2018-05-06T20:22:09.875559: step 353, loss 0.663219, acc 0.6875\n",
      "2018-05-06T20:22:10.865211: step 354, loss 0.464454, acc 0.765625\n",
      "2018-05-06T20:22:11.907531: step 355, loss 0.635543, acc 0.640625\n",
      "2018-05-06T20:22:13.015156: step 356, loss 0.596886, acc 0.671875\n",
      "2018-05-06T20:22:14.227242: step 357, loss 0.584746, acc 0.71875\n",
      "2018-05-06T20:22:15.278445: step 358, loss 0.651252, acc 0.65625\n",
      "2018-05-06T20:22:16.331635: step 359, loss 0.593843, acc 0.765625\n",
      "2018-05-06T20:22:17.380083: step 360, loss 0.624721, acc 0.671875\n",
      "2018-05-06T20:22:18.510828: step 361, loss 0.470087, acc 0.734375\n",
      "2018-05-06T20:22:19.515582: step 362, loss 0.549667, acc 0.734375\n",
      "2018-05-06T20:22:20.553857: step 363, loss 0.544011, acc 0.71875\n",
      "2018-05-06T20:22:21.565045: step 364, loss 0.507144, acc 0.6875\n",
      "2018-05-06T20:22:22.575708: step 365, loss 0.562247, acc 0.71875\n",
      "2018-05-06T20:22:23.627848: step 366, loss 0.60388, acc 0.734375\n",
      "2018-05-06T20:22:24.650563: step 367, loss 0.549413, acc 0.734375\n",
      "2018-05-06T20:22:25.652068: step 368, loss 0.603592, acc 0.6875\n",
      "2018-05-06T20:22:26.683256: step 369, loss 0.639906, acc 0.671875\n",
      "2018-05-06T20:22:27.697439: step 370, loss 0.526603, acc 0.734375\n",
      "2018-05-06T20:22:28.734885: step 371, loss 0.671471, acc 0.65625\n",
      "2018-05-06T20:22:29.777710: step 372, loss 0.48026, acc 0.734375\n",
      "2018-05-06T20:22:30.760619: step 373, loss 0.515152, acc 0.71875\n",
      "2018-05-06T20:22:31.788829: step 374, loss 0.498871, acc 0.71875\n",
      "2018-05-06T20:22:32.877219: step 375, loss 0.550595, acc 0.703125\n",
      "2018-05-06T20:22:33.997691: step 376, loss 0.550034, acc 0.71875\n",
      "2018-05-06T20:22:35.118290: step 377, loss 0.556663, acc 0.65625\n",
      "2018-05-06T20:22:36.283768: step 378, loss 0.575303, acc 0.78125\n",
      "2018-05-06T20:22:37.562382: step 379, loss 0.393824, acc 0.84375\n",
      "2018-05-06T20:22:38.851549: step 380, loss 0.53265, acc 0.71875\n",
      "2018-05-06T20:22:40.028050: step 381, loss 0.551137, acc 0.765625\n",
      "2018-05-06T20:22:41.115936: step 382, loss 0.550369, acc 0.734375\n",
      "2018-05-06T20:22:42.200249: step 383, loss 0.515246, acc 0.8125\n",
      "2018-05-06T20:22:43.434800: step 384, loss 0.54679, acc 0.734375\n",
      "2018-05-06T20:22:44.509483: step 385, loss 0.549856, acc 0.671875\n",
      "2018-05-06T20:22:45.544441: step 386, loss 0.677577, acc 0.625\n",
      "2018-05-06T20:22:46.648601: step 387, loss 0.6277, acc 0.71875\n",
      "2018-05-06T20:22:47.614511: step 388, loss 0.599323, acc 0.71875\n",
      "2018-05-06T20:22:48.630130: step 389, loss 0.528201, acc 0.765625\n",
      "2018-05-06T20:22:49.611700: step 390, loss 0.612414, acc 0.703125\n",
      "2018-05-06T20:22:50.671512: step 391, loss 0.542943, acc 0.71875\n",
      "2018-05-06T20:22:52.005536: step 392, loss 0.580735, acc 0.65625\n",
      "2018-05-06T20:22:53.125322: step 393, loss 0.544653, acc 0.765625\n",
      "2018-05-06T20:22:54.438250: step 394, loss 0.508135, acc 0.75\n",
      "2018-05-06T20:22:55.716007: step 395, loss 0.546858, acc 0.6875\n",
      "2018-05-06T20:22:56.944192: step 396, loss 0.533405, acc 0.75\n",
      "2018-05-06T20:22:58.318038: step 397, loss 0.558632, acc 0.6875\n",
      "2018-05-06T20:22:59.644728: step 398, loss 0.541574, acc 0.78125\n",
      "2018-05-06T20:23:00.792597: step 399, loss 0.521556, acc 0.734375\n",
      "2018-05-06T20:23:01.915289: step 400, loss 0.55847, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-06T20:23:04.516229: step 400, loss 0.470049, acc 0.806\n",
      "\n",
      "2018-05-06T20:23:05.672744: step 401, loss 0.533995, acc 0.75\n",
      "2018-05-06T20:23:06.777915: step 402, loss 0.570245, acc 0.71875\n",
      "2018-05-06T20:23:07.894120: step 403, loss 0.560467, acc 0.640625\n",
      "2018-05-06T20:23:08.977019: step 404, loss 0.530239, acc 0.75\n",
      "2018-05-06T20:23:10.040998: step 405, loss 0.55663, acc 0.640625\n",
      "2018-05-06T20:23:11.158733: step 406, loss 0.536996, acc 0.671875\n",
      "2018-05-06T20:23:12.374243: step 407, loss 0.590301, acc 0.6875\n",
      "2018-05-06T20:23:13.654388: step 408, loss 0.61045, acc 0.71875\n",
      "2018-05-06T20:23:14.970443: step 409, loss 0.575205, acc 0.65625\n",
      "2018-05-06T20:23:16.365412: step 410, loss 0.627327, acc 0.6875\n",
      "2018-05-06T20:23:17.686569: step 411, loss 0.52329, acc 0.765625\n",
      "2018-05-06T20:23:19.096114: step 412, loss 0.478928, acc 0.703125\n",
      "2018-05-06T20:23:20.194370: step 413, loss 0.573577, acc 0.703125\n",
      "2018-05-06T20:23:21.331982: step 414, loss 0.538271, acc 0.703125\n",
      "2018-05-06T20:23:22.405812: step 415, loss 0.555141, acc 0.734375\n",
      "2018-05-06T20:23:23.540183: step 416, loss 0.563644, acc 0.6875\n",
      "2018-05-06T20:23:24.689375: step 417, loss 0.605665, acc 0.671875\n",
      "2018-05-06T20:23:25.853929: step 418, loss 0.516937, acc 0.734375\n",
      "2018-05-06T20:23:26.958097: step 419, loss 0.475465, acc 0.78125\n",
      "2018-05-06T20:23:28.145335: step 420, loss 0.535859, acc 0.796875\n",
      "2018-05-06T20:23:29.351915: step 421, loss 0.617403, acc 0.765625\n",
      "2018-05-06T20:23:30.681941: step 422, loss 0.546398, acc 0.71875\n",
      "2018-05-06T20:23:31.958566: step 423, loss 0.564806, acc 0.671875\n",
      "2018-05-06T20:23:33.218669: step 424, loss 0.613461, acc 0.625\n",
      "2018-05-06T20:23:34.398047: step 425, loss 0.600747, acc 0.65625\n",
      "2018-05-06T20:23:35.566967: step 426, loss 0.58871, acc 0.6875\n",
      "2018-05-06T20:23:36.860530: step 427, loss 0.534917, acc 0.71875\n",
      "2018-05-06T20:23:37.943012: step 428, loss 0.558506, acc 0.71875\n",
      "2018-05-06T20:23:39.144218: step 429, loss 0.426918, acc 0.828125\n",
      "2018-05-06T20:23:40.311092: step 430, loss 0.565488, acc 0.75\n",
      "2018-05-06T20:23:41.536181: step 431, loss 0.500149, acc 0.765625\n",
      "2018-05-06T20:23:42.658750: step 432, loss 0.600143, acc 0.6875\n",
      "2018-05-06T20:23:43.829016: step 433, loss 0.555596, acc 0.765625\n",
      "2018-05-06T20:23:45.068955: step 434, loss 0.528903, acc 0.75\n",
      "2018-05-06T20:23:46.351989: step 435, loss 0.578701, acc 0.75\n",
      "2018-05-06T20:23:47.613835: step 436, loss 0.60697, acc 0.75\n",
      "2018-05-06T20:23:49.045463: step 437, loss 0.493072, acc 0.75\n",
      "2018-05-06T20:23:50.183836: step 438, loss 0.613464, acc 0.65625\n",
      "2018-05-06T20:23:51.285232: step 439, loss 0.573007, acc 0.625\n",
      "2018-05-06T20:23:52.438173: step 440, loss 0.578556, acc 0.703125\n",
      "2018-05-06T20:23:53.585639: step 441, loss 0.571227, acc 0.71875\n",
      "2018-05-06T20:23:54.947014: step 442, loss 0.438408, acc 0.828125\n",
      "2018-05-06T20:23:56.073225: step 443, loss 0.494069, acc 0.75\n",
      "2018-05-06T20:23:57.208498: step 444, loss 0.576187, acc 0.71875\n",
      "2018-05-06T20:23:58.348498: step 445, loss 0.499475, acc 0.765625\n",
      "2018-05-06T20:23:59.659945: step 446, loss 0.541519, acc 0.703125\n",
      "2018-05-06T20:24:00.809655: step 447, loss 0.501029, acc 0.71875\n",
      "2018-05-06T20:24:02.200175: step 448, loss 0.62443, acc 0.703125\n",
      "2018-05-06T20:24:03.829509: step 449, loss 0.551122, acc 0.703125\n",
      "2018-05-06T20:24:05.035798: step 450, loss 0.567813, acc 0.71875\n",
      "2018-05-06T20:24:06.447578: step 451, loss 0.578636, acc 0.703125\n",
      "2018-05-06T20:24:07.885536: step 452, loss 0.546706, acc 0.765625\n",
      "2018-05-06T20:24:09.270012: step 453, loss 0.651692, acc 0.625\n",
      "2018-05-06T20:24:10.501344: step 454, loss 0.563875, acc 0.71875\n",
      "2018-05-06T20:24:11.660080: step 455, loss 0.480745, acc 0.78125\n",
      "2018-05-06T20:24:12.912629: step 456, loss 0.57198, acc 0.671875\n",
      "2018-05-06T20:24:14.096999: step 457, loss 0.589114, acc 0.703125\n",
      "2018-05-06T20:24:15.497397: step 458, loss 0.598968, acc 0.671875\n",
      "2018-05-06T20:24:16.984869: step 459, loss 0.571405, acc 0.6875\n",
      "2018-05-06T20:24:18.224676: step 460, loss 0.551006, acc 0.75\n",
      "2018-05-06T20:24:19.395041: step 461, loss 0.563246, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-06T20:24:20.670194: step 462, loss 0.514917, acc 0.75\n",
      "2018-05-06T20:24:21.858210: step 463, loss 0.480432, acc 0.765625\n",
      "2018-05-06T20:24:23.037134: step 464, loss 0.671191, acc 0.65625\n",
      "2018-05-06T20:24:24.248100: step 465, loss 0.503227, acc 0.71875\n",
      "2018-05-06T20:24:25.412981: step 466, loss 0.474909, acc 0.75\n",
      "2018-05-06T20:24:26.764764: step 467, loss 0.551119, acc 0.703125\n",
      "2018-05-06T20:24:27.930405: step 468, loss 0.511406, acc 0.75\n",
      "2018-05-06T20:24:29.263426: step 469, loss 0.482129, acc 0.734375\n",
      "2018-05-06T20:24:30.690199: step 470, loss 0.494777, acc 0.78125\n",
      "2018-05-06T20:24:31.921167: step 471, loss 0.446835, acc 0.8125\n",
      "2018-05-06T20:24:33.315161: step 472, loss 0.517794, acc 0.75\n",
      "2018-05-06T20:24:34.912700: step 473, loss 0.511717, acc 0.765625\n",
      "2018-05-06T20:24:36.278392: step 474, loss 0.594465, acc 0.640625\n",
      "2018-05-06T20:24:37.762487: step 475, loss 0.545957, acc 0.734375\n",
      "2018-05-06T20:24:39.144890: step 476, loss 0.668632, acc 0.671875\n",
      "2018-05-06T20:24:40.415341: step 477, loss 0.530401, acc 0.65625\n",
      "2018-05-06T20:24:41.779598: step 478, loss 0.524625, acc 0.703125\n",
      "2018-05-06T20:24:43.233572: step 479, loss 0.70867, acc 0.640625\n",
      "2018-05-06T20:24:44.656756: step 480, loss 0.452531, acc 0.78125\n",
      "2018-05-06T20:24:46.051193: step 481, loss 0.575362, acc 0.671875\n",
      "2018-05-06T20:24:47.257807: step 482, loss 0.515548, acc 0.734375\n",
      "2018-05-06T20:24:48.509502: step 483, loss 0.535316, acc 0.703125\n",
      "2018-05-06T20:24:49.629385: step 484, loss 0.456657, acc 0.78125\n",
      "2018-05-06T20:24:50.816575: step 485, loss 0.541062, acc 0.703125\n",
      "2018-05-06T20:24:51.979218: step 486, loss 0.525254, acc 0.8125\n",
      "2018-05-06T20:24:53.306039: step 487, loss 0.622076, acc 0.71875\n",
      "2018-05-06T20:24:54.603127: step 488, loss 0.564764, acc 0.640625\n",
      "2018-05-06T20:24:55.621157: step 489, loss 0.508141, acc 0.734375\n",
      "2018-05-06T20:24:56.769936: step 490, loss 0.610778, acc 0.671875\n",
      "2018-05-06T20:24:57.961431: step 491, loss 0.542616, acc 0.703125\n",
      "2018-05-06T20:24:59.096497: step 492, loss 0.474666, acc 0.71875\n",
      "2018-05-06T20:25:00.595622: step 493, loss 0.513558, acc 0.75\n",
      "2018-05-06T20:25:01.905172: step 494, loss 0.560945, acc 0.734375\n",
      "2018-05-06T20:25:03.242906: step 495, loss 0.467371, acc 0.75\n",
      "2018-05-06T20:25:04.886212: step 496, loss 0.536408, acc 0.71875\n",
      "2018-05-06T20:25:06.183188: step 497, loss 0.628877, acc 0.609375\n",
      "2018-05-06T20:25:07.499450: step 498, loss 0.484727, acc 0.796875\n",
      "2018-05-06T20:25:08.817478: step 499, loss 0.569597, acc 0.71875\n",
      "2018-05-06T20:25:10.378487: step 500, loss 0.502362, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-06T20:25:15.135405: step 500, loss 0.447601, acc 0.81\n",
      "\n",
      "2018-05-06T20:25:16.645969: step 501, loss 0.497917, acc 0.71875\n",
      "2018-05-06T20:25:18.210273: step 502, loss 0.511798, acc 0.71875\n",
      "2018-05-06T20:25:19.573886: step 503, loss 0.569122, acc 0.703125\n",
      "2018-05-06T20:25:21.161341: step 504, loss 0.556241, acc 0.6875\n",
      "2018-05-06T20:25:22.637529: step 505, loss 0.555606, acc 0.734375\n",
      "2018-05-06T20:25:24.085200: step 506, loss 0.559857, acc 0.78125\n",
      "2018-05-06T20:25:25.394479: step 507, loss 0.532642, acc 0.734375\n",
      "2018-05-06T20:25:26.756710: step 508, loss 0.530691, acc 0.703125\n",
      "2018-05-06T20:25:28.141125: step 509, loss 0.433508, acc 0.84375\n",
      "2018-05-06T20:25:29.522718: step 510, loss 0.513648, acc 0.75\n",
      "2018-05-06T20:25:30.827307: step 511, loss 0.604887, acc 0.625\n",
      "2018-05-06T20:25:32.201244: step 512, loss 0.481074, acc 0.734375\n",
      "2018-05-06T20:25:33.706771: step 513, loss 0.450636, acc 0.84375\n",
      "2018-05-06T20:25:35.122645: step 514, loss 0.63749, acc 0.625\n",
      "2018-05-06T20:25:36.655085: step 515, loss 0.567859, acc 0.65625\n",
      "2018-05-06T20:25:38.047423: step 516, loss 0.573594, acc 0.703125\n",
      "2018-05-06T20:25:39.340267: step 517, loss 0.598769, acc 0.65625\n",
      "2018-05-06T20:25:40.575115: step 518, loss 0.591265, acc 0.734375\n",
      "2018-05-06T20:25:41.807814: step 519, loss 0.529676, acc 0.734375\n",
      "2018-05-06T20:25:43.045963: step 520, loss 0.765338, acc 0.578125\n",
      "2018-05-06T20:25:44.359291: step 521, loss 0.588938, acc 0.703125\n",
      "2018-05-06T20:25:45.643994: step 522, loss 0.501697, acc 0.75\n",
      "2018-05-06T20:25:46.893169: step 523, loss 0.530639, acc 0.75\n",
      "2018-05-06T20:25:48.085321: step 524, loss 0.572683, acc 0.734375\n",
      "2018-05-06T20:25:49.239940: step 525, loss 0.621874, acc 0.65625\n",
      "2018-05-06T20:25:50.436420: step 526, loss 0.533121, acc 0.75\n",
      "2018-05-06T20:25:51.538574: step 527, loss 0.516163, acc 0.796875\n",
      "2018-05-06T20:25:52.801772: step 528, loss 0.598417, acc 0.6875\n",
      "2018-05-06T20:25:54.013286: step 529, loss 0.658651, acc 0.671875\n",
      "2018-05-06T20:25:55.155764: step 530, loss 0.439141, acc 0.8125\n",
      "2018-05-06T20:25:56.495754: step 531, loss 0.593779, acc 0.6875\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)\n",
    "\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "\n",
    "        #timestamp = str(int(time.time()))\n",
    "        timestamp = \"1525408577\"\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp, \"2-3-4-5\"))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        \n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        if save_checkpoint:\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "  \n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "        batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "                \n",
    "            if save_checkpoint and current_step % evaluate_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char level cnn \n",
    "- 1525408577  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using clean_str  \n",
    "```\n",
    "Evaluation:\n",
    "2018-04-17T10:40:20.162299: step 5000, loss 0.161694, acc 0.942\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('./runs/1525408577/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(x):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=allow_soft_placement,\n",
    "          log_device_placement=log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_dim,\n",
    "                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            saver.restore(sess, \"runs/1525408577/checkpoints/model-98400\")\n",
    "\n",
    "            feed_dict = {\n",
    "                  cnn.input_x: x,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "            feature_5, feature_2 = sess.run([cnn.f_h, cnn.scores ], feed_dict=feed_dict)\n",
    "    return feature_5, feature_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = list(open(\"data/amazon/rating_5.txt\", \"r\").readlines())\n",
    "review = [s.strip() for s in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []\n",
    "for r in review:\n",
    "    l = r.split(\":::::\")\n",
    "    y.append(float(l[0]))\n",
    "    x.append(l[1].replace(\" \", \"\").replace(\"\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = preprocessing.VocabularyProcessor.restore(os.path.join(\"runs/1525408577\", \"vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x)))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5 ,feature_2 = get_feature(x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "chunk_size = 100\n",
    "for i in range(0, len(x) , chunk_size):\n",
    "    feature_5 ,feature_2 = get_feature(x[i:i+chunk_size])\n",
    "    for f, r in zip(feature_5, y[i:i+chunk_size]):\n",
    "        s  += int(np.argmax(f) == r)\n",
    "    print(s/(i+chunk_size))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(feature_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2 [0]  #[neg, pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3.6)",
   "language": "python",
   "name": "conda_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
