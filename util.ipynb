{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import MeCab\n",
    "from  urllib  import request\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, stopwords=None, parser=None, include_pos=None, exclude_posdetail=None, exclude_reg=None):\n",
    "    \n",
    "        if stopwords:\n",
    "            self.stopwords = stopwords\n",
    "        else:\n",
    "            self.stopwords = self.get_stopwords()\n",
    "        self.include_pos = include_pos if include_pos else  [\"名詞\", \"動詞\", \"形容詞\"]\n",
    "        self.exclude_posdetail = exclude_posdetail if exclude_posdetail else [\"接尾\", \"数\"]\n",
    "        self.exclude_reg = exclude_reg if exclude_reg else r\"$^\"  # no matching reg\n",
    "        if parser:\n",
    "            self.parser = parser\n",
    "        else:\n",
    "            if platform.system() == \"Darwin\":\n",
    "                dic_dir = \"/usr/local/lib/mecab/dic/mecab-ipadic-neologd/\" #mac\n",
    "            else:\n",
    "                dic_dir = \"/usr/lib/mecab/dic/mecab-ipadic-neologd\"\n",
    "            mecab = MeCab.Tagger(\"-Ochasen -d {}\".format(dic_dir))\n",
    "            self.parser = mecab.parse\n",
    "            \n",
    "\n",
    "    def tokenize(self, text, show_pos=False):\n",
    "        text = re.sub(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", \"\", text)    #URL\n",
    "        text = re.sub(r\"\\\"?([-a-zA-Z0-9.`?{}]+\\.jp)\\\"?\" ,\"\", text)  # xxx.jp \n",
    "        text = text.lower()\n",
    "        l = [line.split(\"\\t\") for line in self.parser(text).split(\"\\n\")]\n",
    "        res = [\n",
    "            i[2] if not show_pos else (i[2],i[3]) for i in l \n",
    "                if len(i) >=4 # has POS.\n",
    "                    and i[3].split(\"-\")[0] in self.include_pos\n",
    "                    and i[3].split(\"-\")[1] not in self.exclude_posdetail\n",
    "                    and not re.search(r\"(-|−)\\d\", i[2])\n",
    "                    and not re.search(self.exclude_reg, i[2])\n",
    "                    and i[2] not in self.stopwords          \n",
    "            ]\n",
    "        return res\n",
    "    \n",
    "    def get_stopwords(self):\n",
    "        res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n",
    "        stopwords = [line.decode(\"utf-8\").strip() for line in res]\n",
    "        print(\"Japanese stopword: \", \", \".join(stopwords[:3]), \"...\")\n",
    "        res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/English.txt\")\n",
    "        stopwords += [line.decode(\"utf-8\").strip() for line in res]\n",
    "        print(\"English stopword: ...\", \", \".join(stopwords[-3:]), )\n",
    "        return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese stopword:  あそこ, あたり, あちら ...\n",
      "English stopword: ... you've, z, zero\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['認める', '自分自身', '若さ故の過ち']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokenize(\"認めたくないものだな。自分自身の若さ故の過ちというものを。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file, level=\"char\", lang=\"En\"):\n",
    "       \n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    if level == \"char\":\n",
    "        positive_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in positive_examples]\n",
    "        negative_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in negative_examples]\n",
    "    elif level == \"word\":\n",
    "        if lang == \"Ja\":\n",
    "            t = Tokenizer()\n",
    "            positive_examples = [t.tokenize(s) for s in positive_examples]\n",
    "            negative_examples = [t.tokenize(s) for s in negative_examples]\n",
    "        else:\n",
    "            positive_examples = [s.strip() for s in positive_examples]\n",
    "            negative_examples = [s.strip() for s in negative_examples]\n",
    "    else:\n",
    "        print(\"invaid value of 'level'. ('char' or 'word') \")\n",
    "        \n",
    "    n_pos = len(positive_examples)\n",
    "    n_neg = len(negative_examples)\n",
    "    ratio = n_pos/n_neg\n",
    "    print(\"# pos: \", n_pos)\n",
    "    print(\"# neg: \", n_neg)\n",
    "    print(\"pos/neg:\", ratio)\n",
    "    x_text = positive_examples + negative_examples\n",
    "\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    return x_text, y, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels_multiclass(files, level=\"char\", lang=\"En\"):\n",
    "    labels = []\n",
    "    x_text = []\n",
    "    n_classes = len(files)\n",
    "    \n",
    "    for i, f in enumerate(files):\n",
    "        positive_examples = list(open(f, \"r\").readlines())\n",
    "        if level == \"char\":\n",
    "            positive_examples = [s.replace(\" \", \"\").replace(\"\", \" \").lower() for s in positive_examples]\n",
    "        elif level == \"word\":\n",
    "            if lang == \"Ja\":\n",
    "                t = Tokenizer()\n",
    "                positive_examples = [t.tokenize(s) for s in positive_examples]\n",
    "            else:\n",
    "                positive_examples = [s.strip() for s in positive_examples]\n",
    "        else:\n",
    "            print(\"invaid value of 'level'. ('char' or 'word') \")\n",
    "        print(len(positive_examples))\n",
    "        x_text += positive_examples\n",
    "        positive_labels = [np.identity(n_classes)[i] for _ in positive_examples]\n",
    "        labels.append(positive_labels)\n",
    "    \n",
    "    y = np.concatenate(labels, 0)\n",
    "    \n",
    "    return x_text, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3.6)",
   "language": "python",
   "name": "conda_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
